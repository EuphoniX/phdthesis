% Encoding: UTF-8

@WWW{google_developers_protocol_nodate,
  author   = {{Google Developers}},
  title    = {Protocol {Buffers}},
  year     = {2018},
  url      = {https://developers.google.com/protocol-buffers/},
  urldate  = {2017-09-10},
  abstract = {Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/VY4QIBHH/protocol-buffers.html/\:text/html\:PDF:PDF},
  journal  = {Google Developers},
}

@WWW{xsens_mti_nodate,
  author   = {Xsens},
  title    = {{MTi} (legacy product) - {Products}},
  year     = {2018},
  url      = {https://www.xsens.com/products/mti/},
  urldate  = {2017-09-10},
  abstract = {The MTi is discontinued since August 31st, 2014. Please refer...},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/5KXJ9L3G/mti.html/\:text/html\:PDF:PDF},
  journal  = {Xsens 3D motion tracking},
}

@WWW{raymond_gpsd_nodate,
  author  = {Raymond, Eric S.},
  title   = {{GPSd} — {Put} your {GPS} on the net!},
  year    = {2018},
  url     = {http://www.catb.org/gpsd/},
  urldate = {2017-09-10},
  file    = {html\:PDF:html\:GPSd — Put your GPS on the net!/\:C//\:/Users/Psykie/Zotero/storage/45WLX7IK/gpsd.html/\:text/html\:PDF:PDF},
}

@WWW{victory_co._ltd_columbus_nodate,
  author  = {{Victory Co. Ltd}},
  title   = {Columbus {V}-800},
  year    = {2018},
  url     = {http://www.cbgps.com/v800/index_en.htm},
  urldate = {2017-09-10},
  file    = {html\:PDF:html\:Columbus V-800/\:C//\:/Users/Psykie/Zotero/storage/I4GSYMJT/index_en.html/\:text/html\:PDF:PDF},
}

@book{national_marine_electronics_association_nmea_2002,
	title = {{NMEA} 0183--{Standard} for interfacing marine electronic devices},
	publisher = {NMEA},
	author = {National Marine Electronics Association},
	year = {2002}
}

@Article{ljung_asymptotic_1979,
  author   = {Ljung, L.},
  title    = {Asymptotic behavior of the extended {Kalman} filter as a parameter estimator for linear systems},
  journal  = {IEEE Transactions on Automatic Control},
  year     = {1979},
  volume   = {24},
  number   = {1},
  month    = feb,
  pages    = {36--50},
  issn     = {0018-9286},
  doi      = {10.1109/TAC.1979.1101943},
  abstract = {The extended Kalman filter is an approximate filter for nonlinear systems, based on first-order linearization. Its use for the joint parameter and state estimation problem for linear systems with unknown parameters is well known and widely spread. Here a convergence analysis of this method is given. It is shown that in general, the estimates may be biased or divergent and the causes for this are displayed. Some common special cases where convergence is guaranteed are also given. The analysis gives insight into the convergence mechanisms and it is shown that with a modification of the algorithm, global convergence results can be obtained for a general case. The scheme can then be interpreted as maximization of the likelihood function for the estimation problem, or as a recursive prediction error algorithm.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/4AGFGMN2/1101943.html/\:text/html\:PDF:PDF},
  keywords = {Algorithm design and analysis, Convergence, Estimation theory, Kalman filtering, Linear systems, Linear systems, stochastic discrete-time, Nonlinear filters, Nonlinear systems, Parameter estimation, Prediction algorithms, Recursive estimation, State estimation, stochastic discrete-time},
}

@WWW{arduino_arduino_nodate,
  author  = {Arduino},
  title   = {Arduino {Uno} {Rev}3},
  year    = {2018},
  url     = {https://store.arduino.cc/usa/arduino-uno-rev3},
  urldate = {2017-09-10},
  file    = {html\:PDF:html\:Arduino Uno Rev3/\:C//\:/Users/Psykie/Zotero/storage/UZ9GZQ2P/arduino-uno-rev3.html/\:text/html\:PDF:PDF},
}

@WWW{autonomoustuff_ibeo_nodate,
  author   = {AutonomouStuff},
  title    = {ibeo {Standard} {Four} {Layer} {Multi}-{Echo} {LUX} {Sensor} {\textbar} {LiDAR} {\textbar} {Product}},
  year     = {2018},
  url      = {https://autonomoustuff.com/product/ibeo-lux-standard/},
  urldate  = {2017-09-10},
  abstract = {The ibeo LUX laser scanner is a unique full-range sensor used for object detection and classification to support ADAS applications.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/Y8Q8Y5YA/ibeo-lux-standard.html/\:text/html\:PDF:PDF},
  journal  = {AutonomouStuff, LLC},
}

@WWW{sae_international_formula_nodate,
  author  = {{SAE International}},
  title   = {Formula {SAE} - {SAE} {Collegiate} {Design} {Series} - {Students} - {SAE} {International}},
  year    = {2018},
  url     = {http://students.sae.org/cds/formulaseries/},
  urldate = {2017-09-10},
  file    = {html\:PDF:html\:Formula SAE - SAE Collegiate Design Series - Students - SAE International/\:C//\:/Users/Psykie/Zotero/storage/TQQN4XQW/formulaseries.html/\:text/html\:PDF:PDF},
}

@WWW{kohlhoff_boost.asio_nodate,
  author  = {Kohlhoff, Christopher},
  title   = {Boost.{Asio} - 1.66.0},
  year    = {2018},
  url     = {http://www.boost.org/doc/libs/1_66_0/doc/html/boost_asio.html},
  urldate = {2018-01-23},
  file    = {html\:PDF:html\:Boost.Asio - 1.66.0/\:C//\:/Users/Psykie/Zotero/storage/BURKLYAK/boost_asio.html/\:text/html\:PDF:PDF},
}

@WWW{lambert_bmw_2017,
  author   = {Lambert, Fred},
  title    = {{BMW} and {Porsche} join forces to enable 15-min electric car charging at 450 {kW} charge rate},
  year     = {2017},
  url      = {https://electrek.co/2017/12/05/bmw-porsche-electric-car-charging-450-kw-charge-rate/},
  language = {en},
  month    = dec,
  urldate  = {2018-02-02},
  abstract = {BMW and Porsche are already working together and with other major automakers, like Mercedes and Ford, on the major Ionity ultra-fast (350 kW) electric car charging network in Europe. But now they w…},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/NKH3GCQ6/bmw-porsche-electric-car-charging-450-kw-charge-rate.html/\:text/html\:PDF:PDF},
  journal  = {Electrek},
}

@WWW{kane_fastcharge_2017,
  author   = {Kane, Mark},
  title    = {{FastCharge} {Now} {Evaluating} 450 {kW} {Charging}},
  year     = {2017},
  url      = {https://insideevs.com/fastcharge-now-evaluating-450-kw-charging/},
  language = {en},
  month    = dec,
  urldate  = {2018-02-02},
  file     = {html\:PDF:html\:FastCharge Now Evaluating 450 kW Charging/\:C//\:/Users/Psykie/Zotero/storage/F9UYKQS9/fastcharge-now-evaluating-450-kw-charging.html/\:text/html\:PDF:PDF},
  journal  = {InsideEVs},
}

@WWW{tritium_pty_ltd_veefil--electric_2015,
  author    = {{Tritium}},
  title     = {Veefil--{Electric} vehicle fast charger instruction manual},
  year      = {2015},
  url       = {https://fccid.io/2AFHX-TRI935001US/User-Manual/Users-Manual-3059907.pdf},
  language  = {en},
  urldate   = {2018-02-02},
  publisher = {Tritium Pty Ltd},
}

@TechReport{climateworks_australia_path_2016,
  author      = {{ClimateWorks Australia}},
  title       = {The {Path} {Forward} for {Electric} {Vehicles} in {Australia}},
  institution = {ClimateWorks Australia},
  year        = {2016},
  language    = {en},
  month       = apr,
  pages       = {36},
  url         = {https://climateworks.com.au/publication/document/path-forward-electric-vehicles-australia-joint-submission-federal-government},
  urldate     = {2018-02-02},
  address     = {Melbourne, Australia},
  school      = {ClimateWorks Publication},
}

@WWW{rac_wa_rac_nodate,
  author   = {{RAC WA}},
  title    = {{RAC} {Electric} {Highway}},
  year     = {2018},
  url      = {http://electrichighway.rac.com.au/},
  urldate  = {2018-02-02},
  abstract = {The RAC Electric Highway®, will be a network of publicly accessible electric vehicle fast-charging DC stations located between Perth and the South West.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/DXS9QUM9/electrichighway.rac.com.au.html/\:text/html\:PDF:PDF},
}

@WWW{noauthor_electric_2017,
  author  = {{The REV Project}},
  title   = {Electric {Vehicle} {DC} {Fast}-{Charging} {Station}},
  year    = {2017},
  url     = {http://therevproject.com/trials/dc-charging-trial.php},
  month   = dec,
  urldate = {2018-02-02},
  file    = {html\:PDF:html\:The REV Project/\:C//\:/Users/Psykie/Zotero/storage/WX8677AH/dc-charging-trial.html/\:text/html\:PDF:PDF},
  journal = {The REV Project},
}

@TechReport{mader_western_2013,
  author      = {Mader, Terrence and Bräunl, Thomas},
  title       = {Western {Australian} {Electric} {Vehicle} {Trial}},
  institution = {The University of Western Australia},
  year        = {2013},
  language    = {en},
  pages       = {59},
  url         = {http://therevproject.com/trialreport.pdf},
  urldate     = {2018-02-02},
  address     = {Perth, Australia},
  school      = {The University of Western Australia},
}

@Article{steitz_plug_2018,
  author     = {Steitz, Christoph},
  title      = {Plug wars: the battle for electric car supremacy},
  journal    = {Reuters},
  year       = {2018},
  month      = jan,
  url        = {https://www.reuters.com/article/us-autos-electricity-charging/plug-wars-the-battle-for-electric-car-supremacy-idUSKBN1FD0QM},
  urldate    = {2018-02-02},
  abstract   = {German carmakers hope a network of high-power charging stations they are rolling out with Ford will set an industry standard for plugs and protocols that will give them an edge over electric car rivals.},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/CVQ8I6IM/plug-wars-the-battle-for-electric-car-supremacy-idUSKBN1FD0QM.html/\:text/html\:PDF:PDF},
  keywords   = {Asia / Pacific, Austria, Auto and Truck Manufacturers (TRBC), Automobiles / Auto Parts (Legacy), Automobiles and Auto Parts (TRBC), AUTOS, CHARGING, China (PRC), Company News, Corporate Events, Electric Utilities (TRBC), ELECTRICITY, Europe, Germany, Government Borrowing Requirement, Graphics, Integrated Oil and Gas (TRBC), Japan, Major News, Netherlands, Oil and Gas Refining and Marketing (TRBC), Picture available, Pictures, Power Markets, United Kingdom, United States, US},
  shorttitle = {Plug wars},
}

@TechReport{china_electric_vehicle_charging_infrastructure_promotion_alliance_zhongguo_2017,
  author      = {{China Electric Vehicle Charging Infrastructure Promotion Alliance}},
  title       = {Zhongguo diandong qiche chongdian jichu sheshi fazhan niandu baogao 2016-2017 ban [{China} {Electric} {Vehicle} {Charging} {Infrastructure} {Development} {Annual} {Report} 2016-2017 {Edition}]},
  institution = {National Energy Administration},
  year        = {2017},
  language    = {Chinese},
  month       = apr,
  pages       = {76},
  url         = {http://www.nea.gov.cn/136376732_14978397401671n.pdf},
  urldate     = {2018-02-02},
  address     = {Beijing, China},
  school      = {National Energy Administration},
}

@WWW{charging_interface_initiative_e._v._ccs_2018,
  author   = {{Charging Interface Initiative e. V.}},
  title    = {{CCS} {Specification}},
  year     = {2018},
  url      = {http://www.charinev.org/ccs-at-a-glance/ccs-specification/},
  language = {en},
  month    = jan,
  urldate  = {2018-02-02},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/GJBVW5FH/ccs-specification.html/\:text/html\:PDF:PDF},
}

@Article{morrissey_future_2016,
  author     = {Morrissey, Patrick and Weldon, Peter and O’Mahony, Margaret},
  title      = {Future standard and fast charging infrastructure planning: {An} analysis of electric vehicle charging behaviour},
  journal    = {Energy Policy},
  year       = {2016},
  volume     = {89},
  month      = feb,
  pages      = {257--270},
  issn       = {0301-4215},
  doi        = {10.1016/j.enpol.2015.12.001},
  url        = {http://www.sciencedirect.com/science/article/pii/S0301421515302159},
  urldate    = {2018-02-02},
  abstract   = {There has been a concentrated effort by European countries to increase the share of electric vehicles (EVs) and an important factor in the rollout of the associated infrastructure is an understanding of the charging behaviours of existing EV users in terms of location of charging, the quantity of energy they require, charge duration, and their preferred mode of charging. Data were available on the usage of charging infrastructure for the entire island of Ireland since the rollout of infrastructure began. This study provides an extensive analysis of this charge event data for public charging infrastructure, including data from fast charging infrastructure, and additionally a limited quantity of household data. For the household data available, it was found that EV users prefer to carry out the majority of their charging at home in the evening during the period of highest demand on the electrical grid indicating that incentivisation may be required to shift charging away from this peak grid demand period. Car park locations were the most popular location for public charging amongst EV users, and fast chargers recorded the highest usage frequencies, indicating that public fast charging infrastructure is most likely to become commercially viable in the short- to medium-term.},
  file       = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/4PQ7PNMV/S0301421515302159.html/\:text/html\:PDF:PDF},
  keywords   = {Electric vehicle, Fast chargers, Public charging infrastructure},
  shorttitle = {Future standard and fast charging infrastructure planning},
}

@WWW{plugshare_plugshare_nodate,
  author   = {PlugShare},
  title    = {{PlugShare} - {EV} {Charging} {Station} {Map}},
  year     = {2018},
  url      = {https://www.plugshare.com/},
  urldate  = {2018-02-02},
  abstract = {Find a place to plug in your electric car (EV) with PlugShare's database of over 50,000 charging stations! Map nearby Superchargers for the Tesla Model S, Quick Charge (CHAdeMO) for the Nissan LEAF, and map nearby charging stations for the Chevy Volt, BMW i3, and all other electric vehicles.},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\J2FEKBEQ\\\\\\\\www.plugshare.com.html\\\:text/html\:PDF:PDF},
}

@PhdThesis{johansen_fast-charging_2013,
  author   = {Johansen, Joachim Skov},
  title    = {Fast-{Charging} {Electric} {Vehicles} using {AC}},
  year     = {2013},
  language = {en},
  url      = {http://www1.udel.edu/V2G/resources/JSJ-PreliminaryReportJSJ200913.pdf},
  address  = {Kongens Lyngby, Denmark},
  file     = {pdf\:PDF:pdf\:JSJ-PreliminaryReportJSJ200913.pdf/\:C//\:/Users/Psykie/Zotero/storage/ZDJXS94X/JSJ-PreliminaryReportJSJ200913.pdf/\:application/pdf\:PDF:PDF},
  school   = {Technical University of Denmark},
}

@WWW{tesla_supercharger_nodate,
  author   = {Tesla},
  title    = {Supercharger},
  year     = {2018},
  url      = {https://www.tesla.com/en_AU/supercharger},
  urldate  = {2018-02-02},
  abstract = {The world's fastest electric vehicle charging stations, Superchargers will charge your Tesla in a matter of minutes instead of hours.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/Y3M9XIUW/supercharger.html/\:text/html\:PDF:PDF},
}

@Article{kalwar_inductively_2015,
  author   = {Kalwar, Kafeel Ahmed and Aamir, Muhammad and Mekhilef, Saad},
  title    = {Inductively coupled power transfer ({ICPT}) for electric vehicle charging – {A} review},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2015},
  volume   = {47},
  month    = jul,
  pages    = {462--475},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2015.03.040},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032115001938},
  urldate  = {2018-02-02},
  abstract = {The deficiency in the availability of petroleum products has given rise to the incorporation of electric vehicles (EVs) globally as a substitute for the conventional transportation system. Significant research has been pursued over last two decades in the development of efficient EV charging methods. A preliminary review of few methods developed for wireless charging revealed that ICPT is a promising and convenient method for the wireless charging of EVs. This paper includes the equivalent circuit analysis and characteristics of the ICPT system and focuses on the research progress in respect of the designs for the charging coil, leakage inductance compensation topologies, power level enhancement and misalignment toleration. The improvement in these factors has been essential for the implementation of EV charging. A brief discussion over design process and control of ICPT system has been added. Conclusions have been made on the basis of the information extracted from the literature and some future recommendations are provided.},
  file     = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/7G2D9EUL/S1364032115001938.html/\:text/html\:PDF:PDF},
  keywords = {Electric vehicle (EV), Inductively coupled power transfer (ICPT), Wireless charging of electric vehicle},
}

@Article{birrell_how_2015,
  author   = {Birrell, Stewart A. and Wilson, Daniel and Yang, Chek Pin and Dhadyalla, Gunwant and Jennings, Paul},
  title    = {How driver behaviour and parking alignment affects inductive charging systems for electric vehicles},
  journal  = {Transportation Research Part C: Emerging Technologies},
  year     = {2015},
  series   = {Technologies to support green driving},
  volume   = {58},
  month    = sep,
  pages    = {721--731},
  issn     = {0968-090X},
  doi      = {10.1016/j.trc.2015.04.011},
  url      = {http://www.sciencedirect.com/science/article/pii/S0968090X15001527},
  urldate  = {2018-02-02},
  abstract = {Inductive charging, a form of wireless charging, uses an electromagnetic field to transfer energy between two objects. This emerging technology offers an alternative solution to users having to physically plug in their electric vehicle (EV) to charge. Whilst manufacturers claim inductive charging technology is market ready, the efficiency of transfer of electrical energy is highly reliant on the accurate alignment of the coils involved. Therefore understanding the issue of parking misalignment and driver behaviour is an important human factors question, and the focus of this paper. Two studies were conducted, one a retrospective analysis of 100 pre-parked vehicles, the second a dynamic study where 10 participants parked an EV aiming to align with a charging pad with no bay markings as guidance. Results from both studies suggest that drivers are more accurate at parking laterally than in the longitudinal direction, with a mean lateral distance from the centre of the bay being 12.12 and 9.57cm (retrospective and dynamic studies respectively) compared to longitudinally 23.73 and 73.48cm. With current inductive charging systems having typical tolerances of approximately ±10cm from their centre point, this study has shown that only 5\% of vehicles in both studies would be aligned sufficiently accurately to allow efficient transfer of electrical energy through induction.},
  file     = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/LKLEDRBD/S0968090X15001527.html/\:text/html\:PDF:PDF},
  keywords = {Driver behaviour, Electric vehicles, Inductive charging, Parking alignment, Wireless charging},
}

@WWW{department_of_infrastructure_and_regional_development_green_2018,
  author  = {{Department of Infrastructure {and} Regional Development}},
  title   = {Green {Vehicle} {Guide}},
  year    = {2018},
  url     = {https://www.greenvehicleguide.gov.au/},
  urldate = {2018-02-02},
  file    = {html\:PDF:html\:Green Vehicle Guide Home\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\V7ZHKY7L\\\\\\\\www.greenvehicleguide.gov.au.html\\\:text/html\:PDF:PDF},
}

@WWW{open_charge_alliance_ocpp_nodate,
  author  = {{Open Charge Alliance}},
  title   = {{OCPP} 1.6, {OCPP}, {Protocols} - {Open} {Charge} {Alliance}},
  year    = {2018},
  url     = {http://www.openchargealliance.org/protocols/ocpp/ocpp-16/},
  urldate = {2018-02-02},
  file    = {html\:PDF:html\:OCPP 1.6, OCPP, Protocols - Open Charge Alliance/\:C//\:/Users/Psykie/Zotero/storage/7DHPJI97/ocpp-16.html/\:text/html\:PDF:PDF},
}

@WWW{bullis_electric_2013,
  author   = {Bullis, Kevin},
  title    = {Electric {Vehicles} {Out} in the {Cold}},
  year     = {2013},
  url      = {https://www.technologyreview.com/s/522496/electric-vehicles-out-in-the-cold/},
  language = {en},
  month    = dec,
  urldate  = {2018-02-02},
  abstract = {Electric vehicle range drops in cold weather, and technological solutions are years away.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/VX8UJI8C/electric-vehicles-out-in-the-cold.html/\:text/html\:PDF:PDF},
  journal  = {MIT Technology Review},
}

@Article{wang_critical_2016,
  author   = {Wang, Qian and Jiang, Bin and Li, Bo and Yan, Yuying},
  title    = {A critical review of thermal management models and solutions of lithium-ion batteries for the development of pure electric vehicles},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2016},
  volume   = {64},
  month    = oct,
  pages    = {106--128},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2016.05.033},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032116301435},
  urldate  = {2018-02-02},
  abstract = {Power train electrification is promoted as a potential alternative to reduce carbon intensity of transportation. Lithium-ion batteries are found to be suitable for hybrid electric vehicles (HEVs) and pure electric vehicles (EVs), and temperature control on lithium batteries is vital for long-term performance and durability. Unfortunately, battery thermal management (BTM) has not been paid close attention partly due to poor understanding of battery thermal behaviour. Cell performance change dramatically with temperature, but it improves with temperature if a suitable operating temperature window is sustained. This paper provides a review on two aspects that are battery thermal model development and thermal management strategies. Thermal effects of lithium-ion batteries in terms of thermal runaway and response under cold temperatures will be studied, and heat generation methods are discussed with aim of performing accurate battery thermal analysis. In addition, current BTM strategies utilised by automotive suppliers will be reviewed to identify the imposing challenges and critical gaps between research and practice. Optimising existing BTMs and exploring new technologies to mitigate battery thermal impacts are required, and efforts in prioritising BTM should be made to improve the temperature uniformity across the battery pack, prolong battery lifespan, and enhance the safety of large packs.},
  file     = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/KBNUYIQF/S1364032116301435.html/\:text/html\:PDF:PDF},
  keywords = {Heat pipe, Lithium-ion battery thermal management, Low carbon vehicles, Pure electric and hybrid cars},
}

@Book{dell_towards_2014,
  author    = {Dell, Ronald M. and Moseley, Patrick T. and Rand, David A. J.},
  title     = {Towards Sustainable Road Transport},
  year      = {2014},
  publisher = {Elsevier LTD, Oxford},
  isbn      = {0124046169},
  pagetotal = {345},
  url       = {https://www.ebook.de/de/product/22141108/ronald_m_dell_patrick_t_moseley_david_a_j_rand_towards_sustainable_road_transport.html},
  ean       = {9780124046160},
  month     = aug,
}

@Article{speidel_leaving_2016,
  author   = {Speidel, Stuart and Bräunl, Thomas},
  title    = {Leaving the grid—{The} effect of combining home energy storage with renewable energy generation},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2016},
  volume   = {60},
  month    = jul,
  pages    = {1213--1224},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2015.12.325},
  url      = {https://www.sciencedirect.com/science/article/pii/S1364032115017086},
  urldate  = {2018-02-02},
  abstract = {Household renewable energy generation through the use of solar panels is becoming more commonplace as the installation cost is reducing and electricity prices are rising. Solar energy is an intermittent source, only generated during the day subject to interference from weather and seasonal variation. Energy storage solutions such as Lithium Ion batteries are also reducing is cost and have become a viable solution for storing the solar energy generated for use at other times.
In this paper we discuss the feasibility and limitations of various renewable energy, energy storage, feed into grid and off the grid systems. We also explore the results of our case study, The University of Western Australia׳s Future Farm, which featured a 10 kW solar system with 20 kWh battery storage, off the grid. Finally we use West Australians daily energy usage information to model the energy and savings of installing solar panels, home energy storage and using an electric vehicle.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/MLAZ579S/Speidel and Bräunl - 2016 - Leaving the grid—The effect of combining home ener.pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/EBWHQ36K/S1364032115017086.html/\:text/html\:PDF:PDF},
  keywords = {Electric vehicles, Grid independence, Local energy storage, Off-grid, Renewable energy},
}

@Article{noauthor_ieee_2016,
  author   = {IEEE},
  title    = {{IEEE} {Standard} {Technical} {Specifications} of a {DC} {Quick} {Charger} for {Use} with {Electric} {Vehicles}},
  journal  = {IEEE Std 2030.1.1-2015},
  year     = {2016},
  month    = feb,
  pages    = {1--97},
  doi      = {10.1109/IEEESTD.2016.7400449},
  abstract = {Direct-current (dc) charging is a method of charging that facilitates rapid energy transfer from the electric grid to plug-in vehicles. This method of charging allows significantly more current to be drawn by the vehicle versus lower rated alternating-current (ac) systems. A combination of vehicles that can accept high-current dc charge and the dc supply equipment that provides it has led to the use of terminology such as fast charging, fast charger, dc charger, quick charger, etc. DC charging and ac charging vary by the location at which ac current is converted to dc current. For typical dc charging, the current is converted at the off-board charger, which is separate from the vehicle. For ac charging, the current is converted inside the vehicle, by means of an on-board charger. The location of the ac to dc conversion equipment, or converter, shapes the complexity of the equipment design. Regarding ac charging, as previously mentioned, the conversion is on boardthe vehicle. This allows the original equipment maker (OEM) designed systems to control the charging operation in its entirety. The on-board charger (converter) and battery controller solutionis under direct control of the vehicle manufacturer. For dc charging, an entirely new challenge exists for OEMs. The dc charger is now external to the vehicle and requires the vehicle engineers to control an external power device. For the reason of necessary interoperability, standards such as IEEE Std 2030.1.1 are provided to assist developers.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/Y3QIX3Z4/definitions.html/\:text/html\:PDF:PDF},
  keywords = {Electric vehicles, ac charging, AC-DC power convertors, ac-to-dc conversion equipment, automotive, battery chargers, Battery chargers, battery controller solution, battery powered vehicles, CHAdeMO, charging operation control, converter, dc charger, dc charging, DC motors, DC quick charger, dc supply equipment, direct-current charging, electric grid, electric vehicle, electric vehicles, equipment design complexity, external power device control, fast charger, fast charging, high-current dc charge, IEC 61851-23, IEEE, IEEE 2030.1.1(TM), IEEE standard technical specifications, IEEE standards, IEEE Standards, IEEE Std 2030.1.1-2015, OEM, off-board charger, on-board charger, original equipment maker, plug-in vehicles, power conversion, power grids, rapid charging, rapid energy transfer, SAE, SAE J1772, SAE J2836/2},
}

@Article{bailey_is_2015,
  author   = {Bailey, Joseph and Miele, Amy and Axsen, Jonn},
  title    = {Is awareness of public charging associated with consumer interest in plug-in electric vehicles?},
  journal  = {Transportation Research Part D: Transport and Environment},
  year     = {2015},
  volume   = {36},
  month    = may,
  pages    = {1--9},
  issn     = {1361-9209},
  doi      = {10.1016/j.trd.2015.02.001},
  url      = {http://www.sciencedirect.com/science/article/pii/S1361920915000103},
  urldate  = {2018-02-02},
  abstract = {Policymakers often seek to increase the visibility of plug-in electric vehicle (PEV) chargers in public locations in effort to build familiarity and interest in PEVs. However, it is not clear if the visibility of public charging stations actually has an impact on PEV demand. The purposes of the present study are to (1) assess the current levels of visibility for public PEV charging infrastructure within Canada and (2) identify whether or not a statistically significant relationship exists between consumer awareness of public charging infrastructure and interest in purchasing a PEV. We use data collected from a sample of 1739 Canadian new-vehicle buyers in 2013. About 18\% of Canadian respondents have seen at least one public charger, while the proportion is highest in British Columbia (31\%). We find a significant bivariate relationship between public charger awareness and PEV interest. However, when controlling for multiple explanatory variables in regression analyses, the relationship is weak or non-existent. While perceived existence of at least one charger exhibits no significant relationship with PEV interest, perceived existence of multiple chargers can have a weak but significant relationship. Thus, public charger awareness is not a strong predictor of PEV interest; other variables are more important, such as the availability of level 1 (110/120-volt) charging at home.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/T4PBRZRZ/Bailey et al. - 2015 - Is awareness of public charging associated with co.pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/YL3PSZWK/S1361920915000103.html/\:text/html\:PDF:PDF},
  keywords = {Consumer demand, Plug-in electric vehicles, Public charging stations, Regression, Survey},
}

@Article{gebauer_changing_2016,
  author   = {Gebauer, Fabian and Vilimek, Roman and Keinath, Andreas and Carbon, Claus-Christian},
  title    = {Changing attitudes towards e-mobility by actively elaborating fast-charging technology},
  journal  = {Technological Forecasting and Social Change},
  year     = {2016},
  volume   = {106},
  month    = may,
  pages    = {31--36},
  issn     = {0040-1625},
  doi      = {10.1016/j.techfore.2016.02.006},
  url      = {http://www.sciencedirect.com/science/article/pii/S0040162516000470},
  urldate  = {2018-02-02},
  abstract = {Since electromobility (e-mobility) is a large field of innovation, it is crucial to examine new developments with potential users in mind. Therefore, we investigated the impact that new fast-charging technologies for electric vehicles (EV) have on ordinary people's assessment about the future prospects of e-mobility—which is an important prerequisite for increased attitudes towards e-mobility in general. First we let participants perform a typical charging process, where they were either introduced to the slower-operating, alternating current (AC) system or the fast-operating direct current (DC) system. In a second experiment we used the same procedure but instead of letting participants actively perform the charging process, they were only given written information about these charging technologies. Results show that participants' future estimation about EVs only rises when they actively charge an EV in the fast DC condition but not in the AC condition. General attitudes towards EVs increase independently of the AC or DC condition. None of these effects could be seen without active hands-on experience (second experiment). These indications imply the value of investing in fast-charging systems to induce more favorable judgments regarding the future prospect of EVs. The importance of letting people actively take part in the way e-mobility works will be discussed regarding the potentially improvement of participants' attitudes towards e-mobility.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/UXR45WPQ/Gebauer et al. - 2016 - Changing attitudes towards e-mobility by actively .pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/JQN4C8XT/S0040162516000470.html/\:text/html\:PDF:PDF},
  keywords = {Acceptance, E-mobility, Fast DC charging, Future concept, Prediction},
}

@WWW{statista_worldwide_nodate,
  author   = {Statista},
  title    = {Worldwide number of battery electric vehicles in use from 2012 to 2016 (in 1,000s)},
  year     = {2018},
  url      = {https://www.statista.com/statistics/270603/worldwide-number-of-hybrid-and-electric-vehicles-since-2009/},
  language = {en},
  urldate  = {2018-02-02},
  abstract = {How many electric cars are there in the world? This statistic shows the number of electric vehicles in the world 2012-2016. There were some 1.2 million electric vehicles in use globally in 2016.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/R7J57RL6/worldwide-number-of-hybrid-and-electric-vehicles-since-2009.html/\:text/html\:PDF:PDF},
  journal  = {Statista},
}

@WWW{department_of_infrastructure_and_regional_development_vehicle_2016,
  author    = {{Department of Infrastructure {and} Regional Development}},
  title     = {Vehicle emissions standards for cleaner air},
  year      = {2016},
  url       = {https://infrastructure.gov.au/roads/environment/forum/files/Vehicle_Noxious_Emissions_RIS.pdf},
  language  = {English},
  month     = dec,
  urldate   = {2018-02-02},
  publisher = {Australian Government},
}

@WWW{straubel_driving_2014,
  author   = {Straubel, JB},
  title    = {Driving {Range} for the {Model} {S} {Family}},
  year     = {2014},
  url      = {https://www.tesla.com/en_AU/blog/driving-range-model-s-family},
  month    = dec,
  urldate  = {2018-02-02},
  abstract = {As the Model S family has expanded over time it has become more relevant to compare range from one variant to another with a consistent set of assumptions so our customers can know what to expect and make the best decision to fit their needs. This can be a bit difficult since the background test methodology and standards from the US EPA are evolving over time. There are also many customer vehicle configuration choices, both before and after purchase, that can affect range as much as or more than the vehicle platform choice itself.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/NCJF3R3M/driving-range-model-s-family.html/\:text/html\:PDF:PDF},
  journal  = {Tesla Blog},
}

@WWW{international_electrotechnical_commission_iec_2017,
  author  = {{International Electrotechnical Commission}},
  title   = {{IEC} 61851-1:2017},
  year    = {2017},
  url     = {https://webstore.iec.ch/publication/33644},
  month   = feb,
  urldate = {2018-02-02},
  file    = {html\:PDF:html\:IEC 61851-1//\:2017 | IEC Webstore/\:C//\:/Users/Psykie/Zotero/storage/SP2JH8LH/33644.html/\:text/html\:PDF:PDF},
  journal = {IEC Webstore},
}

@WWW{autoware_autoware_nodate,
  author   = {Autoware},
  title    = {Autoware},
  year     = {2018},
  url      = {https://autoware.ai/},
  language = {en},
  urldate  = {2018-02-13},
  abstract = {Open-Source To Self-Driving},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\EP6AEXG9\\\\\\\\www.autoware.ai.html\\\:text/html\:PDF:PDF},
  journal  = {Autoware},
}

@WWW{baidu_apollo_nodate,
  author  = {Baidu},
  title   = {Apollo},
  year    = {2018},
  url     = {http://apollo.auto/},
  urldate = {2018-02-13},
  file    = {html\:PDF:html\:Apollo/\:C//\:/Users/Psykie/Zotero/storage/H8GMYCLX/apollo.auto.html/\:text/html\:PDF:PDF},
}

@InProceedings{ofjall_biologically_2014,
  author    = {Öfjäll, Kristoffer and Felsberg, Michael},
  title     = {Biologically Inspired Online Learning of Visual Autonomous Driving},
  booktitle = {Proceedings of the British Machine Vision Conference 2014},
  year      = {2014},
  publisher = {BMVA Press},
  doi       = {10.5244/C.28.94},
  editors   = {Valstar, Michel and French, Andrew and Pridmore, Tony},
}

@Book{buehler_darpa_2009,
  author    = {Buehler, Martin and Iagnemma, Karl and Singh, Sanjiv},
  title     = {The DARPA Urban Challenge: Autonomous Vehicles in City Traffic},
  year      = {2009},
  edition   = {1st},
  publisher = {Springer Publishing Company, Incorporated},
  isbn      = {3642039901, 9783642039904},
}

@InProceedings{abadi_tensorflow:_2016,
  author    = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title     = {TensorFlow: A System for Large-Scale Machine Learning},
  booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
  year      = {2016},
  publisher = {{USENIX} Association},
  isbn      = {978-1-931971-33-1},
  pages     = {265--283},
  url       = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
  address   = {Savannah, GA},
}

@Article{paalanen_feature_2006,
  author   = {Paalanen, Pekka and Kamarainen, Joni-Kristian and Ilonen, Jarmo and Kälviäinen, Heikki},
  title    = {Feature representation and discrimination based on {Gaussian} mixture model probability densities—{Practices} and algorithms},
  journal  = {Pattern Recognition},
  year     = {2006},
  volume   = {39},
  month    = jul,
  pages    = {1346--1358},
  issn     = {0031-3203},
  doi      = {10.1016/j.patcog.2006.01.005},
  abstract = {This study promotes the use of statistical methods in specific classification tasks since statistical methods have certain advantages which advocate their use in pattern recognition. One central problem in statistical methods is estimation of class conditional probability density functions based on examples in a training set. In this study maximum likelihood estimation methods for Gaussian mixture models are reviewed and discussed from a practical point of view. In addition, good practices for utilizing probability densities in feature classification and selection are discussed for Bayesian and, more importantly, for non-Bayesian tasks. As a result, the use of confidence information in the classification is proposed and a method for confidence estimation is presented. The propositions are tested experimentally.},
  annote   = {The following values have no corresponding Zotero field:number: 7},
  keywords = {Classifier, Confidence, EM, Gaussian mixture model, Highest density region},
}

@article{santana_learning_2016,
	title = {Learning a driving simulator},
	journal = {arXiv preprint arXiv:1608.01230},
	author = {Santana, Eder and Hotz, George},
	year = {2016}
}

@WWW{udacity_self-driving_2017,
  author = {Udacity},
  title  = {Self-{Driving} {Car} {Engineer} {Nanodegree}},
  year   = {2017},
  url    = {https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013},
  annote = {The following values have no corresponding Zotero field:secondary-title: Nanodegree Program},
}

@WWW{nica_bmw_2016,
  author = {Nica, Gabriel},
  title  = {{BMW} {CEO} {Wants} {Autonomous} {Driving} {Cars} within {Five} {Years}},
  year   = {2016},
  date   = {2016-08-02},
  url    = {https://www.bmwblog.com/2016/08/02/bmw-ceo-wants-autonomous-driving-cars-within-five-years/},
  month  = aug,
}

@WWW{etherington_ubers_2016,
  author = {Etherington, Darrell},
  title  = {Uber’s self-driving cars start picking up passengers in {San} {Francisco}},
  year   = {2016},
  date   = {2016-12-14},
  url    = {https://techcrunch.com/2016/12/14/ubers-self-driving-cars-start-picking-up-passengers-in-san-francisco/},
  month  = dec,
}

@WWW{waymo_waymo_2017,
  author = {Waymo},
  title  = {Waymo},
  year   = {2017},
  url    = {https://waymo.com/},
}

@WWW{shapiro_introducing_2016,
  author = {Shapiro, Danny},
  title  = {Introducing {Xavier}, the {NVIDIA} {AI} {Supercomputer} for the {Future} of {Autonomous} {Transportation}},
  year   = {2016},
  date   = {2016-09-28},
  url    = {https://blogs.nvidia.com/blog/2016/09/28/xavier/},
  month  = sep,
}

@WWW{mobileye_evolution_2017,
  author     = {Mobileye},
  title      = {The {Evolution} of {EyeQ}},
  year       = {2017},
  url        = {https://www.mobileye.com/our-technology/evolution-eyeq-chip/},
  shorttitle = {The {Evolution} of {EyeQ}},
}

@Article{l._c._chen_deeplab:_2017,
  author   = {Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L. Yuille},
  title    = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2018},
  volume   = {40},
  number   = {4},
  month    = apr,
  pages    = {834--848},
  issn     = {0162-8828},
  doi      = {10.1109/TPAMI.2017.2699184},
  keywords = {convolution;feature extraction;feedforward neural nets;image segmentation;learning (artificial intelligence);random processes;highlight convolution;atrous convolution;Deep Convolutional Neural Networks;atrous spatial pyramid pooling;image context;fully connected Conditional Random Field;PASCAL VOC-2012 semantic image segmentation task;deep convolutional nets;Deep Learning;DeepLab;semantic image segmentation;probabilistic graphical models;Convolution;Image segmentation;Semantics;Image resolution;Computational modeling;Neural networks;Context;Convolutional neural networks;semantic segmentation;atrous convolution;conditional random fields},
}

@InProceedings{treml_speeding_2016,
  author    = {Treml, Michael and Arjona-Medina, Jose and Unterthiner, Thomas and Durgesh, Rupesh and Friedmann, Felix and Schuberth, Peter and Mayr, Andreas and Heusel, Martin and Hofmarcher, Markus and Widrich, Michael and Nessler, Bernhard and Hochreiter, Sepp},
  title     = {Speeding up Semantic Segmentation for Autonomous Driving},
  booktitle = {MLITS, NIPS Workshop},
  year      = {2016},
  month     = dec,
}

@InProceedings{s._song_sun_2015,
  author    = {Shuran Song and Samuel P. Lichtenberg and Jianxiong Xiao},
  title     = {{SUN} {RGB}-{D}: {A} {RGB}-{D} scene understanding benchmark suite},
  booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1063-6919},
  pages     = {567--576},
  doi       = {10.1109/CVPR.2015.7298655},
  abstract  = {Although RGB-D sensors have enabled major break-throughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in high-level scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overfitting to a small testing set, and study cross-sensor bias.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {2D polygons, 3D annotations, 3D bounding boxes, 3D evaluation metrics, 3D reconstruction, 3D room layout, Benchmark testing, Cameras, cross-sensor bias, data-hungry algorithms, Estimation, high-level scene understanding, image colour analysis, image sensors, Iterative closest point algorithm, Layout, object orientations, PASCAL VOC, RGB-D images, RGB-D scene understanding benchmark suite, RGB-D sensors, scene category, Sensors, SUN RGB-D, Three-dimensional displays, vision tasks},
}

@InProceedings{k._he_deep_2016,
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title     = {Deep {Residual} {Learning} for {Image} {Recognition}},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year      = {2016},
  publisher = {{IEEE}},
  month     = jun,
  pages     = {770--778},
  doi       = {10.1109/CVPR.2016.90},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\&\#x00D7; deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords  = {Training, Visualization, image classification, Image segmentation, Neural networks, CIFAR-10, COCO object detection dataset, COCO segmentation, Complexity theory, deep residual learning, deep residual nets, deeper neural network training, Degradation, ILSVRC \& COCO 2015 competitions, ILSVRC 2015 classification task, image recognition, ImageNet dataset, ImageNet localization, ImageNet test set, learning (artificial intelligence), neural nets, object detection, residual function learning, residual nets, VGG nets, visual recognition tasks},
}

@article{simonyan_very_2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	journal = {arXiv preprint arXiv:1409.1556},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014}
}

@article{paszke_enet:_2016,
	title = {Enet: {A} deep neural network architecture for real-time semantic segmentation},
	journal = {arXiv preprint arXiv:1606.02147},
	author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
	year = {2016}
}

@article{garcia-garcia_review_2017,
	title = {A {Review} on {Deep} {Learning} {Techniques} {Applied} to {Semantic} {Segmentation}},
	journal = {arXiv preprint arXiv:1704.06857},
	author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
	year = {2017}
}

@InProceedings{cordts_cityscapes_2016,
  author    = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  title     = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  month     = jun,
}

@incollection{brostow_segmentation_2008,
	address = {Berlin, Heidelberg},
	title = {Segmentation and {Recognition} {Using} {Structure} from {Motion} {Point} {Clouds}},
	isbn = {978-3-540-88682-2},
	abstract = {We propose an algorithm for semantic segmentation based on 3D point clouds derived from ego-motion. We motivate five simple cues designed to model specific patterns of motion and 3D world structure that vary with object category. We introduce features that project the 3D cues back to the 2D image plane while modeling spatial layout and context. A randomized decision forest combines many such features to achieve a coherent 2D segmentation and recognize the object categories present. Our main contribution is to show how semantic segmentation is possible based solely on motion-derived 3D world structure. Our method works well on sparse, noisy point clouds, and unlike existing approaches, does not need appearance-based descriptors.},
	booktitle = {Computer {Vision} – {ECCV} 2008: 10th {European} {Conference} on {Computer} {Vision}, {Marseille}, {France}, {October} 12-18, 2008, {Proceedings}, {Part} {I}},
	publisher = {Springer Berlin Heidelberg},
	author = {Brostow, Gabriel J. and Shotton, Jamie and Fauqueur, Julien and Cipolla, Roberto},
	editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
	year = {2008},
	pages = {44--57},
	annote = {The following values have no corresponding Zotero field:label: Brostow2008electronic-resource-num: 10.1007/978-3-540-88682-2\_5}
}

@article{thoma_survey_2016,
	title = {A survey of semantic segmentation},
	journal = {arXiv preprint arXiv:1602.06541},
	author = {Thoma, Martin},
	year = {2016}
}

@InProceedings{d._kochanov_scene_2016,
  author    = {Deyvid Kochanov and Aljosa Osep and Jorg Stuckler and Bastian Leibe},
  title     = {Scene flow propagation for semantic mapping and object discovery in dynamic street scenes},
  booktitle = {2016 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  year      = {2016},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {1785--1792},
  doi       = {10.1109/IROS.2016.7759285},
  abstract  = {Scene understanding is an important prerequisite for vehicles and robots that operate autonomously in dynamic urban street scenes. For navigation and high-level behavior planning, the robots not only require a persistent 3D model of the static surroundings-equally important, they need to perceive and keep track of dynamic objects. In this paper, we propose a method that incrementally fuses stereo frame observations into temporally consistent semantic 3D maps. In contrast to previous work, our approach uses scene flow to propagate dynamic objects within the map. Our method provides a persistent 3D occupancy as well as semantic belief on static as well as moving objects. This allows for advanced reasoning on objects despite noisy single-frame observations and occlusions. We develop a novel approach to discover object instances based on the temporally consistent shape, appearance, motion, and semantic cues in our maps. We evaluate our approaches to dynamic semantic mapping and object discovery on the popular KITTI benchmark and demonstrate improved results compared to single-frame methods.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  keywords  = {Visualization, Semantics, Image segmentation, Three-dimensional displays, object detection, stereo image processing, 3D model, dynamic street scene, Dynamics, high-level behavior planning, image fusion, KITTI benchmark, moving object, navigation, object discovery, persistent 3D occupancy, Probabilistic logic, robots, scene flow propagation, scene understanding, semantic 3D maps, semantic mapping, solid modelling, static object, stereo frame observation fusion, Vehicle dynamics},
}

@InProceedings{d._a._chacra_road_2016,
  author    = {David Abou Chacra and John Zelek},
  title     = {Road {Segmentation} in {Street} {View} {Images} {Using} {Texture} {Information}},
  booktitle = {2016 13th Conference on Computer and Robot Vision ({CRV})},
  year      = {2016},
  publisher = {{IEEE}},
  month     = jun,
  pages     = {424--431},
  doi       = {10.1109/CRV.2016.47},
  abstract  = {Road segmentation is a problem encountered fairly frequently, especially in the framework of scene understanding and self-driving cars. On the flip side, there are several Street View databases that offer large amounts of useful data, which are still relatively untapped. In this paper we propose a road segmentation algorithm specifically aimed at segmenting roads from street view images. We use fisher vectors to encode small windows extracted from the main image at multiple scales, then classify these patches and use a voting scheme to get the final segmentation. We optionally utilize a spatial prior and superpixels to improve our segmentation. Our algorithm performs well and outputs a good segmentation for further use in road evaluation. We test our method on the KITTI road dataset and compare it to the state of the art.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2016 13th Conference on Computer and Robot Vision (CRV)},
  keywords  = {Roads, image classification, image resolution, image segmentation, Databases, Feature extraction, Fisher vectors, Gaussian window voting scheme, Image edge detection, image texture, KITTI road dataset, Object segmentation, road segmentation, Scene Parsing, spatial prior, Street View Data, Street View images, superpixels, support vector machines, SVM Classifier, texture information, Textures},
}

@InProceedings{oliveira_efficient_2016,
  author    = {Gabriel L. Oliveira and Wolfram Burgard and Thomas Brox},
  title     = {Efficient deep models for monocular road segmentation},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2016},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {4885--4891},
  doi       = {10.1109/IROS.2016.7759717},
  issn      = {2153-0866},
  keywords  = {convolution;image segmentation;intelligent transportation systems;neural nets;deep model;road scene segmentation;RGB image;semantic segmentation;convolutional neural network;CNN;intelligent transportation system;Roads;Image segmentation;Computer architecture;Training;Semantics;Robots;Runtime},
}

@InProceedings{caltagirone_fast_2017,
  author    = {Luca Caltagirone and Samuel Scheidegger and Lennart Svensson and Mattias Wahde},
  title     = {Fast LIDAR-based road detection using fully convolutional neural networks},
  booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
  year      = {2017},
  publisher = {{IEEE}},
  month     = jun,
  pages     = {1019--1024},
  doi       = {10.1109/IVS.2017.7995848},
  keywords  = {Roads;Three-dimensional displays;Computer architecture;Laser radar;Training;Microprocessors;Benchmark testing},
}

@InProceedings{j._yao_estimating_2015,
  author    = {Jian Yao and Srikumar Ramalingam and Yuichi Taguchi and Yohei Miki and Raquel Urtasun},
  title     = {Estimating {Drivable} {Collision}-{Free} {Space} from {Monocular} {Video}},
  booktitle = {2015 {IEEE} Winter Conference on Applications of Computer Vision},
  year      = {2015},
  publisher = {{IEEE}},
  month     = jan,
  isbn      = {1550-5790},
  pages     = {420--427},
  doi       = {10.1109/WACV.2015.62},
  abstract  = {In this paper we propose a novel algorithm for estimating the drivable collision-free space for autonomous navigation of on-road and on-water vehicles. In contrast to previous approaches that use stereo cameras or LIDAR, we show a method to solve this problem using a single camera. Inspired by the success of many vision algorithms that employ dynamic programming for efficient inference, we reduce the free space estimation task to an inference problem on a 1D graph, where each node represents a column in the image and its label denotes a position that separates the free space from the obstacles. Our algorithm exploits several image and geometric features based on edges, color, and homography to define potential functions on the 1D graph, whose parameters are learned through structured SVM. We show promising results on the challenging KITTI dataset as well as video collected from boats.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE Winter Conference on Applications of Computer Vision},
  keywords  = {mobile robots, road vehicles, Roads, Cameras, Estimation, image colour analysis, video signal processing, Image edge detection, support vector machines, path planning, autonomous navigation, Boats, drivable collision-free space estimation, geometric feature, image color, image edges, image feature, image homography, Inference algorithms, inference problem, KITTI dataset, marine vehicles, monocular video, Navigation, on-road vehicles, on-water vehicles, structured SVM, telerobotics},
}

@article{cord_automatic_2012,
	title = {Automatic {Road} {Defect} {Detection} by {Textural} {Pattern} {Recognition} {Based} on {AdaBoost}},
	volume = {27},
	issn = {1467-8667},
	doi = {10.1111/j.1467-8667.2011.00736.x},
	abstract = {Abstract: The state of roads is continuously degrading due to meteorological conditions, ground movements, and traffic, leading to the formation of defects, such as grabbing, holes, and cracks. In this article, a method to automatically distinguish images of road surfaces with defects from road surfaces without defects is presented. This method, based on supervised learning, is generic and may be applied to all type of defects present in those images. They typically present strong textural information with patterns that show fluctuations at small scales and some uniformity at larger scales. The textural information is described by applying a large set of linear and nonlinear filters. To select the most pertinent ones for the current application, a supervised learning based on AdaBoost is performed. The whole process is tested both on a textural recognition task based on the VisTex image database and on road images collected by a dedicated road imaging system. A comparison with a recent cracks detection algorithm from Oliveira and Correia demonstrates the proposed method's efficiency.},
	journal = {Computer-Aided Civil and Infrastructure Engineering},
	author = {Cord, Aurélien and Chambon, Sylvie},
	year = {2012},
	pages = {244--259},
	annote = {The following values have no corresponding Zotero field:number: 4publisher: Blackwell Publishing Inc}
}

@InProceedings{jian_unstructured_2009,
  author    = {Jian Wang and Zhong Ji and Yu-Ting Su},
  title     = {Unstructured road detection using hybrid features},
  booktitle = {2009 International Conference on Machine Learning and Cybernetics},
  year      = {2009},
  volume    = {1},
  publisher = {{IEEE}},
  month     = jul,
  isbn      = {2160-133X},
  pages     = {482--486},
  doi       = {10.1109/ICMLC.2009.5212506},
  abstract  = {Road detection is a key step of the autonomous guided vehicle system such as road following. In this paper, a novel unstructured road detection method is proposed. First, white balance and gray level stretch technique are adopted to enhance image performance. Then, a small overlapped sliding window is scanned over the frame from which hybrid features are extracted. Next, a SVM-based classifier is employed to distinguish the road area from background. At last, the morphological operation and moving average filter technology are performed to obtain precise location of the road region. The proposed algorithm has been evaluated by different type of unstructured roads and the experimental results show its effectiveness.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2009 International Conference on Machine Learning and Cybernetics},
  keywords  = {Roads, image classification, Feature extraction, support vector machines, Autonomous guided vehicle, autonomous guided vehicle system, Computer vision, Cybernetics, gray level stretch technique, hybrid features, image enhancement, image performance enhancement, Machine learning, Mobile robots, Remotely operated vehicles, road following, road traffic, Shape, support vector machines-based classifier, SVM, SVM-based classifier, unstructured road detection, Vehicle detection, white balance technique},
}

@InProceedings{m._foedisch_adaptive_2004,
  author    = {M. Foedisch and A. Takeuchi},
  title     = {Adaptive real-time road detection using neural networks},
  booktitle = {Proceedings. The 7th International {IEEE} Conference on Intelligent Transportation Systems ({IEEE} Cat. No.04TH8749)},
  year      = {2004},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {167--172},
  doi       = {10.1109/ITSC.2004.1398891},
  abstract  = {We have developed an adaptive real-time road detection application based on neural networks for autonomous driving. By taking advantage of the unique structure in road images, the network training can be processed while the system is running. The algorithm employs color features derived from color histograms. We have focused on the automatic adaptation of the system, which has reduced manual road annotations by human.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Proceedings. The 7th International IEEE Conference on Intelligent Transportation Systems (IEEE Cat. No.04TH8749)},
  keywords  = {autonomous driving, road vehicles, Roads, feature extraction, image colour analysis, learning (artificial intelligence), neural nets, object detection, adaptive real time road detection, Adaptive systems, automatic adaptation system, color feature algorithm, color histograms, Focusing, Histograms, Intelligent networks, Intelligent systems, manual road annotations reduction, network training, neural networks, NIST, Real time systems, road images},
}

@InProceedings{shengyan_self-supervised_2010,
  author    = {Shengyan Zhou and K Iagnemma},
  title     = {Self-supervised learning method for unstructured road detection using {Fuzzy} {Support} {Vector} {Machines}},
  booktitle = {2010 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  year      = {2010},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0858},
  pages     = {1183--1189},
  doi       = {10.1109/IROS.2010.5650300},
  abstract  = {Road detection is a crucial problem in the application of autonomous vehicle and on-road mobile robot. Most of the recent methods only achieve reliable results in some particular well-arranged environments. In this paper, we describe a road detection algorithm for front-view monocular camera using road probabilistic distribution model (RPDM) and online learning method. The primary contribution of this paper is that the combination of dynamical RPDM and Fuzzy Support Vector Machines (FSVMs) makes the algorithm being capable of self-supervised learning and optimized learning from the inheritance of previous result. The secondary contribution of this paper is that the proposed algorithm uses road geometrical assumption to extract assumption based misclassified points and retrains itself online which makes it easier to find potential misclassified points. Those points take an important role in online retraining the classifier which makes the algorithm adaptive to environment changing.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  keywords  = {mobile robots, robot vision, learning (artificial intelligence), object detection, support vector machines, autonomous vehicle, front-view monocular camera, fuzzy set theory, fuzzy support vector machines, geometry, on-road mobile robot, online learning method, probability, road probabilistic distribution model, self-supervised learning method, unstructured road detection algorithm},
}

@InProceedings{p._conrad_performance_2003,
  author    = {P. Conrad and M. Foedisch},
  title     = {Performance evaluation of color based road detection using neural nets and support vector machines},
  booktitle = {32nd Applied Imagery Pattern Recognition Workshop, 2003. Proceedings.},
  year      = {2003},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {157--160},
  doi       = {10.1109/AIPR.2003.1284265},
  abstract  = {We present a comparison of two methods for color based road segmentation. The first was implemented using a neural network, while the second approach is based on support vector machines. A large number of training images were used with varying road conditions including roads with snow, dirt or gravel surfaces, and asphalt. We experimented with grouping the training images by road condition and generating a separate model for each group. The system would automatically select the appropriate one for each novel image. Those results were compared with creating a single model with all images. In another set of experiments, we added the image coordinates of each point as an additional feature in the models. Finally, we compared the results and the efficiency of neural networks and support vector machines of segmentation with each combination of feature sets and image groups.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 32nd Applied Imagery Pattern Recognition Workshop, 2003. Proceedings.},
  keywords  = {image segmentation, Neural networks, learning (artificial intelligence), neural nets, object detection, support vector machines, Remotely operated vehicles, Histograms, asphalt, color based road detection, color based road segmentation, dirt surface, feature sets, gravel surfaces, image coordinates, image groups, neural network, performance evaluation, Pixel, road condition, Road vehicles, Snow, snow surfaces, training images, Vehicle driving, Vehicle safety},
}

@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {1573-1405},
	doi = {10.1023/b:visi.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	year = {2004},
	pages = {91--110},
	annote = {The following values have no corresponding Zotero field:number: 2label: Lowe2004work-type: journal article},
	annote = {The following values have no corresponding Zotero field:number: 2label: Lowe2004work-type: journal article}
}

@InProceedings{u._lee_eurecar_2016,
  author    = {Unghui Lee and Jiwon Jung and Seunghak Shin and Yongseop Jeong and Kibaek Park and David Hyunchul Shim and In-so Kweon},
  title     = {{EureCar} turbo: {A} self-driving car that can handle adverse weather conditions},
  booktitle = {2016 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  year      = {2016},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {2301--2306},
  doi       = {10.1109/IROS.2016.7759359},
  abstract  = {Autonomous driving technology has made significant advances in recent years. In order for self-driving cars to become practical, they are required to operate safely and reliably even under adverse driving conditions. However, most current autonomous driving cars have only been shown to be operational under amiable weather conditions, i.e., on sunny days on dry roads. In order to enable autonomous cars to handle adverse driving conditions such as rain and wet roads, the algorithm must be able to detect roads within a tolerable margin of error using sensors such as cameras and laser scanners. In this paper, we propose a sensor fusion algorithms that is able to operate under a variety of weather conditions, including rain. Our algorithm was validated when a strong shower occurred during the 2014 Hyundai Motor Company's Autonomous Car Competition. In this paper, we present the competition results that were collected on the same course on both sunny and rainy days. Based on the comparison, we propose the future directions to improve the autonomous driving capability under adverse environmental conditions.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  keywords  = {road vehicles, Roads, Cameras, Sensors, autonomous driving cars, autonomous driving technology, EureCar turbo, handle adverse weather conditions, Hyundai motor company autonomous car competition, Laser radar, Lasers, Meteorology, road detection, road safety, self driving car, sensor fusion algorithms, Vehicles},
}

@InProceedings{h._cho_multi-sensor_2014,
  author    = {Hyunggi Cho and Young-Woo Seo and B.V.K. Vijaya Kumar and Ragunathan Raj Rajkumar},
  title     = {A multi-sensor fusion system for moving object detection and tracking in urban driving environments},
  booktitle = {2014 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  year      = {2014},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {1836--1843},
  doi       = {10.1109/ICRA.2014.6907100},
  abstract  = {A self-driving car, to be deployed in real-world driving environments, must be capable of reliably detecting and effectively tracking of nearby moving objects. This paper presents our new, moving object detection and tracking system that extends and improves our earlier system used for the 2007 DARPA Urban Challenge. We revised our earlier motion and observation models for active sensors (i.e., radars and LIDARs) and introduced a vision sensor. In the new system, the vision module detects pedestrians, bicyclists, and vehicles to generate corresponding vision targets. Our system utilizes this visual recognition information to improve a tracking model selection, data association, and movement classification of our earlier system. Through the test using the data log of actual driving, we demonstrate the improvement and performance gain of our new tracking system.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2014 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords  = {mobile robots, image classification, Cameras, object detection, image fusion, Laser radar, Vehicles, automobiles, autonomous car, DARPA Urban Challenge, data association, feature selection, movement classification, moving object detection, moving object tracking, multisensor fusion system, object recognition, object tracking, Radar tracking, Sensor phenomena and characterization, tracking model selection, urban driving environments, vision sensor, visual recognition information},
}

@WWW{raspberry_pi_foundation_raspberry_2017,
  author = {{Raspberry Pi Foundation}},
  title  = {Raspberry {Pi} 3 {Model} {B}},
  year   = {2017},
  url    = {https://www.raspberrypi.org/products/raspberry-pi-3-model-b/},
}

@InProceedings{jia_caffe:_2014,
  author    = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  title     = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
  year      = {2014},
  series    = {MM '14},
  publisher = {ACM},
  location  = {Orlando, Florida, USA},
  isbn      = {978-1-4503-3063-3},
  pages     = {675--678},
  doi       = {10.1145/2647868.2654889},
  acmid     = {2654889},
  address   = {New York, NY, USA},
  keywords  = {computer vision, machine learning, neural networks, open source, parallel computation},
  numpages  = {4},
}

@InProceedings{j._sock_probabilistic_2016,
  author    = {Juil Sock and Jun Kim and Jihong Min and Kiho Kwak},
  title     = {Probabilistic traversability map generation using 3D-{LIDAR} and camera},
  booktitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  year      = {2016},
  publisher = {{IEEE}},
  month     = may,
  pages     = {5631--5637},
  doi       = {10.1109/ICRA.2016.7487782},
  abstract  = {Estimating the traversability of rough terrain is a critical task for an outdoor mobile robot. While classifying structured environment can be learned from large number of training data, it is an extremely difficult task to learn and estimate traversability of unstructured rough terrain. Moreover, in many cases information from a single sensor may not be sufficient for estimating traversability reliably in the absence of artificial landmarks such as lane markings or curbs. Our approach estimates traversability of the terrain and build a 2D probabilistic grid map online using 3D-LIDAR and camera. The combination of LIDAR and camera is favoured in many robotic application because they provide complementary information. Our approach assumes the data captured by these two sensors are independent and build separate traversability maps, each with information captured from one sensor. Traversability estimation with vision sensor autonomously collects training data and update classifier without human intervention as the vehicle traverse the terrain. Traversability estimation with 3D-LIDAR measures the slopes of the ground to predict the traversability. Two independently built probabilistic maps are fused using Bayes' rule to improve the detection performance. This is in contrast with other methods in which each sensor performs different tasks. We have implemented the algorithm on a UGV(Unmanned Ground Vehicle) and tested our approach on a rough terrain to evaluate the detection performance.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords  = {mobile robots, Roads, Probabilistic logic, Laser radar, Vehicles, vision sensor, 2D probabilistic grid map online, 3D-LIDAR, camera, cameras, lane markings, optical radar, outdoor mobile robot, probabilistic traversability map generation, radar imaging, Robot sensing systems, rough terrain, traversability estimation, unmanned ground vehicle},
}

@InProceedings{otterness_evaluation_nodate,
  author    = {Nathan Otterness and Ming Yang and Sarah Rust and Eunbyung Park and James H. Anderson and F. Donelson Smith and Alex Berg and Shige Wang},
  title     = {An Evaluation of the NVIDIA TX1 for Supporting Real-Time Computer-Vision Workloads},
  booktitle = {2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)},
  year      = {2017},
  publisher = {{IEEE}},
  month     = apr,
  pages     = {353--364},
  doi       = {10.1109/RTAS.2017.3},
  keywords  = {graphics processing units;microcomputers;mobile robots;multiprocessing systems;real-time systems;robot vision;traffic engineering computing;NVIDIA TX1;computer-vision workloads;autonomous vehicles;safety-critical real-time systems;graphics processing units;size weight and power limits;SWaP limits;multicore platforms;multicore+GPU solution;Graphics processing units;Real-time systems;Kernel;Benchmark testing;Random access memory;Hardware;Multicore processing},
}

@WWW{djtobias_cherry_2017,
  author = {DJTobias},
  title  = {Cherry {Autonomous} {Racecar}},
  year   = {2017},
  url    = {https://github.com/DJTobias/Cherry-Autonomous-Racecar},
}

@WWW{kangalow_anthony_2016,
  author = {kangalow},
  title  = {Anthony {Virtuoso}’s {Jetson} {TX}1 {Rover} {Project}},
  year   = {2016},
  date   = {2016-08-07},
  url    = {https://www.jetsonhacks.com/2016/08/07/anthony-virtuosos-jetson-tx1-rover-project/},
}

@article{rezaei_vision-based_2017,
	title = {Vision-{Based} {Driver}-{Assistance} {Systems}},
	issn = {978-3-319-50551-0},
	doi = {10.1007/978-3-319-50551-0_1},
	journal = {Computer Vision for Driver Assistance: Simultaneous Traffic and Driver Monitoring},
	author = {Rezaei, Mahdi and Klette, Reinhard},
	year = {2017},
	pages = {1--18},
	annote = {The following values have no corresponding Zotero field:pub-location: Champublisher: Springer International Publishinglabel: Rezaei2017}
}

@WWW{ford_motor_company_ford_2016,
  author = {{Ford Motor Company}},
  title  = {Ford {Debuts} {Next}-{Generation} {Fusion} {Hybrid} {Autonomous} {Development} {Vehicle}},
  year   = {2016},
  date   = {2016-12-28},
  url    = {https://www.businesswire.com/news/home/20161227005301/en/Ford-Debuts-Next-Generation-Fusion-Hybrid-Autonomous-Development},
  month  = dec,
}

@WWW{tesla_motors_autopilot_2017,
  author = {{Tesla Motors}},
  title  = {Autopilot},
  year   = {2019},
  url    = {https://www.tesla.com/en_AU/autopilot},
}

@article{harris_documents_2015,
	title = {Documents confirm {Apple} is building self-driving car},
	volume = {14},
	journal = {The Guardian},
	author = {Harris, Mark},
	year = {2015}
}

@article{bojarski_end_2016,
	title = {End to end learning for self-driving cars},
	journal = {arXiv preprint arXiv:1604.07316},
	author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai},
	year = {2016}
}

@article{markoff_google_2010,
	title = {Google cars drive themselves, in traffic},
	volume = {10},
	journal = {The New York Times},
	author = {Markoff, John},
	year = {2010},
	pages = {9},
	annote = {The following values have no corresponding Zotero field:number: A1}
}

@article{gudigar_review_2016,
	title = {A review on automatic detection and recognition of traffic sign},
	volume = {75},
	issn = {1573-7721},
	doi = {10.1007/s11042-014-2293-7},
	abstract = {Evidently, Intelligent Transport System (ITS) has progressed tremendously all its way. The core of ITS are detection and recognition of traffic sign, which are designated to fulfill safety and comfort needs of driver. This paper provides a critical review on three major steps in Automatic Traffic Sign Detection and Recognition(ATSDR) system i.e., segmentation, detection and recognition in the context of vision based driver assistance system. In addition, it focuses on different experimental setups of image acquisition system. Further, discussion on possible future research challenges is made to make ATSDR more efficient, which inturn produce a wide range of opportunities for the researchers to carry out the detailed analysis of ATSDR and to incorporate the future aspects in their research.},
	journal = {Multimedia Tools and Applications},
	author = {Gudigar, Anjan and Chokkadi, Shreesha and U, Raghavendra},
	year = {2016},
	pages = {333--364},
	annote = {The following values have no corresponding Zotero field:number: 1label: Gudigar2016work-type: journal article}
}

@InProceedings{f._zhang_vehicle_2014,
  author    = {Feihu Zhang and Daniel Clarke and Alois Knoll},
  title     = {Vehicle detection based on {LiDAR} and camera fusion},
  booktitle = {17th International {IEEE} Conference on Intelligent Transportation Systems ({ITSC})},
  year      = {2014},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0009},
  pages     = {1620--1625},
  doi       = {10.1109/ITSC.2014.6957925},
  abstract  = {Vehicle detection is important for advanced driver assistance systems (ADAS). Both LiDAR and cameras are often used. LiDAR provides excellent range information but with limits to object identification; on the other hand, the camera allows for better recognition but with limits to the high resolution range information. This paper presents a sensor fusion based vehicle detection approach by fusing information from both LiDAR and cameras. The proposed approach is based on two components: a hypothesis generation phase to generate positions that potential represent vehicles and a hypothesis verification phase to classify the corresponding objects. Hypothesis generation is achieved using the stereo camera while verification is achieved using the LiDAR. The main contribution is that the complementary advantages of two sensors are utilized, with the goal of vehicle detection. The proposed approach leads to an enhanced detection performance; in addition, maintains tolerable false alarm rates compared to vision based classifiers. Experimental results suggest a performance which is broadly comparable to the current state of the art, albeit with reduced false alarm rate.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  keywords  = {Cameras, Feature extraction, Shape, Laser radar, Vehicles, optical radar, ADAS, advanced driver assistance systems, camera fusion, false alarm rates, hypothesis generation phase, hypothesis verification phase, intelligent transportation systems, LiDAR, range information, road vehicle radar, sensor fusion, stereo camera, Support vector machines, vehicle detection},
}

@InProceedings{y._alkhorshid_road_2016,
  author    = {Yasamin Alkhorshid and Kamelia Aryafar and Sven Bauer and Gerd Wanielik},
  title     = {Road {Detection} through {Supervised} {Classification}},
  booktitle = {2016 15th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
  year      = {2016},
  publisher = {{IEEE}},
  month     = dec,
  pages     = {806--809},
  doi       = {10.1109/ICMLA.2016.0144},
  abstract  = {Autonomous driving is a rapidly evolving technology. Autonomous vehicles are capable of sensing their environment and navigating without human input through sensory information such as radar, Lidar, GNSS, vehicle odometry, and computer vision. This sensory input provides a rich dataset that can be used in combination with machine learning models to tackle multiple problems in supervised settings. In this paper we focus on road detection through gray-scale images as the sole sensory input. Our contributions are twofold: first, we introduce an annotated dataset of urban roads for machine learning tasks, second, we introduce a baseline road detection on this dataset through supervised classification and hand-crafted feature vectors.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  keywords  = {Automobiles, autonomous driving, Roads, traffic engineering computing, Training, feature extraction, image classification, Benchmark testing, Cameras, Sensors, learning (artificial intelligence), road traffic, road detection, annotated dataset, autonomous vehicles, baseline road detection, computer vision, dataset, GNSS, gray-scale images, hand-crafted feature vectors, lidar, machine learning, machine learning models, radar, semantic annotation, sole sensory input, supervised classification, vehicle odometry},
}

@article{j._gillula_how_2006,
	title = {How to teach a van to drive: an undergraduate perspective on the 2005 {DARPA} grand challenge},
	volume = {26},
	issn = {1066-033X},
	doi = {10.1109/MCS.2006.1636306},
	abstract = {This paper describes how a team of undergraduate volunteers from California Institute of Technology (Caltech) developed a robotic vehicle that can navigate completely autonomously through the Mojave Desert. Called Alice, the vehicle was Caltech's entry to the 2005 DARPA Grand Challenge which aimed to generate the technology needed to build and program an unmanned ground vehicle through 130 miles of difficult terrain completely autonomously in under ten hours. Although Alice failed to win the competition, she did succeed in her original purpose of teaching a new generation of students about engineering, how to apply theory to the real world, how to debug and deal with shortcomings and schedules, and most importantly, how to work as a team on a complex problem.},
	journal = {IEEE Control Systems},
	author = {J. Gillula and J. Leibs},
	year = {2006},
	keywords = {mobile robots, road vehicles, autonomous navigation, Remotely operated vehicles, Real time systems, Vehicle driving, unmanned ground vehicle, 2005 DARPA Grand Challenge, Alice robotic vehicle, California Institute of Technology, control engineering education, Drag, Mojave Desert, Path planning, Sun, Switches, Tires, Wheels},
	pages = {19--26},
	annote = {The following values have no corresponding Zotero field:number: 3}
}

@InProceedings{hao_path_2014,
  author    = {Hao Zhu and Mengyin Fu and Yi Yang and Xinyu Wang and Meiling Wang},
  title     = {A path planning algorithm based on fusing lane and obstacle map},
  booktitle = {17th International {IEEE} Conference on Intelligent Transportation Systems ({ITSC})},
  year      = {2014},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0009},
  pages     = {1442--1448},
  doi       = {10.1109/ITSC.2014.6957889},
  abstract  = {This paper proposes a path planning algorithm for autonomous driving in urban environments. The processing of video and Velodyne pointcloud provides information about the positions of lane markers and obstacles in the local map, which are then converted to a lane costmap and obstacle costmap. The referenced GIS follow line is used for generating a series of offset curves, and the best follow line is selected according to a combination of lane, obstacle and background cost. Additional handling of planning path and maximum speed is provided. Our planning algorithm can handle various road types such as U-turn, intersections, and different driving behaviors including passing over or following front vehicles, etc. The proposed navigation framework is implemented on an autonomous vehicle, which exhibits good performance on Future Challenge 2013, Changshu, China.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  keywords  = {autonomous driving, mobile robots, road vehicles, Roads, robot vision, Cameras, video signal processing, image fusion, autonomous vehicle, path planning, Laser radar, Vehicles, following front vehicles, fusing lane, geographic information systems, lane costmap, obstacle costmap, obstacle map, path planning algorithm, Planning, referenced GIS follow line, Turning, urban environments, Velodyne pointcloud, video processing},
}

@InProceedings{s._kuthirummal_graph_2011,
  author    = {Sujit Kuthirummal and Aveek Das and Supun Samarasekera},
  title     = {A graph traversal based algorithm for obstacle detection using lidar or stereo},
  booktitle = {2011 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  year      = {2011},
  publisher = {{IEEE}},
  month     = sep,
  isbn      = {2153-0858},
  pages     = {3874--3880},
  doi       = {10.1109/IROS.2011.6094685},
  abstract  = {We present a novel computationally efficient approach to obstacle detection that is applicable to both structured (e.g. indoor, road) and unstructured (e.g. off-road, grassy terrain) environments. In contrast to previous works that attempt to explicitly identify obstacles, we explicitly detect scene regions that are traversable - safe for the robot to go to - from its current position. Traversability is defined on a 2D grid of cells. Given 3D points, we map them to individual cells and compute histograms of elevations of the points in each cell. This elevation information is then used in a graph based algorithm to label all traversable cells. In this manner, positive and negative obstacles, as well as unknown regions are implicitly detected and avoided. Our notion of traversability does not make any flat-world assumptions and does not need sensor pitch-roll compensation. It also accounts for overhanging structures like tree branches. We demonstrate that our approach can be used with both lidar and stereo sensors even though the two sensors differ in their resolution and accuracy. We present several results from our real-time implementation on realistic environments using both lidar and stereo.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  keywords  = {Cameras, Histograms, Laser radar, Robot sensing systems, Robot kinematics, Three dimensional displays},
}

@article{montemerlo_junior:_2008,
	title = {Junior: {The} {Stanford} entry in the {Urban} {Challenge}},
	volume = {25},
	issn = {1556-4967},
	doi = {10.1002/rob.20258},
	abstract = {This article presents the architecture of Junior, a robotic vehicle capable of navigating urban environments autonomously. In doing so, the vehicle is able to select its own routes, perceive and interact with other traffic, and execute various urban driving skills including lane changes, U-turns, parking, and merging into moving traffic. The vehicle successfully finished and won second place in the DARPA Urban Challenge, a robot competition organized by the U.S. Government. © 2008 Wiley Periodicals, Inc.},
	journal = {Journal of Field Robotics},
	author = {Montemerlo, Michael and Becker, Jan and Bhat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
	year = {2008},
	pages = {569--597},
	annote = {The following values have no corresponding Zotero field:number: 9publisher: Wiley Subscription Services, Inc., A Wiley Company}
}

@article{thrun_stanley:_2006,
	title = {Stanley: {The} robot that won the {DARPA} {Grand} {Challenge}},
	volume = {23},
	issn = {1556-4967},
	doi = {10.1002/rob.20147},
	abstract = {This article describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley was developed for high-speed desert driving without manual intervention. The robot's software system relied predominately on state-of-the-art artificial intelligence technologies, such as machine learning and probabilistic reasoning. This paper describes the major components of this architecture, and discusses the results of the Grand Challenge race. © 2006 Wiley Periodicals, Inc.},
	journal = {Journal of Field Robotics},
	author = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Stavens, David and Aron, Andrei and Diebel, James and Fong, Philip and Gale, John and Halpenny, Morgan and Hoffmann, Gabriel and Lau, Kenny and Oakley, Celia and Palatucci, Mark and Pratt, Vaughan and Stang, Pascal and Strohband, Sven and Dupont, Cedric and Jendrossek, Lars-Erik and Koelen, Christian and Markey, Charles and Rummel, Carlo and van Niekerk, Joe and Jensen, Eric and Alessandrini, Philippe and Bradski, Gary and Davies, Bob and Ettinger, Scott and Kaehler, Adrian and Nefian, Ara and Mahoney, Pamela},
	year = {2006},
	pages = {661--692},
	annote = {The following values have no corresponding Zotero field:number: 9publisher: Wiley Subscription Services, Inc., A Wiley Company}
}

@InProceedings{d._m._cole_using_2006,
  author    = {D. M. Cole and P. M. Newman},
  title     = {Using laser range data for 3D {SLAM} in outdoor environments},
  booktitle = {Proceedings 2006 {IEEE} International Conference on Robotics and Automation, 2006. {ICRA} 2006.},
  year      = {2006},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {1556--1563},
  doi       = {10.1109/ROBOT.2006.1641929},
  abstract  = {Traditional simultaneous localization and mapping (SLAM) algorithms have been used to great effect in flat, indoor environments such as corridors and offices. We demonstrate that with a few augmentations, existing 2D SLAM technology can be extended to perform full 3D SLAM in less benign, outdoor, undulating environments. In particular, we use data acquired with a 3D laser range finder. We use a simple segmentation algorithm to separate the data stream into distinct point clouds, each referenced to a vehicle position. The SLAM technique we then adopt inherits much from 2D delayed state (or scan-matching) SLAM in that the state vector is an ever growing stack of past vehicle positions and inter-scan registrations are used to form measurements between them. The registration algorithm used is a novel combination of previous techniques carefully balancing the need for maximally wide convergence basins, robustness and speed. In addition, we introduce a novel post-registration classification technique to detect matches which have converged to incorrect local minima},
  annote    = {The following values have no corresponding Zotero field:alt-title: Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
  keywords  = {Convergence, mobile robots, robot vision, Vehicles, 3D laser range finder, 3D SLAM, Clouds, Data engineering, Delay, Indoor environments, laser range data, laser ranging, Laser theory, mobile robotics, Position measurement, registration algorithm, segmentation algorithm, Simultaneous localization and mapping, simultaneous localization and mapping algorithms},
}

@InProceedings{mengyin_multiple_2014,
  author    = {Mengyin Fu and Hao Zhu and Yi Yang and Meiling Wang and Yu Li},
  title     = {Multiple map representations for vehicle localization and scene reconstruction},
  booktitle = {17th International {IEEE} Conference on Intelligent Transportation Systems ({ITSC})},
  year      = {2014},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0009},
  pages     = {2241--2242},
  doi       = {10.1109/ITSC.2014.6958036},
  abstract  = {This paper proposes a RBPF SLAM algorithm for localization map building, and the SLAM result can be used for fusing multiple 3D photorealistic local maps of the environment. For the localization map, a heightmap representation based on the Velodyne pointcloud is presented, and a RBPF SLAM algorithm is established for precise mapping and localization. For the scene reconstruction map, the Velodyne LIDAR and Ladybug3 omnidirectional camera are fused to provide a photorealistic 3D representation of the scene.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  keywords  = {3D photorealistic local maps, Cameras, Global Positioning System, height map representation, image reconstruction, image representation, Ladybug3 omnidirectional camera, Laser radar, light detection and ranging, localization map building, multiple map representation, photorealistic 3D scene representation, RBPF SLAM algorithm, scene reconstruction, simultaneous localization and mapping, SLAM (robots), Three-dimensional displays, traffic engineering computing, Transforms, vehicle localization, Vehicles, Velodyne LIDAR, Velodyne point cloud},
}

@Book{lavalle_planning_2006,
  author    = {LaValle, Steven M.},
  title     = {Planning Algorithms},
  year      = {2006},
  publisher = {Cambridge University Press},
  doi       = {10.1017/CBO9780511546877},
  place     = {Cambridge},
}

@InProceedings{p._y._shinzato_road_2014,
  author    = {Patrick Y. Shinzato and Diego Gomes and Denis F. Wolf},
  title     = {Road estimation with sparse 3D points from stereo data},
  booktitle = {17th International {IEEE} Conference on Intelligent Transportation Systems ({ITSC})},
  year      = {2014},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0009},
  pages     = {1688--1693},
  doi       = {10.1109/ITSC.2014.6957936},
  abstract  = {Obstacle detection is a fundamental task for Advanced Driver Assistance Systems (ADAS) and Self-driving cars. Several commercial systems like Adaptive Cruise Controls and Collision Warning Systems depend on them to notify the driver about a risky situation. Several approaches have been presented in the literature in the last years. However, most of them are limited to specific scenarios and restricted conditions. In this paper we propose a fast obstacle estimation to stereo cameras followed by a robust road estimation method. Our approach uses only disparity maps and works with a low number of pixels instead of the entire image. Experimental tests have been carried out in different conditions using the standard ROAD-KITTI benchmark, obtaining positive results.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  keywords  = {control engineering computing, Roads, Training, Benchmark testing, Estimation, image sensors, stereo image processing, Image edge detection, driver information systems, Histograms, automobiles, ADAS, advanced driver assistance systems, collision avoidance, adaptive cruise controls, collision warning systems, disparity maps, fast obstacle estimation, Mathematical model, obstacle detection, risky situation, road traffic control, robust road estimation method, self-driving cars, sparse 3D points, standard ROAD-KITTI benchmark, stereo cameras, stereo data},
}

@InProceedings{c._oh_illumination_2012,
  author    = {Changbeom Oh and Jongin Son and Kwanghoon Sohn},
  title     = {Illumination robust road detection using geometric information},
  booktitle = {2012 15th International {IEEE} Conference on Intelligent Transportation Systems},
  year      = {2012},
  publisher = {{IEEE}},
  month     = sep,
  isbn      = {2153-0009},
  pages     = {1566--1571},
  doi       = {10.1109/ITSC.2012.6338790},
  abstract  = {Road detection is an important task in intelligent transportation system (ITS). Over the past few decades, several vision-based approaches for road detection have been proposed and most of them are based on color information. However, color information may result in false road detection under variation of illumination conditions. To deal with illumination problems, we propose an illumination invariant road detection method using geometric information. By incorporating geometric information with a color-based road probability map, the proposed method robustly detect road regions on real scene containing variation of illumination such as shadow and mixed artificial lights. Experimental results show that the proposed method outperforms the conventional methods.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2012 15th International IEEE Conference on Intelligent Transportation Systems},
  keywords  = {color information, color-based road probability map, computational geometry, false road detection, geometric information, illumination invariant road detection, illumination robust road detection, Image color analysis, image colour analysis, Image edge detection, Image segmentation, intelligent transportation system, ITS, Lighting, mixed artificial lights, object detection, probability, Roads, Robustness, traffic engineering computing, vision-based approaches},
}

@InProceedings{j._siegemund_temporal_2011,
  author    = {J. Siegemund and U. Franke and W. Förstner},
  title     = {A temporal filter approach for detection and reconstruction of curbs and road surfaces based on {Conditional} {Random} {Fields}},
  booktitle = {2011 {IEEE} Intelligent Vehicles Symposium ({IV})},
  year      = {2011},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {637--642},
  doi       = {10.1109/IVS.2011.5940447},
  abstract  = {A temporal filter approach for real-time detection and reconstruction of curbs and road surfaces from 3D point clouds is presented. Instead of local thresholding, as used in many other approaches, a 3D curb model is extracted from the point cloud. The 3D points are classified to different parts of the model (i.e. road and sidewalk) using a temporally integrated Conditional Random Field (CRF). The parameters of curb and road surface are then estimated from the respectively assigned points, providing a temporal connection via a Kalman filter.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2011 IEEE Intelligent Vehicles Symposium (IV)},
  keywords  = {traffic engineering computing, Computational modeling, Cameras, Estimation, object detection, solid modelling, road traffic, Three dimensional displays, image reconstruction, Kalman filters, Kalman filter, 3D curb model, conditional random fields, curb detection, curb reconstruction, Labeling, road surface detection, road surface reconstruction, Surface reconstruction, temporal filter approach},
}

@Article{t._drage_integration_2014,
  author   = {T. Drage and J. Kalinowski and T. Bräunl},
  title    = {Integration of {Drive}-by-{Wire} with {Navigation} {Control} for a {Driverless} {Electric} {Race} {Car}},
  journal  = {IEEE Intelligent Transportation Systems Magazine},
  year     = {2014},
  volume   = {6},
  pages    = {23--33},
  issn     = {1939-1390},
  doi      = {10.1109/MITS.2014.2327160},
  abstract = {This article presents the design and implementation of a drive-by-wire system and a navigation control system for an autonomous Formula SAE race car. The result is the development of a platform for research into autonomous driving which can be easily replicated. Drive-by-wire actuators for acceleration, braking and steering of the vehicle are discussed, as well as the embedded low-level control system. The high-level navigation system features sensor fusion of a 6-dof IMU with a standard GPS and the integration of an automotive LIDAR. Operation of the vehicle is via a multi-threaded program with asynchronous IO and is based upon recording and driving waypoints. In addition to independent safety interlocks, active safety systems are an integral part to both the drive-by-wire and navigation systems.},
  annote   = {The following values have no corresponding Zotero field:number: 4},
  keywords = {DC motors, electric vehicles, autonomous driving, navigation, Mobile robots, optical radar, sensor fusion, Global Positioning System, Automatic control, Control systems, acceleration, active safety systems, asynchronous IO, automotive electronics, automotive LIDAR, autonomous Formula SAE race car, braking, drive-by-wire actuators, drive-by-wire system, driverless electric race car, embedded low level control system, high level navigation system, independent safety interlocks, multithreaded program, navigation control system, navigation systems, standard GPS, steering, vehicle},
}

@InProceedings{j._xiao_sun_2010,
  author    = {Jianxiong Xiao and James Hays and Krista A. Ehinger and Aude Oliva and Antonio Torralba},
  title     = {{SUN} database: {Large}-scale scene recognition from abbey to zoo},
  booktitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
  year      = {2010},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1063-6919},
  pages     = {3485--3492},
  doi       = {10.1109/CVPR.2010.5539970},
  abstract  = {Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  keywords  = {image classification, Layout, scene category, object recognition, computer vision, Sun, abbey, Anthropometry, Bridges, finer-grained scene representation, human factors, human scene classification performance, Humans, Image databases, large-scale scene recognition, Large-scale systems, Legged locomotion, object categorization, scene categorization, scene understanding database, scene understanding research, Spatial databases, state-of-the-art algorithms, SUN database, visual databases, zoo},
}

@InProceedings{o._linde_object_2004,
  author    = {O. Linde and T. Lindeberg},
  title     = {Object recognition using composed receptive field histograms of higher dimensionality},
  booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. {ICPR} 2004.},
  year      = {2004},
  volume    = {2},
  publisher = {{IEEE}},
  month     = aug,
  isbn      = {1051-4651},
  pages     = {1--6 Vol.2},
  doi       = {10.1109/ICPR.2004.1333965},
  abstract  = {Effective methods for recognising objects or spatio-temporal events can be constructed based on receptive field responses summarised into histograms or other histogram-like image descriptors. This work presents a set of composed histogram features of higher dimensionality, which give significantly better recognition performance compared to the histogram descriptors of lower dimensionality that were used in the original papers by Swain \& Bollard (1991) or Schiele \& Crowley (2000). The use of histograms of higher dimensionality is made possible by a sparse representation for efficient computation and handling of higher-dimensional histograms. Results of extensive experiments are reported, showing how the performance of histogram-based recognition schemes depend upon different combinations of cues, in terms of Gaussian derivatives or differential invariants applied to either intensity information, chromatic information or both. It is shown that there exist composed higher-dimensional histogram descriptors with much better performance for recognising known objects than previously used histogram features. Experiments are also reported of classifying unknown objects into visual categories.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
  keywords  = {Histograms, object recognition, computer vision, chromatic information, Computer science, Councils, Image recognition, intensity information, Laboratories, Numerical analysis, receptive field histograms, sparse representation, spatiotemporal events, Statistics, Wavelet coefficients},
}

@InCollection{zhou_learning_2014,
  author    = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  title     = {Learning Deep Features for Scene Recognition using Places Database},
  booktitle = {Advances in Neural Information Processing Systems 27},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {487--495},
  url       = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
}

@WWW{niko_suenderhauf_vprice_2015,
  author = {Niko Suenderhauf},
  title  = {The {VPRiCE} {Challenge} 2015 – {Visual} {Place} {Recognition} in {Changing} {Environments}},
  year   = {2015},
  date   = {2015-05-26},
  url    = {https://roboticvision.atlassian.net/wiki/spaces/PUB/pages/14188617/The+VPRiCE+Challenge+2015+Visual+Place+Recognition+in+Changing+Environments},
  annote = {The following values have no corresponding Zotero field:publisher: Australian Centre for Robotic Vision},
}

@article{cummins_fab-map:_2008,
	title = {{FAB}-{MAP}: {Probabilistic} {Localization} and {Mapping} in the {Space} of {Appearance}},
	volume = {27},
	doi = {10.1177/0278364908090961},
	abstract = {This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment—identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mobile robotics.},
	journal = {The International Journal of Robotics Research},
	author = {Cummins, Mark and Newman, Paul},
	month = jun,
	year = {2008},
	pages = {647--665},
	annote = {The following values have no corresponding Zotero field:number: 6}
}

@Article{zaklouta_real-time_2014,
  author   = {Zaklouta, Fatin and Stanciulescu, Bogdan},
  title    = {Real-time traffic sign recognition in three stages},
  journal  = {Robotics and Autonomous Systems},
  year     = {2014},
  volume   = {62},
  pages    = {16--24},
  issn     = {0921-8890},
  doi      = {10.1016/j.robot.2012.07.019},
  abstract = {Traffic Sign Recognition (TSR) is an important component of Advanced Driver Assistance Systems (ADAS). The traffic signs enhance traffic safety by informing the driver of speed limits or possible dangers such as icy roads, imminent road works or pedestrian crossings. We present a three-stage real-time Traffic Sign Recognition system in this paper, consisting of a segmentation, a detection and a classification phase. We combine the color enhancement with an adaptive threshold to extract red regions in the image. The detection is performed using an efficient linear Support Vector Machine (SVM) with Histogram of Oriented Gradients (HOG) features. The tree classifiers, K -d tree and Random Forest, identify the content of the traffic signs found. A spatial weighting approach is proposed to improve the performance of the K -d tree. The Random Forest and Fisher’s Criterion are used to reduce the feature space and accelerate the classification. We show that only a subset of about one third of the features is sufficient to attain a high classification accuracy on the German Traffic Sign Recognition Benchmark (GTSRB).},
  annote   = {The following values have no corresponding Zotero field:number: 1},
  keywords = {Advanced Driver Assistance Systems (ADAS), Color segmentation, Feature space reduction, German Traffic Sign Recognition Benchmark (GTSRB), Intelligent transport systems, Traffic Sign Recognition (TSR)},
}

@InProceedings{m._russell_opencv_2013,
  author    = {Matthew Russell and Scott Fischaber},
  title     = {{OpenCV} based road sign recognition on {Zynq}},
  booktitle = {2013 11th {IEEE} International Conference on Industrial Informatics ({INDIN})},
  year      = {2013},
  publisher = {{IEEE}},
  month     = jul,
  isbn      = {1935-4576},
  pages     = {596--601},
  doi       = {10.1109/INDIN.2013.6622951},
  abstract  = {Road sign recognition is a key component in autonomous vehicles and also has applications in driver assistance systems and road sign maintenance. Here an algorithm is presented using the Xilinx Zynq-7020 chip on a Zedboard to scan 1920\&\#x00D7;1080 images taken by an ON Semiconductor VITA-2000 sensor attached via the FMC slot. The PL section of the Zynq is used to perform essential image pre-processing functions and color based filtering of the image. Software classifies the shapes in the filtered image, and OpenCV's template matching function is used to identify the signs from a database of UK road signs. The system was designed in six weeks, and can process one frame in approximately 5 seconds. This is a promising start for a real-time System on Chip based approach to the problem of road sign recognition and also for using the Zynq platform for rapid deployment of these types of applications.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2013 11th IEEE International Conference on Industrial Informatics (INDIN)},
  keywords  = {road vehicles, Roads, traffic engineering computing, image classification, Image segmentation, image colour analysis, driver assistance systems, Shape, autonomous vehicles, filtering theory, Image color analysis, color-based image filtering, Field programmable gate arrays, filtered image shape classification, FMC slot, Hardware, image preprocessing functions, image scanning, on-semiconductor VITA-2000 sensor, OpenCV-based road sign recognition, public domain software, real-time system-on-chip-based approach, road sign identification, road sign maintenance, Software, system-on-chip, template matching function, UK road sign database, Xilinx Zynq-7020 chip, Zedboard, Zynq PL section},
}

@book{muhammad_opencv_2015,
	title = {{OpenCV} {Android} {Programming} {By} {Example}},
	isbn = {1-78528-293-X},
	publisher = {Packt Publishing Ltd},
	author = {Muhammad, Amgad},
	year = {2015}
}

@Article{richard_o._duda_use_1972,
  author  = {Richard O. Duda and Peter E. Hart},
  title   = {Use of the {Hough} transformation to detect lines and curves in pictures},
  journal = {Communications of the ACM},
  year    = {1972},
  volume  = {15},
  pages   = {11--15},
  issn    = {0001-0782},
  doi     = {10.1145/361237.361242},
  annote  = {The following values have no corresponding Zotero field:number: 1custom1: 361242},
}

@WWW{mobileye_mobileye_2016,
  author = {Mobileye},
  title  = {Mobileye},
  year   = {2018},
  url    = {https://www.mobileye.com/},
  month  = may,
}

@article{c._e._shannon_mathematical_1948,
	title = {A mathematical theory of communication},
	volume = {27},
	issn = {0005-8580},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist$^{\textrm{1}}$ and Hartley$^{\textrm{2}}$ on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
	journal = {The Bell System Technical Journal},
	author = {C. E. Shannon},
	year = {1948},
	pages = {379--423},
	annote = {The following values have no corresponding Zotero field:number: 3}
}

@InProceedings{tomas_krajnik_visual_2015,
  author    = {Tomáš Krajník and Jan Blažíček and João M. Santos},
  title     = {Visual road following using intrinsic images},
  booktitle = {2015 European Conference on Mobile Robots ({ECMR})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = sep,
  pages     = {1--6},
  doi       = {10.1109/ECMR.2015.7324212},
  abstract  = {We present a real-time visual-based road following method for mobile robots in outdoor environments. The approach combines an image processing method, that allows to retrieve illumination invariant images, with an efficient path following algorithm. The method allows a mobile robot to autonomously navigate along pathways of different types in adverse lighting conditions using monocular vision. To validate the proposed method, we have evaluated its ability to correctly determine boundaries of pathways in a challenging outdoor dataset. Moreover, the method's performance was tested on a mobile robotic platform that autonomously navigated long paths in urban parks. The experiments demonstrated that the mobile robot was able to identify outdoor pathways of different types and navigate through them despite the presence of shadows that significantly influenced the paths' appearance.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Mobile Robots (ECMR), 2015 European Conference on},
  keywords  = {mobile robots, robot vision, Cameras, Navigation, Robot sensing systems, mobile robotics, roads, Lighting, illumination invariant image retrieval, image processing, image retrieval, intrinsic images, outdoor environments, real-time systems, real-time visual-based road following, visual navigation},
}

@InProceedings{jannik_fritsch_new_2013,
  author    = {Jannik Fritsch and Tobias Kühnl and Andreas Geiger},
  title     = {A new performance measure and evaluation benchmark for road detection algorithms},
  booktitle = {16th International {IEEE} Conference on Intelligent Transportation Systems ({ITSC} 2013)},
  year      = {2013},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0009},
  pages     = {1693--1700},
  doi       = {10.1109/ITSC.2013.6728473},
  abstract  = {Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and ego-lane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted ego-lane area for driver assistance applications by fitting a driving corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)},
  keywords  = {road vehicles, Roads, Benchmark testing, object detection, Measurement, road area, 2D birds eye view space, 2D world, Aerospace electronics, annotated training, behavior-based metric, behavior-based performance measure, BEV, classical pixel-level metrics, Detection algorithms, driver assistance systems, driver information systems, driving corridor, ego-lane, evaluation benchmark, illumination conditions, inner-city, KITTI autonomous driving project, open-access dataset, performance measure, road terrain detection algorithms, rural environments, scene layout, Space vehicles, test images, urban road scenes, vehicle control, well-marked roads},
}

@InProceedings{m._revilloud_improved_2013,
  author    = {Marc Revilloud and Dominique Gruyer and Evangeline Pollard},
  title     = {An improved approach for robust road marking detection and tracking applied to multi-lane estimation},
  booktitle = {2013 {IEEE} Intelligent Vehicles Symposium ({IV})},
  year      = {2013},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {783--790},
  doi       = {10.1109/IVS.2013.6629562},
  abstract  = {In this paper, an original and innovative algorithm for multi-lane detection and estimation is proposed. Based on a three-step process, (1) road primitives extraction, (2) road markings detection and tracking, (3) lanes shape estimation. This algorithm combines several advantages at each processing level and is quite robust to the extraction method and more specifically to the choice of the extraction threshold. The detection step is so efficient, by using robust poly-fitting based on the point intensity of extracted points, that correction step is almost not necessary anymore. This approach has been used in several project in real condition and its performances have been evaluated with the sensor data generated from SiVIC platform. This validation stage has been done with a sequence of 2500 simulated images. Results are very encouraging : more than 95\% of marking lines are detected for less than 2\% of false alarm, with 3 cm accuracy at a range of 60 m.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Intelligent Vehicles Symposium (IV), 2013 IEEE},
  keywords  = {Roads, traffic engineering computing, feature extraction, Cameras, Estimation, object detection, Shape, Vehicles, object tracking, automated highways, Robustness, curve fitting, extraction threshold, lanes shape estimation, multilane detection, multilane estimation, point intensity, road primitives extraction, road tracking, robust poly-fitting, robust road marking detection, sensor data, shape recognition, SiVIC platform},
}

@InProceedings{z._tao_lane_2013,
  author    = {Z. Tao and P. Bonnifait and V. Frémont and J. Ibañez-Guzman},
  title     = {Lane marking aided vehicle localization},
  booktitle = {16th International {IEEE} Conference on Intelligent Transportation Systems ({ITSC} 2013)},
  year      = {2013},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0009},
  pages     = {1509--1515},
  doi       = {10.1109/ITSC.2013.6728444},
  abstract  = {A localization system that exploits L1-GPS estimates, vehicle data, and features from a video camera as well as lane markings embedded in digital navigation maps is presented. A sensitivity analysis of the detected lane markings is proposed in order to quantify both the lateral and longitudinal errors caused by 2D-world hypothesis violation. From this, a camera observation model for vehicle localization is proposed. The paper presents also a method to build a map of the lane markings in a first stage. The solver is based on dynamical Kalman filtering with a two-stage map-matching process which is described in details. This is a software-based solution using existing automotive components. Experimental results in urban conditions demonstrate an significant increase in the positioning quality.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)},
  keywords  = {Roads, Cameras, Shape, Vehicles, computer vision, Global Positioning System, Kalman filters, 2D world hypothesis violation, Accuracy, automotive components, camera observation model, digital navigation maps, dynamical Kalman filtering, image matching, lane marking aided vehicle localization system, lane markings detection, LI-GPS estimates, map matching process, sensitivity analysis, video camera},
}

@Article{wang_lane_2000,
  author   = {Wang, Yue and Shen, Dinggang and Teoh, Eam Khwang},
  title    = {Lane detection using spline model},
  journal  = {Pattern Recognition Letters},
  year     = {2000},
  volume   = {21},
  pages    = {677--689},
  issn     = {0167-8655},
  doi      = {10.1016/S0167-8655(00)00021-0},
  abstract = {In this paper, a Catmull–Rom spline-based lane model which describes the perspective effect of parallel lines has been proposed for generic lane boundary. Since Catmull–Rom spline can form arbitrary shapes by different sets of control points, it can describe a wider range of lane structures compared with other lane models, i.e. straight and parabolic models. Moreover, the lane detection problem has been formulated here as the problem of determining the set of control points of lane model. The proposed algorithm first detects the vanishing point (line) by using a Hough-like technique and then solves the lane detection problem by suggesting a maximum likelihood approach. Also, we have employed a multi-resolution strategy for rapidly achieving an accurate solution. This coarse-to-fine matching offers us an acceptable solution at an affordable computational cost, and thus speeds up the process of lane detection. As a result, the proposed method is robust to noise, shadows, and illumination variations in the captured road images, and is also applicable to both the marked and the unmarked roads.},
  annote   = {The following values have no corresponding Zotero field:number: 8},
  keywords = {Catmull–Rom spline, Intelligent vehicle, Lane detection, Lane model, Machine vision, Maximum likelihood},
}

@InProceedings{r._chen_research_2013,
  author    = {Rongbao Chen and Huafeng Xiao and Xianwei Dou and Wei Hou},
  title     = {Research on {Recognition} {Methods} of {Bus} {Front} {Road} {Condition} {Based} on {Video}},
  booktitle = {2013 Seventh International Conference on Image and Graphics},
  year      = {2013},
  publisher = {{IEEE}},
  month     = jul,
  pages     = {439--442},
  doi       = {10.1109/ICIG.2013.94},
  abstract  = {Among safe driving systems of intelligent transportation, the road recognition for bus driving, Especially for front road driving condition, becomes more and more important. Moreover, improving the road recognition of public transport vehicles can greatly improve safe driving performance as well as safety and comfort of passenger. Road before driving generally includes vehicle, pedestrians and suspected obstacles, which can be detected based on image technology intuitively and quickly. Based on video image processing technology, using imaging principles, this paper predicts the bus driving route of next moment, and calculates the minimum safe distance after making Canny edge improved algorithm. Coupling radar real-time detects for speed and distance of obstacles in front, the real-time monitoring of road front driving for bus is realized. Finally, the collision avoidance simulation verifies the correctness of this method, which has very broad application areas.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Image and Graphics (ICIG), 2013 Seventh International Conference on},
  keywords  = {road vehicles, Roads, traffic engineering computing, object detection, video signal processing, Image edge detection, road safety, Vehicles, edge detection, automated highways, collision avoidance, Radar imaging, bus driving route, bus front road condition, Canny edge improved algorithm, collision avoidance simulation, front road driving condition, image technology, imaging principles, intelligent transportation, minimum safe distance, obstacles distance, obstacles speed, passenger comfort, passenger safety, pedestrians, public transport vehicles, radar real-time detection, real-time monitoring, Real-time systems, recognition methods, road recognition, safe driving performance, safe driving systems, Safety, video image processing technology},
}

@InProceedings{c._siagian_mobile_2013,
  author    = {Christian Siagian and Chin-Kai Chang and Laurent Itti},
  title     = {Mobile robot navigation system in outdoor pedestrian environment using vision-based road recognition},
  booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
  year      = {2013},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {564--571},
  doi       = {10.1109/ICRA.2013.6630630},
  abstract  = {We present a mobile robot navigation system guided by a novel vision-based road recognition approach. The system represents the road as a set of lines extrapolated from the detected image contour segments. These lines enable the robot to maintain its heading by centering the vanishing point in its field of view, and to correct the long term drift from its original lateral position. We integrate odometry and our visual road recognition system into a grid-based local map that estimates the robot pose as well as its surroundings to generate a movement path. Our road recognition system is able to estimate the road center on a standard dataset with 25,076 images to within 11.42 cm (with respect to roads at least 3 m wide). It outperforms three other state-of-the-art systems. In addition, we extensively test our navigation system in four busy college campus environments using a wheeled robot. Our tests cover more than 5 km of autonomous driving without failure. This demonstrates robustness of the proposed approach against challenges that include occlusion by pedestrians, non-standard complex road markings and shapes, shadows, and miscellaneous obstacle objects.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation (ICRA), 2013 IEEE International Conference on},
  keywords  = {autonomous driving, mobile robots, Roads, robot vision, Estimation, object detection, Image edge detection, path planning, Navigation, object recognition, Robot kinematics, pose estimation, pedestrians, college campus environments, Equations, field of view, grid-based local map, image contour segment detection, long term drift, mobile robot navigation system, nonstandard complex road markings, nonstandard complex road shape, obstacle objects, odometry, outdoor pedestrian environment, robot pose estimation, shadows, vision-based road recognition approach, wheeled robot, wheels},
}

@InProceedings{min_component-model_2015,
  author    = {Min Wang and Liangwei Jiang and Wenjie Lu and Aifen Fang},
  title     = {Component-model based detection and recognition of road vehicles},
  booktitle = {2015 {IEEE} International Conference on Progress in Informatics and Computing ({PIC})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = dec,
  pages     = {449--453},
  doi       = {10.1109/PIC.2015.7489887},
  abstract  = {Feature extraction and recognition of a vehicle is always a popular research spot in the intelligent transportation system (ITS) area. Based on this technology, many applications, such as the license plate recognition, brand recognition, driving behavior understanding and so on, can be realized to improve the transportation management-control level. Unlike normal task-oriented vehicle recognition methods, a new feature extraction and recognition framework based on a component-model for vehicles is introduced in this paper, which extracts features from vehicle components with a coarse-to-fine mechanism. This kind of deep learned visual feature can be used for vehicle detection, license plate recognition and brand recognition. Furthermore, a component dataset including 110 different brands of vehicles is built up for evaluation. The proposed method obtained a good performance in the experimental result, which is significant for the practical application.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE International Conference on Progress in Informatics and Computing (PIC)},
  keywords  = {road vehicles, Visualization, feature extraction, object detection, Vehicles, object recognition, intelligent transportation systems, vehicle detection, intelligent transportation system, ITS, intelligent transportation, brand recognition, coarse-to-fine mechanism, component segmantation, component-model based detection, component-model based recognition, deep learned visual feature, feature recognition, license plate recognition, Licenses, Silicon, vehicle components, vehicle recognition},
}

@InProceedings{s._strygulec_road_2013,
  author    = {S. {Strygulec} and D. {Müller} and M. {Meuter} and C. {Nunn} and S. {Ghosh} and C. {Wöhler}},
  title     = {Road Boundary Detection and Tracking using monochrome camera images},
  booktitle = {Proceedings of the 16th International Conference on Information Fusion},
  year      = {2013},
  month     = jul,
  pages     = {864--870},
  keywords  = {driver information systems;feature extraction;image classification;image sensors;image sequences;image texture;object detection;object recognition;object tracking;parameter estimation;particle filtering (numerical methods);probability;statistical distributions;video signal processing;road boundary detection;road boundary tracking;monochrome camera images;road boundary recognition;clothoid road model;parameter estimation;particle filter;condensation algorithm;patch-based classifier;probability distribution;road class;nonroad class;video sequences;driver assistance;Roads;Noise},
}

@InProceedings{t._h._bui_road_2013,
  author    = {T. H. {Bui} and T. {Saitoh} and E. {Nobuyama}},
  title     = {Road area detection based on texture orientations estimation and vanishing point detection},
  booktitle = {The SICE Annual Conference 2013},
  year      = {2013},
  month     = sep,
  pages     = {1138--1143},
  keywords  = {Roads;Image color analysis;Histograms;Shape;Estimation;Accuracy;Transforms;Road detection;vanishing point detection;texture orientations;Gabor filters},
}

@Article{xiao_monocular_2016,
  author   = {Liang Xiao and Bin Dai and Daxue Liu and Dawei Zhao and Tao Wu},
  title    = {Monocular Road Detection Using Structured Random Forest},
  journal  = {International Journal of Advanced Robotic Systems},
  year     = {2016},
  volume   = {13},
  number   = {3},
  pages    = {101},
  doi      = {10.5772/63561},
  eprint   = {https://doi.org/10.5772/63561},
  abstract = { Road detection is a key task for autonomous land vehicles. Monocular vision-based road-detection algorithms are mostly based on machine learning approaches and are usually cast as classification problems. However, the pixel-wise classifiers are faced with the ambiguity caused by changes in road appearance, illumination and weather. An effective way to reduce the ambiguity is to model the contextual information with structured learning and prediction. Currently, the widely used structured prediction model in road detection is the Markov random field or conditional random field. However, the random field-based methods require additional complex optimization after pixel-wise classification, making them unsuitable for real-time applications. In this paper, we present a structured random forest-based road-detection algorithm which is capable of modelling the contextual information efficiently. By mapping the structured label space to a discrete label space, the test function of each split node can be trained in a similar way to that of the classical random forests. Structured random forests make use of the contextual information of image patches as well as the structural information of the labels to get more consistent results. Besides this benefit, by predicting a batch of pixels in a single classification, the structured random forest-based road detection can be much more efficient than the conventional pixel-wise random forest. Experimental results tested on the KITTI-ROAD dataset and data collected in typical unstructured environments show that structured random forest-based road detection outperforms the classical pixel-wise random forest both in accuracy and efficiency. },
}

@Article{yoav_freund_decision-theoretic_1997,
  author   = {Yoav Freund and Robert E Schapire},
  title    = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
  journal  = {Journal of Computer and System Sciences},
  year     = {1997},
  volume   = {55},
  number   = {1},
  pages    = {119--139},
  issn     = {0022-0000},
  doi      = {10.1006/jcss.1997.1504},
  url      = {http://www.sciencedirect.com/science/article/pii/S002200009791504X},
  abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
}

@article{alkhorshid_road_2016,
	title = {Road {Detection} through {Supervised} {Classification}},
	volume = {abs/1605.03150},
	journal = {CoRR},
	author = {Alkhorshid, Yasamin and Aryafar, Kamelia and Bauer, Sven and Wanielik, Gerd},
	year = {2016}
}

@Article{wang_adaptive_2015,
  author  = {Wang, Qi and Fang, Jianwu and Yuan, Yuan},
  title   = {Adaptive road detection via context-aware label transfer},
  journal = {Neurocomputing},
  year    = {2015},
  volume  = {158},
  pages   = {174--183},
  issn    = {0925-2312},
  doi     = {10.1016/j.neucom.2015.01.054},
}

@InProceedings{m._aly_real_2008,
  author    = {Mohamed Aly},
  title     = {Real time detection of lane markers in urban streets},
  booktitle = {2008 {IEEE} Intelligent Vehicles Symposium},
  year      = {2008},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {7--12},
  doi       = {10.1109/IVS.2008.4621152},
  abstract  = {We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Intelligent Vehicles Symposium, 2008 IEEE},
  keywords  = {Cameras, object detection, driver information systems, Computer vision, Vehicle detection, Road vehicles, filtering theory, Robustness, Bezier splines, Curve fitting, Filtering, Filters, frequency 50 Hz, Intelligent vehicles, RANSAC line fitting, real-time lane marker detection, Road accidents, selective oriented Gaussian filters, splines (mathematics), urban streets},
}

@InProceedings{c._fernandez_curvature-based_2015,
  author    = {C. Fernández and D. F. Llorca and C. Stiller and M. A. Sotelo},
  title     = {Curvature-based curb detection method in urban environments using stereo and laser},
  booktitle = {2015 {IEEE} Intelligent Vehicles Symposium ({IV})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {579--584},
  doi       = {10.1109/IVS.2015.7225747},
  abstract  = {This paper addresses the problem of curb detection for ADAS or autonomous navigation in urban scenarios. The algorithm is based on clouds of 3D points. It is evaluated using 3D information from a pair of stereo cameras and a LIDAR. Curbs are detected based on road surface curvature. The curvature estimation requires a dense point cloud, therefore the density of the LIDAR cloud has been augmented using Iterative Closest Point (ICP) based on the previous scans. The proposed algorithm can deal with curbs of different curvature and heights, from as low as 3 cm, in a range up to 20 m (whenever that curbs are connected in the curvature image). The curb parameters are modeled using straight lines and compared to the ground-truth using the lateral error as the key parameter indicator. The ground-truth sequences were manually labeled on urban images from the KITTI dataset and made publicly available for the scientific community.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE Intelligent Vehicles Symposium (IV)},
  keywords  = {Roads, Cameras, Estimation, Iterative closest point algorithm, Sensors, Three-dimensional displays, object detection, stereo image processing, driver information systems, autonomous navigation, KITTI dataset, Laser radar, optical radar, ADAS, stereo camera, 3D information, 3D point, curb parameter, curvature estimation, curvature image, curvature-based curb detection method, dense point cloud, ground-truth sequence, ICP, iterative closest point, key parameter indicator, laser, lateral error, LIDAR cloud, road surface curvature, scientific community, urban environment, urban image},
}

@InProceedings{hu_roadside_2011,
  author       = {Hu, Tingbo and Wu, Tao},
  title        = {Roadside curb detection based on fusing stereo vision and mono vision},
  booktitle    = {Fourth International Conference on Machine Vision (ICMV 2011): Computer Vision and Image Analysis; Pattern Recognition and Basic Technologies},
  year         = {2012},
  volume       = {8350},
  organization = {International Society for Optics and Photonics},
  pages        = {83501H},
}

@article{shibata_detection_2014,
	title = {Detection of road surface conditions in winter using road surveillance cameras at daytime, night-time and twilight},
	volume = {14},
	issn = {1738-7906},
	journal = {International Journal of Computer Science and Network Security (IJCSNS)},
	author = {Shibata, Keiji and Takeuch, Kazuya and Kawai, Shohei and Horita, Yuukou},
	year = {2014},
	pages = {21},
	annote = {The following values have no corresponding Zotero field:number: 11}
}

@InProceedings{j._dai_night-time_2014,
  author    = {Jing Dai and Yuqiang Fang and Tao Wu and Dawei Zhao and Hangen He},
  title     = {Night-{Time} {Road} {Boundary} {Detection} with {Infrared} {Channel} {Features} {Classifier}},
  booktitle = {2014 {IEEE} International Conference on Computer and Information Technology},
  year      = {2014},
  publisher = {{IEEE}},
  month     = sep,
  pages     = {751--755},
  doi       = {10.1109/CIT.2014.35},
  abstract  = {Road boundary detection is very important to Intelligent Vehicle (IV) System. Recently, road boundary detection during night-time driving condition attracts more and more attentions. In this paper, we propose a novel and fast method for night-time road boundary detection on infrared images. Firstly, a set of novel Infrared Channel Features (ICF) are proposed for describing infrared image patterns. Furthermore, we proposed an Infrared Edge classifier to generate a task-driven probability edge map. Finally, road boundary extraction is performed on the edge map by two steps: searching available road boundaries and second order polynomial approximation. Experiment show that the proposed method performs well with effectiveness and efficiency.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Computer and Information Technology (CIT), 2014 IEEE International Conference on},
  keywords  = {road vehicles, Roads, Training, feature extraction, image classification, object detection, Image edge detection, road traffic, probability, intelligent transportation systems, road boundary detection, curve fitting, Decision trees, ICF, infrared channel features, infrared channel features classifier, infrared edge classifier, infrared image patterns, intelligent vehicle system, IV system, night vision, night-time driving condition, night-time road boundary detection, polynomial approximation, Polynomials, random forests, road boundary extraction, second order polynomial approximation, task-driven probability edge map, Vegetation},
}

@InProceedings{t._scharwachter_visual_2014,
  author    = {T. Scharwächter and M. Schuler and U. Franke},
  title     = {Visual guard rail detection for advanced highway assistance systems},
  booktitle = {2014 {IEEE} Intelligent Vehicles Symposium Proceedings},
  year      = {2014},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {900--905},
  doi       = {10.1109/IVS.2014.6856573},
  abstract  = {In this paper we present a novel method to detect guard rails in highway scenarios using a stereo camera setup. In contrast to previous methods, we combine geometry information with appearance cues using a state-of-the-art feature encoding method. In our system pipeline, we follow a hough-based approach to localize potential guard rails in the image and require each detected line to fulfill linearity in depth as well as certain height expectations. To leverage the appearance information, we exploit an efficient bag-of-features representation that relies on randomized clustering forests. The effectiveness of our approach is demonstrated on a large novel dataset with pixel-level annotations of guard rails in real-world highway scenarios.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2014 IEEE Intelligent Vehicles Symposium Proceedings},
  keywords  = {Cameras, object detection, stereo image processing, Feature extraction, driver information systems, Shape, Histograms, Vehicles, stereo camera, Hough transforms, advanced highway assistance system, appearance cues, bag-of-features representation, feature encoding method, geometry information, Hough-based approach, pixel-level annotation, Rails, randomized clustering forest, Road transportation, visual guard rail detection},
}

@InProceedings{f._oniga_curb_2011,
  author    = {Florin Oniga and Sergiu Nedevschi},
  title     = {Curb detection for driving assistance systems: {A} cubic spline-based approach},
  booktitle = {2011 {IEEE} Intelligent Vehicles Symposium ({IV})},
  year      = {2011},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {945--950},
  doi       = {10.1109/IVS.2011.5940580},
  abstract  = {In this paper we present a real-time algorithm that detects curbs using a cubic spline model. A Digital Elevation Map (DEM) is used to represent the dense stereovision data. Curb measurements (cells) are detected on the current frame DEM. In order to compensate the small number of curb measurements for each frame we perform temporal integration. The result is a rich set of curb measurements that provides a good support for the least square cubic spline fitting. Thus, the curb cubic spline approximation is more stable and available on a much larger area, around the ego car. This compensates the limited field of view of typical stereo sensors. The detected curbs enrich the description of the ego car's surrounding 3D environment and can be used for driving assistance applications.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Intelligent Vehicles Symposium (IV), 2011 IEEE},
  keywords  = {driving assistance systems, image sensors, Sensors, stereo image processing, Image edge detection, driver information systems, Three dimensional displays, least squares approximations, curb detection, splines (mathematics), Polynomials, cartography, cubic spline based approach, curb measurements, Current measurement, dense stereovision data, digital elevation map, ego car, Fitting, least square cubic spline fitting, Spline, stereo sensors},
}

@InProceedings{j._huang_robust_2013,
  author    = {Junjie Huang and Huawei Liang and Zhilin Wang and Tao Mei and Yan Song},
  title     = {Robust lane marking detection under different road conditions},
  booktitle = {2013 {IEEE} International Conference on Robotics and Biomimetics ({ROBIO})},
  year      = {2013},
  publisher = {{IEEE}},
  month     = dec,
  pages     = {1753--1758},
  doi       = {10.1109/ROBIO.2013.6739721},
  abstract  = {In this paper a new lane marking detection algorithm in different road conditions for monocular vision was proposed. Traditional detection algorithms implement the same operation for different road conditions. It is difficult to simultaneously satisfy the requirements of timesaving and robustness in different road conditions. Our algorithm divides the road conditions into two classes. One class is for the clean road, and the other one is for the road with disturbances such as shadows, non-lane markings and vehicles. Our algorithm has its advantages in clean road while has a robust detection of lane markings in complex road. On the remapping image obtained from inverse perspective transformation, a search strategy is used to judge whether pixels belong to the same lane marking. When disturbances appear on the road, this paper uses probabilistic Hough transform to detect lines, and finds out the true lane markings by use of their geometrical features. The experimental results have shown the robustness and accuracy of our algorithm with respect to shadows, changing illumination and non-lane markings.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  keywords  = {Roads, Cameras, object detection, Detection algorithms, driver information systems, road traffic, probability, Vehicles, Transforms, Hough transforms, Brightness, different road conditions, monocular vision, probabilistic Hough transform, remapping image, robust lane marking detection, robustness, search strategy},
}

@article{fernando_real-time_2014,
	title = {Real-time {Lane} {Detection} on {Suburban} {Streets} using {Visual} {Cue} {Integration}},
	issn = {1729-8806},
	doi = {10.5772/58248},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Fernando, Shehan and Udawatta, Lanka and Horan, Ben and Pathir, Pubudu},
	year = {2014},
	pages = {1}
}

@InProceedings{k._ghazali_road_2012,
  author    = {Kamarul Ghazali and Rui Xiao and Jie Ma},
  title     = {Road {Lane} {Detection} {Using} {H}-{Maxima} and {Improved} {Hough} {Transform}},
  booktitle = {2012 Fourth International Conference on Computational Intelligence, Modelling and Simulation},
  year      = {2012},
  publisher = {{IEEE}},
  month     = sep,
  isbn      = {2166-8523},
  pages     = {205--208},
  doi       = {10.1109/CIMSim.2012.31},
  abstract  = {A fast and improved algorithm with the ability to detect unexpected lane changes is aimed in this paper. A short segment of a long curve has relative low curvature which is approximated as a straight line. Based on the characteristics of physical road lane, this paper presents a lane detection technique based on H-MAXIMA transformation and improved Hough Transform algorithm which first defines the region of interest from input image for reducing searching space, divided the image into near field of view and far field of view. In near field of view, Hough transform has been applied to detect lane markers after image noise filtering. The proposed method has been developed using image processing programming language platform and was tested on collected video data. Promising result was obtained with high efficiency of detection.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2012 Fourth International Conference on Computational Intelligence, Modelling and Simulation},
  keywords  = {Roads, Computational modeling, video signal processing, Image edge detection, road safety, Transforms, road accidents, Hough transforms, Image color analysis, Machine vision, approximation theory, curve curvature approximation, far field of view, H\_MAXIMA, H-maxima transformation, Hough Transform, Hough transform algorithm, image denoising, image division, image noise filtering, image processing programming language platform, input image, lane marker detection, lane markers, near field of view, physical road lane detection, region of interest, searching space reduction, straight line, unexpected lane change detection, video data collection},
}

@InProceedings{t._y._chen_vehicle_2015,
  author    = {Tsong-Yi Chen and Chao-Ho Chen and Guan-Ming Luo and Wu-Chih Hu and Johng-Chern Chern},
  title     = {Vehicle {Detection} in {Nighttime} {Environment} by {Locating} {Road} {Lane} and {Taillights}},
  booktitle = {2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing ({IIH}-{MSP})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = sep,
  pages     = {60--63},
  doi       = {10.1109/IIH-MSP.2015.82},
  abstract  = {In this study, we developed a driver-assistance system on Lane Detection during nighttime by mounting a CCD camera inside the car to capture images and to use computer visions to detect lanes and the driving condition in front of the vehicle. This system can increase the safety of driving during low light condition. The features of this system includes: lane detection, surrounding vehicles detection, lane deviation detection, and distance estimation. After testing on highways with high volume of vehicles, the system in this study can actually reach the ideal results. The camera execution speed can reach about 20 frames per second. This could accomplish the real-time image progressing on highways.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP)},
  keywords  = {Cameras, Estimation, object detection, Vehicle detection, Vehicles, computer vision, roads, Mathematical model, Image color analysis, Road transportation, CCD camera, CCD image sensors, computer visions, distance estimation, driver-assistance system, driving condition, highways, lane detection, lane deviation detection, Lane Deviation Warning, real-time image progressing, surrounding vehicles detection, Vehicles Detection},
}

@InProceedings{q._lin_real-time_2010,
  author    = {Q. Lin and Y. Han and H. Hahn},
  title     = {Real-{Time} {Lane} {Departure} {Detection} {Based} on {Extended} {Edge}-{Linking} {Algorithm}},
  booktitle = {2010 Second International Conference on Computer Research and Development},
  year      = {2010},
  publisher = {{IEEE}},
  month     = may,
  pages     = {725--730},
  doi       = {10.1109/ICCRD.2010.166},
  abstract  = {Lane detection can provide important information for safety driving. In this paper, a real time vision-based lane detection method is presented to find the position and type of lanes in each video frame. In the proposed lane detection method, lane hypothesis is generated and verified based on an effective combination of lane-mark edge-link features. First, lane-mark candidates are searched inside region of interest (ROI). During this searching process, an extended edge-linking algorithm with directional edge-gap closing is used to produce more complete edge-links, and features like lane-mark edge orientation and lane-mark width are used to select candidate lane-mark edge-link pairs. For the verification of lane-mark candidates, color is checked inside the region enclosed by candidate edge-link pairs in YUV color space. Additionally, the continuity of the lane is estimated employing a Bayesian probability model based on lane-mark color and edge-link length ratio. Finally, a simple lane departure model is built to detect lane departures based on lane locations in the image. Experiment results show that the proposed lane detection method can work robustly in real-time, and can achieve an average speed of 30{\textasciitilde}50ms per frame for 180\&\#x00D7;120 image size, with a correct detection rate over 92\%.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Computer Research and Development, 2010 Second International Conference on},
  keywords  = {traffic engineering computing, image colour analysis, Layout, object detection, Image edge detection, Detection algorithms, computer vision, edge detection, Bayes methods, Bayesian methods, Robustness, real-time systems, Safety, region of interest, lane detection, Bayesian probability, belief networks, directional edge gap closing, edge-link pairs scan, extended edge linking algorithm, extended edge-linking, Flowcharts, Lane departure warning, lane hypothesis, lane mark color, lane mark edge link feature, lane mark edge orientation, Minimization methods, real-time lane departure detection, real-time vision based lane detection method, Research and development, safety driving, Solid modeling, YUV color space},
}

@article{p._moghadam_fast_2012,
	title = {Fast {Vanishing}-{Point} {Detection} in {Unstructured} {Environments}},
	volume = {21},
	issn = {1057-7149},
	doi = {10.1109/TIP.2011.2162422},
	abstract = {Vision-based road detection in unstructured environments is a challenging problem as there are hardly any discernible and invariant features that can characterize the road or its boundaries in such environments. However, a salient and consistent feature of most roads or tracks regardless of type of the environments is that their edges, boundaries, and even ruts and tire tracks left by previous vehicles on the path appear to converge into a single point known as the vanishing point. Hence, estimating this vanishing point plays a pivotal role in the determination of the direction of the road. In this paper, we propose a novel methodology based on image texture analysis for the fast estimation of the vanishing point in challenging and unstructured roads. The key attributes of the methodology consist of the optimal local dominant orientation method that uses joint activities of only four Gabor filters to precisely estimate the local dominant orientation at each pixel location in the image plane, the weighting of each pixel based on its dominant orientation, and an adaptive distance-based voting scheme for the estimation of the vanishing point. A series of quantitative and qualitative analyses are presented using natural data sets from the Defense Advanced Research Projects Agency Grand Challenge projects to demonstrate the effectiveness and the accuracy of the proposed methodology.},
	journal = {IEEE Transactions on Image Processing},
	author = {P. Moghadam and J. A. Starzyk and W. S. Wijesoma},
	year = {2012},
	keywords = {Roads, Estimation, object detection, Image edge detection, image texture, road traffic, Pixel, Gabor filters, adaptive distance-based voting scheme, Algorithms, Defense Advanced Research Projects Agency Grand Challenge projects, discernible features, Dominant texture orientation, Ecosystem, estimation theory, fast vanishing-point detection, Filter banks, Image Enhancement, Image Interpretation, Computer-Assisted, image plane, image texture analysis, Imaging, Three-Dimensional, invariant features, joint activity, Materials, optimal local dominant orientation method, Pattern Recognition, Automated, pixel location, qualitative analysis, quantitative analysis, Reproducibility of Results, road direction, Sensitivity and Specificity, tire tracks, Transportation, unstructured environments, unstructured roads, vanishing point estimation, vanishing-point detection, vision-based road detection},
	pages = {425--430},
	annote = {The following values have no corresponding Zotero field:number: 1}
}

@InProceedings{d._ding_adaptive_2013,
  author    = {Dajun Ding and Chanho Lee and Kwang-Yeob Lee},
  title     = {An adaptive road {ROI} determination algorithm for lane detection},
  booktitle = {2013 {IEEE} International Conference of {IEEE} Region 10 ({TENCON} 2013)},
  year      = {2013},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2159-3442},
  pages     = {1--4},
  doi       = {10.1109/TENCON.2013.6718807},
  abstract  = {Road conditions can provide important information for driving safety in driving assistance system. The input images usually include unnecessary information and road conditions need to be analyzed only in a region of interest (ROI) to reduce the amount of computation. In this paper, a vision-based road ROI determination algorithm is proposed to detect the road region using the positional information of a vanishing point and line segments. The line segments are detected using Hough Transform. The road ROI can be determined automatically and adaptively in every frame. The proposed method is applied to various video images from black boxes, and is verified to be robust.},
  annote    = {The following values have no corresponding Zotero field:alt-title: TENCON 2013 - 2013 IEEE Region 10 Conference (31194)},
  keywords  = {Roads, Image segmentation, object detection, Image edge detection, driver information systems, road safety, Vehicles, computer vision, Conferences, edge detection, Hough transforms, Noise, region of interest, lane detection, adaptive road ROI determination algorithm, black box, driving assistance system, driving safety, Green products, Hough transform, line segment detection, positional information, road conditions, road region detect, ROI determination, vanishing point, video images, vision-based road ROI determination algorithm},
}

@Article{abbas_novel_2016,
  author   = {Nasrabadi Abbas and Vaezi Mahdi},
  title    = {A Novel Neural Network based Voting Approach for Road Detection via Image Entropy and Color Filtering},
  journal  = {Indian Journal of Science and Technology},
  year     = {2016},
  volume   = {9},
  number   = {7},
  issn     = {0974 -5645},
  url      = {http://www.indjst.org/index.php/indjst/article/view/87851},
  abstract = {Background: The dramatic increase in the traffic needs effective management that leads to the creation of intelligent transportation systems (ITS) or smart roads. Method: In this paper, a novel and quickly executable neural networks based method is presented to decide the configuration of an adaptive filter and choose among many possible segments of image. Findings: The trained network can be put in practice easily, a number of test image are presented to the software, and the program calculates the multiplication of each scene’s entropy and histogram average and based on that the trained radial basis network predicts the suitable filtering parameter. Result show the outcome of the network trial on three examples. It’s obviously because that the current input has values which are relatively close to the values put at the training stage. Applications/Improvements: A lot of research can be committed about enhanced filtering and masking ideas. A human operator is needed to approve the output’s readiness to be segmented.},
  keywords = {Color Filtering, Image Entropy, Machine Vision, Neural Networks, Online Road Detection},
}

@InProceedings{t._h._bui_local_2012,
  author    = {T. H. {Bui} and E. {Nobuyama}},
  title     = {A local soft voting method for texture-based vanishing point detection from unstructured road images},
  booktitle = {2012 Proceedings of SICE Annual Conference (SICE)},
  year      = {2012},
  month     = aug,
  pages     = {396--401},
  keywords  = {computer vision;Gabor filters;image texture;roads;local soft voting method;texture-based vanishing point detection;unstructured road image;vanishing point estimation algorithm;Gabor filter;texture orientation;confidence level estimation;image pixel;adaptive soft voting method;computational cost reduction;Roads;Gabor filters;Computational efficiency;Transforms;Image edge detection;Educational institutions;Electronic mail;vanishing point;Gabor filter;confidence level;soft voting},
}

@article{h._kong_general_2010,
	title = {General {Road} {Detection} {From} a {Single} {Image}},
	volume = {19},
	issn = {1057-7149},
	doi = {10.1109/TIP.2010.2045715},
	abstract = {Given a single image of an arbitrary road, that may not be well-paved, or have clearly delineated edges, or some a priori known color or texture distribution, is it possible for a computer to find this road? This paper addresses this question by decomposing the road detection process into two steps: the estimation of the vanishing point associated with the main (straight) part of the road, followed by the segmentation of the corresponding road area based upon the detected vanishing point. The main technical contributions of the proposed approach are a novel adaptive soft voting scheme based upon a local voting region using high-confidence voters, whose texture orientations are computed using Gabor filters, and a new vanishing-point-constrained edge detection technique for detecting road boundaries. The proposed method has been implemented, and experiments with 1003 general road images demonstrate that it is effective at detecting road regions in challenging conditions.},
	journal = {IEEE Transactions on Image Processing},
	author = {H. Kong and J. Y. Audibert and J. Ponce},
	year = {2010},
	keywords = {image segmentation, image colour analysis, image texture, road detection, road boundary detection, roads, Gabor filters, vanishing point detection, Algorithms, Image Enhancement, Image Interpretation, Computer-Assisted, Pattern Recognition, Automated, Reproducibility of Results, Sensitivity and Specificity, Transportation, soft voting, adaptive soft voting, Dominant edge detection, image color distribution, image texture distribution, vanishing-point-constrained edge detection technique},
	pages = {2211--2220},
	annote = {The following values have no corresponding Zotero field:number: 8}
}

@InProceedings{j._c._bazin_3-line_2012,
  author    = {Jean-Charles Bazin and Marc Pollefeys},
  title     = {3-line {RANSAC} for orthogonal vanishing point detection},
  booktitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  year      = {2012},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0858},
  pages     = {4282--4287},
  doi       = {10.1109/IROS.2012.6385802},
  abstract  = {A wide range of robotic systems needs to estimate their rotation for diverse tasks like automatic control and stabilization, among many others. In regards of the limitations of traditional navigation equipments (like GPS and inertial sensors), this paper follows a vision approach based on the observation of vanishing points (VPs). Urban environments (outdoor as well as indoor) generally contain orthogonal VPs which constitutes an important constraint to fulfill in order to correctly acquire the structure of the scenes. In contrast to existing VP-based techniques, our method inherently enforces the orthogonality of the VPs by directly incorporating the orthogonality constraint into the model estimation step of the RANSAC procedure, which allows real-time applications. The model is estimated from only 3 lines, which corresponds to the theoretical minimal sampling for rotation estimation and constitutes our 3-line RANSAC. We also propose a 1-line RANSAC when the horizon plane is known. Our algorithm has been validated successfully on challenging real datasets.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  keywords  = {robot vision, Cameras, Estimation, Complexity theory, urban environments, Real-time systems, 3-line RANSAC, automatic control, Clustering algorithms, Google, iterative methods, minimal sampling, model estimation, natural scenes, orthogonal vanishing point detection, orthogonality constraint, robotic systems, Robots, rotation estimation, sampling methods, scene structure, stability, stabilization},
}

@Article{rother_new_2002,
  author   = {Rother, Carsten},
  title    = {A new approach to vanishing point detection in architectural environments},
  journal  = {Image and Vision Computing},
  year     = {2002},
  volume   = {20},
  pages    = {647--655},
  issn     = {0262-8856},
  doi      = {10.1016/S0262-8856(02)00054-9},
  abstract = {A man-made environment is characterized by many parallel lines and orthogonal edges. In this article, a new method for detecting the three mutually orthogonal directions of such an environment is presented. Since real-time performance is not necessary for architectural applications, such as building reconstruction, a computationally intensive approach was chosen. However, this enables us to avoid one fundamental error of most other existing techniques. Compared to theirs, our approach is furthermore more rigorous, since all conditions given by three mutually orthogonal directions are identified and utilized. We assume a partly calibrated camera with unknown focal length and unknown principal point. By examining these camera parameters, which can be determined from orthogonal directions, falsely detected vanishing points may be rejected.},
  annote   = {The following values have no corresponding Zotero field:number: 9–10},
  keywords = {Architecture, Camera calibration, Geometric constraints, Vanishing lines, Vanishing points},
}

@InProceedings{verbickas_sky_2014,
  author    = {Verbickas, Rytis and Whitehead, Anthony},
  title     = {Sky and {Ground} {Detection} {Using} {Convolutional} {Neural} {Networks}},
  booktitle = {International Conference on Machine Vision and Machine Learning (MVML)},
  year      = {2014},
}

@Article{lie_robust_2005,
  author   = {Lie, Wen-Nung and Lin, Tom C. I. and Lin, Ting-Chih and Hung, Keng-Shen},
  title    = {A robust dynamic programming algorithm to extract skyline in images for navigation},
  journal  = {Pattern Recognition Letters},
  year     = {2005},
  volume   = {26},
  pages    = {221--230},
  issn     = {0167-8655},
  doi      = {10.1016/j.patrec.2004.08.021},
  abstract = {In this paper, a skyline image detection algorithm is proposed for navigation of mobile vehicles or planes in mountainous environments. First, edge detection and subsequent binary thresholding are applied on the luminance component to obtain the edge map, from which a multi-stage graph is constructed for determining the skyline curve by using the dynamic programming (DP) algorithm. In optimal DP search, characteristics (e.g., preferred position and orientation) of the skyline are utilized to help in correct linking of curves. The tolerance in short breakage of skyline curve is considered for robustness consideration. Experiments show that the processing speed is fast (approximately 0.12–0.21 s for a 352 × 240 pixel image on a Pentium-M 1.3 GHz CPU) and promising for real-time applications.},
  annote   = {The following values have no corresponding Zotero field:number: 2},
  keywords = {Dynamic programming, Edge/contour linking, Skyline extraction},
}

@InProceedings{f._janda_road_2013,
  author    = {F. Janda and S. Pangerl and E. Lang and E. Fuchs},
  title     = {Road boundary detection for run-off road prevention based on the fusion of video and radar},
  booktitle = {2013 {IEEE} Intelligent Vehicles Symposium ({IV})},
  year      = {2013},
  publisher = {{IEEE}},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {1173--1178},
  doi       = {10.1109/IVS.2013.6629625},
  abstract  = {An approach for detecting the road boundary on different types of roads without any preliminary knowledge is presented. We fuse information obtained from an algorithm which detects road markings and road edges in images acquired by a video camera as well as data from a radar sensor. Each road marking, each road edge and each road barrier is tracked individually. Hence we can even capture exits or laybys. We use an edge image for road marking detection and texture information for road edge detection. Additional data provided by a radar sensor is used to measure targets referring to static barriers along the road side such as guardrails. The output of each processing unit is fused into a Kalman filter framework, where the confidence of each subsystem influences the innovation of the overall system. The underlying geometric road model comprises parameters for multiple lanes, the flanking road edge as well as the vehicle's relative pose. The work is part of the project Interactive.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Intelligent Vehicles Symposium (IV), 2013 IEEE},
  keywords  = {Roads, traffic engineering computing, Cameras, image sensors, Image edge detection, road traffic, Radar tracking, radar imaging, edge detection, Kalman filters, Radar detection, road boundary detection, video camera, Kalman filter framework, Radar measurements, radar sensor, road barrier, road edge detection, road markings detection, run-off road prevention, video and radar fusion},
}

@incollection{scharwachter_efficient_2013,
	address = {Berlin, Heidelberg},
	title = {Efficient {Multi}-cue {Scene} {Segmentation}},
	isbn = {978-3-642-40602-7},
	booktitle = {Pattern {Recognition}: 35th {German} {Conference}, {GCPR} 2013, {Saarbrücken}, {Germany}, {September} 3-6, 2013. {Proceedings}},
	publisher = {Springer Berlin Heidelberg},
	author = {Scharwächter, Timo and Enzweiler, Markus and Franke, Uwe and Roth, Stefan},
	editor = {Weickert, Joachim and Hein, Matthias and Schiele, Bernt},
	year = {2013},
	pages = {435--445},
	annote = {The following values have no corresponding Zotero field:label: Scharwächter2013electronic-resource-num: 10.1007/978-3-642-40602-7\_46}
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	journal = {The International Journal of Robotics Research},
	author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
	month = sep,
	year = {2013},
	pages = {1231--1237},
	annote = {The following values have no corresponding Zotero field:number: 11}
}

@InProceedings{p._y._shinzato_fast_2012,
  author    = {Patrick Y. Shinzato and Valdir Grassi and Fernando S. Osorio and Denis F. Wolf},
  title     = {Fast visual road recognition and horizon detection using multiple artificial neural networks},
  booktitle = {2012 IEEE Intelligent Vehicles Symposium},
  year      = {2012},
  publisher = {{IEEE}},
  month     = jun,
  pages     = {1090--1095},
  doi       = {10.1109/IVS.2012.6232175},
  issn      = {1931-0587},
  keywords  = {image classification;image colour analysis;image texture;mobile robots;neural nets;object detection;object recognition;road vehicles;traffic engineering computing;fast visual road recognition;horizon detection;multiple artificial neural networks;autonomous vehicles;mobile robotics;visual information;autonomous navigation;urban environments;visual road detection system;color;texture;RGB;HSV;YUV;camera;Roads;Artificial neural networks;Databases;Training;Entropy;Visualization;Image color analysis},
}

@article{brust_convolutional_2015,
	title = {Convolutional {Patch} {Networks} with {Spatial} {Prior} for {Road} {Detection} and {Urban} {Scene} {Understanding}},
	volume = {abs/1502.06344},
	journal = {CoRR},
	author = {Brust, Clemens-Alexander and Sickert, Sven and Simon, Marcel and Rodner, Erik and Denzler, Joachim},
	year = {2015}
}

@Article{sibel_yenikaya_keeping_2013,
  author    = {Sibel Yenikaya and Gökhan Yenikaya and Ekrem Düven},
  title     = {Keeping the vehicle on the road: {A} survey on on-road lane detection systems},
  journal   = {{ACM} Computing Surveys},
  year      = {2013},
  volume    = {46},
  number    = {1},
  month     = oct,
  pages     = {1--43},
  issn      = {0360-0300},
  doi       = {10.1145/2522968.2522970},
  annote    = {The following values have no corresponding Zotero field:number: 1custom1: 2522970},
  publisher = {Association for Computing Machinery ({ACM})},
}

@article{bar_hillel_recent_2014,
	title = {Recent progress in road and lane detection: a survey},
	volume = {25},
	issn = {1432-1769},
	doi = {10.1007/s00138-011-0404-2},
	abstract = {The problem of road or lane perception is a crucial enabler for advanced driver assistance systems. As such, it has been an active field of research for the past two decades with considerable progress made in the past few years. The problem was confronted under various scenarios, with different task definitions, leading to usage of diverse sensing modalities and approaches. In this paper we survey the approaches and the algorithmic techniques devised for the various modalities over the last 5 years. We present a generic break down of the problem into its functional building blocks and elaborate the wide range of proposed methods within this scheme. For each functional block, we describe the possible implementations suggested and analyze their underlying assumptions. While impressive advancements were demonstrated at limited scenarios, inspection into the needs of next generation systems reveals significant gaps. We identify these gaps and suggest research directions that may bridge them.},
	journal = {Machine Vision and Applications},
	author = {Bar Hillel, Aharon and Lerner, Ronen and Levi, Dan and Raz, Guy},
	year = {2014},
	pages = {727--745},
	annote = {The following values have no corresponding Zotero field:number: 3label: Bar Hillel2014work-type: journal article}
}

@article{j._c._mccall_video-based_2006,
	title = {Video-based lane estimation and tracking for driver assistance: survey, system, and evaluation},
	volume = {7},
	issn = {1524-9050},
	doi = {10.1109/TITS.2006.869595},
	abstract = {Driver-assistance systems that monitor driver intent, warn drivers of lane departures, or assist in vehicle guidance are all being actively considered. It is therefore important to take a critical look at key aspects of these systems, one of which is lane-position tracking. It is for these driver-assistance objectives that motivate the development of the novel "video-based lane estimation and tracking" (VioLET) system. The system is designed using steerable filters for robust and accurate lane-marking detection. Steerable filters provide an efficient method for detecting circular-reflector markings, solid-line markings, and segmented-line markings under varying lighting and road conditions. They help in providing robustness to complex shadowing, lighting changes from overpasses and tunnels, and road-surface variations. They are efficient for lane-marking extraction because by computing only three separable convolutions, we can extract a wide variety of lane markings. Curvature detection is made more robust by incorporating both visual cues (lane markings and lane texture) and vehicle-state information. The experiment design and evaluation of the VioLET system is shown using multiple quantitative metrics over a wide variety of test conditions on a large test path using a unique instrumented vehicle. A justification for the choice of metrics based on a previous study with human-factors applications as well as extensive ground-truth testing from different times of day, road conditions, weather, and driving scenarios is also presented. In order to design the VioLET system, an up-to-date and comprehensive analysis of the current state of the art in lane-detection research was first performed. In doing so, a comparison of a wide variety of methods, pointing out the similarities and differences between methods as well as when and where various methods are most useful, is presented},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {J. C. McCall and M. M. Trivedi},
	year = {2006},
	keywords = {road vehicles, Roads, object detection, video signal processing, driver information systems, Navigation, Vehicle detection, Vehicle driving, computer vision, intelligent vehicles, Data mining, Robustness, Filters, lane detection, Active safety systems, circular reflector markings, curvature detection, driver assistance system, driver monitoring, lane position tracking, lane-departure warning, machine vision, Monitoring, performance metrics, Shadow mapping, steerable filters, System testing, video lane estimation and tracking system},
	pages = {20--37},
	annote = {The following values have no corresponding Zotero field:number: 1}
}

@incollection{bottazzi_adaptive_2014,
	address = {Cham},
	title = {Adaptive {Regions} of {Interest} {Based} on {HSV} {Histograms} for {Lane} {Marks} {Detection}},
	isbn = {978-3-319-05582-4},
	booktitle = {Robot {Intelligence} {Technology} and {Applications} 2: {Results} from the 2nd {International} {Conference} on {Robot} {Intelligence} {Technology} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Bottazzi, Vitor S. and Borges, Paulo V. K. and Stantic, Bela and Jo, Jun},
	editor = {Kim, Jong-Hwan and Matson, Eric T. and Myung, Hyun and Xu, Peter and Karray, Fakhri},
	year = {2014},
	pages = {677--687},
	annote = {The following values have no corresponding Zotero field:label: Bottazzi2014electronic-resource-num: 10.1007/978-3-319-05582-4\_58}
}

@InProceedings{t._ahmad_edge-less_2015,
  author    = {Touqeer Ahmad and George Bebis and Monica Nicolescu and Ara Nefian and Terry Fong},
  title     = {An Edge-Less Approach to Horizon Line Detection},
  booktitle = {2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)},
  year      = {2015},
  publisher = {{IEEE}},
  month     = dec,
  pages     = {1095--1102},
  doi       = {10.1109/ICMLA.2015.67},
  keywords  = {dynamic programming;edge detection;feature extraction;graph theory;image classification;learning (artificial intelligence);edge-less approach;horizon line detection;visual cue;robot localization;visual geo-localization;edge detection;machine learning;dynamic programming;horizon line extraction;classification map;edge map;pixel classification score;pixel likelihood;multistage graph;geographical locations;Image edge detection;Training;Navigation;Support vector machines;Image color analysis;Hidden Markov models;Visualization;horizon line detection;skyline extraction;support vector machines;convolutional neural networks;dynamic programming},
}

@InProceedings{t._ahmad_fusion_2015,
  author    = {Touqeer Ahmad and George Bebis and Monica Nicolescu and Ara Nefian and Terry Fong},
  title     = {Fusion of edge-less and edge-based approaches for horizon line detection},
  booktitle = {2015 6th International Conference on Information, Intelligence, Systems and Applications ({IISA})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = jul,
  pages     = {1--6},
  doi       = {10.1109/IISA.2015.7387988},
  abstract  = {Horizon line detection requires finding a boundary which segments an image into sky and non-sky regions. It has many applications including visual geo-localization and geo-tagging, robot navigation/localization, and ship detection and port security. Recently, two machine learning based approaches have been proposed for horizon line detection: one relying on edge classification and the other relying on pixel classification. In the edge-based approach, a classifier is used to refine the edge map by removing non-horizon edges. The refined edge map is then used to form a multi-stage graph where dynamic programming is applied to extract the horizon line. In the edge-less approach, classification is used to obtain a confidence of horizon-ness at each pixel location. The horizon line is then extracted by applying dynamic programming on the resultant dense classification map rather than on the edge map. Both approaches have shown to outperform the classical approach where dynamic programming is applied on the non-refined edge map. In this paper, we provide a comparison between the edge-less and edge-based approaches using two challenging data sets. Moreover, we propose fusing the information about the horizon-ness and edge-ness of each pixel. Our experimental results illustrate that the proposed fusion approach outperforms both the edge-based and edge-less approaches.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Information, Intelligence, Systems and Applications (IISA), 2015 6th International Conference on},
  keywords  = {Visualization, feature extraction, image classification, image segmentation, learning (artificial intelligence), object detection, image fusion, Image edge detection, Navigation, Vehicles, dynamic programming, graph theory, horizon line detection, horizon line extraction, multistage graph, visual geo-localization, edge classification, edge-based detection approach, edge-less detection approach, geo-tagging, information fusion, machine learning based approach, pixel classification, Security},
}

@article{cristoforis_real-time_2016,
	title = {Real-time monocular image-based path detection},
	volume = {11},
	issn = {1861-8219},
	doi = {10.1007/s11554-013-0356-z},
	abstract = {In this work, we present a new real-time image-based monocular path detection method. It does not require camera calibration and works on semi-structured outdoor paths. The core of the method is based on segmenting images and classifying each super-pixel to infer a contour of navigable space. This method allows a mobile robot equipped with a monocular camera to follow different naturally delimited paths. The contour shape can be used to calculate the forward and steering speed of the robot. To achieve real-time computation necessary for on-board execution in mobile robots, the image segmentation is implemented on a low-power embedded GPU. The validity of our approach has been verified with an image dataset of various outdoor paths as well as with a real mobile robot.},
	journal = {Journal of Real-Time Image Processing},
	author = {Cristóforis, Pablo De and Nitsche, Matías A. and Krajník, Tomáš and Mejail, Marta},
	year = {2016},
	pages = {335--348},
	annote = {The following values have no corresponding Zotero field:number: 2label: Cristóforis2016work-type: journal article}
}

@WWW{nvidia_corporation_parallel_2016,
  author = {{NVIDIA Corporation}},
  title  = {Parallel {Programming} and {Computing} {Platform} {\textbar} {CUDA}},
  year   = {2016},
  url    = {https://developer.nvidia.com/cuda-zone},
}

@WWW{berkeleyvision_caffe_2016,
  author = {BerkeleyVision},
  title  = {Caffe {\textbar} {Deep} {Learning} {Framework}},
  year   = {2016},
  url    = {https://caffe.berkeleyvision.org/},
}

@WWW{khronosgroup_opencl_2016,
  author = {KhronosGroup},
  title  = {{OpenCL} - {The} open standard for parallel programming of heterogeneous systems},
  year   = {2016},
  url    = {https://www.khronos.org/opencl/},
}

@InProceedings{k._moren_accelerating_2014,
  author    = {Konrad Moren and Thomas Perschke and Diana Gohringer},
  title     = {Accelerating local feature extraction using {OpenCL} on heterogeneous platforms},
  booktitle = {Proceedings of the 2014 Conference on Design and Architectures for Signal and Image Processing},
  year      = {2014},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {1--8},
  doi       = {10.1109/dasip.2014.7115626},
  abstract  = {Local feature extraction is one of the most important steps in image processing applications such as image matching and object recognition. The Scale Invariant Feature Transformation (SIFT) algorithm is one of the most robust as well as one of the most computation intensive algorithms to extract local features. Recent implementations of the algorithm focus on homogeneous processors like multi-core CPUs or many-core GPUs. In this paper, we introduce an OpenCL-based implementation, which can be used in homogeneous and heterogeneous CPU/GPU environments. We analyze possible coarse-grained and fine-grained parallelization solutions of the SIFT algorithm. Using a set of optimizations we implement a high-performance SIFT implementations for very different CPU/GPU architectures. The scalable implementation allows for a fast processing, more than 40 FPS for Full-HD images.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Design and Architectures for Signal and Image Processing (DASIP), 2014 Conference on},
  keywords  = {Kernel, feature extraction, Histograms, object recognition, image matching, coarse-grained parallelization solution, computation intensive algorithms, fine-grained parallelization solution, graphics processing units, Heterogeneous computing, heterogeneous platforms, homogeneous processors, image processing applications, local feature extraction acceleration, many-core GPU, Multi-core CPU, multicore CPU, multiprocessing systems, OpenCL, Optimization, Platform specific optimizations, Runtime, scale invariant feature transformation algorithm, SIFT, SIFT algorithm, transforms},
}

@TechReport{braunl_eyesim_2000,
  author = {Bräunl, Thomas},
  title  = {The {EyeSim} {Mobile} {Robot} {Simulator}},
  year   = {2000},
  school = {CITR, The University of Auckland, New Zealand},
}

@Article{opencv_opencv_2016,
  author               = {Bradski, G.},
  title                = {{The OpenCV Library}},
  journal              = {Dr. Dobb's Journal of Software Tools},
  year                 = {2000},
  citeulike-article-id = {2236121},
  keywords             = {bibtex-import},
  posted-at            = {2008-01-15 19:21:54},
}

@WWW{googledevelopers_tango_2016,
  author   = {GoogleDevelopers},
  title    = {ARCore},
  year     = {2019},
  url      = {https://developers.google.com/ar/},
  abstract = {Build apps that understand space and motion in high fidelity on Tango devices.},
}

@book{braunl_embedded_2008,
	title = {Embedded robotics: mobile robot design and applications with embedded systems},
	isbn = {3-540-70534-1},
	publisher = {Springer Science \& Business Media},
	author = {Bräunl, Thomas},
	year = {2008}
}

@InProceedings{l._nardi_introducing_2015,
  author    = {Luigi Nardi and Bruno Bodin and M. Zeeshan Zia and John Mawer and Andy Nisbet and Paul H. J. Kelly and Andrew J. Davison and Mikel Lujan and Michael F. P. O{\textquotesingle}Boyle and Graham Riley and Nigel Topham and Steve Furber},
  title     = {Introducing {SLAMBench}, a performance and accuracy benchmarking methodology for {SLAM}},
  booktitle = {2015 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {5783--5790},
  doi       = {10.1109/ICRA.2015.7140009},
  abstract  = {Real-time dense computer vision and SLAM offer great potential for a new level of scene modelling, tracking and real environmental interaction for many types of robot, but their high computational requirements mean that use on mass market embedded platforms is challenging. Meanwhile, trends in low-cost, low-power processing are towards massive parallelism and heterogeneity, making it difficult for robotics and vision researchers to implement their algorithms in a performance-portable way. In this paper we introduce SLAMBench, a publicly-available software framework which represents a starting point for quantitative, comparable and validatable experimental research to investigate trade-offs in performance, accuracy and energy consumption of a dense RGB-D SLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP, OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D sequences with trajectory and scene ground truth for reliable accuracy comparison of different implementation and algorithms. We present an analysis and breakdown of the constituent algorithmic elements of KinectFusion, and experimentally investigate their execution time on a variety of multicore and GPU-accelerated platforms. For a popular embedded platform, we also present an analysis of energy efficiency for different configuration alternatives.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords  = {Kernel, robot vision, Benchmark testing, Cameras, image colour analysis, Three-dimensional displays, image sequences, SLAM (robots), Simultaneous localization and mapping, public domain software, Accuracy, OpenCL, accuracy benchmarking methodology, C++, C++ language, control system analysis computing, CUDA, energy consumption, Graphics processing units, ICL-NUIM dataset, KinectFusion, OpenMP, parallel architectures, publicly-available software framework, RGB-D SLAM system, simultaneous localisation and mapping, SLAMBench, synthetic RGB-D sequences},
}

@article{g._mohanarajah_cloud-based_2015,
	title = {Cloud-{Based} {Collaborative} 3D {Mapping} in {Real}-{Time} {With} {Low}-{Cost} {Robots}},
	volume = {12},
	issn = {1545-5955},
	doi = {10.1109/TASE.2015.2408456},
	abstract = {This paper presents an architecture, protocol, and parallel algorithms for collaborative 3D mapping in the cloud with low-cost robots. The robots run a dense visual odometry algorithm on a smartphone-class processor. Key-frames from the visual odometry are sent to the cloud for parallel optimization and merging with maps produced by other robots. After optimization the cloud pushes the updated poses of the local key-frames back to the robots. All processes are managed by Rapyuta, a cloud robotics framework that runs in a commercial data center. This paper includes qualitative visualization of collaboratively built maps, as well as quantitative evaluation of localization accuracy, bandwidth usage, processing speeds, and map storage.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {G. Mohanarajah and V. Usenko and M. Singh and R. D' Andrea and M. Waibel},
	year = {2015},
	keywords = {control engineering computing, mobile robots, Visualization, Three-dimensional displays, Robot sensing systems, Robot kinematics, cartography, Optimization, Cloning, cloud computing, Cloud robotics, cloud robotics framework, cloud-based collaborative 3D mapping, cloud-based mapping, commercial data center, computer centres, dense visual odometry, dense visual odometry algorithm, distance measurement, low-cost robots, parallel algorithms, parallel optimization, platform-as-a-Service, Rapyuta, real-time robots, smartphone-class processor},
	pages = {423--431},
	annote = {The following values have no corresponding Zotero field:number: 2}
}

@InProceedings{nescher_simultaneous_2016,
  author    = {Thomas Nescher and Markus Zank and Andreas Kunz},
  title     = {Simultaneous mapping and redirected walking for ad hoc free walking in virtual environments},
  booktitle = {2016 IEEE Virtual Reality (VR)},
  year      = {2016},
  publisher = {{IEEE}},
  month     = mar,
  pages     = {239--240},
  doi       = {10.1109/VR.2016.7504742},
  issn      = {2375-5334},
  keywords  = {human computer interaction;SLAM (robots);virtual reality;redirected walking;user-worn tracking;simultaneous localization and mapping;ad hoc free walking;virtual environments;SLAM;planning RDW controller;Legged locomotion;Simultaneous localization and mapping;Virtual environments;Cameras;Three-dimensional displays;SLAM;redirected walking;tracking;head tracking;virtual reality;locomotion;walkable area;obstacle avoidance},
}

@InProceedings{puneet_jain_poster:_2015,
  author    = {Puneet Jain and Justin Manweiler and Romit Roy Choudhury},
  title     = {Poster: {User} {Location} {Fingerprinting} at {Scale}},
  booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking - {MobiCom} {\textquotesingle}15},
  year      = {2015},
  publisher = {ACM},
  pages     = {260--262},
  doi       = {10.1145/2789168.2795175},
  address   = {2795175},
  annote    = {2795175260-262},
}

@InProceedings{s._baker_equivalence_2001,
  author    = {S. Baker and I. Matthews},
  title     = {Equivalence and efficiency of image alignment algorithms},
  booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
  year      = {2001},
  volume    = {1},
  publisher = {{IEEE} Comput. Soc},
  isbn      = {1063-6919},
  pages     = {I--1090--I--1097 vol.1},
  doi       = {10.1109/cvpr.2001.990652},
  abstract  = {There are two major formulations of image alignment using gradient descent. The first estimates an additive increment to the parameters (the additive approach), the second an incremental warp (the compositional approach). We first prove that these two formulations are equivalent. A very efficient algorithm was proposed by Hager and Belhumeur (1998) using the additive approach that unfortunately can only be applied to a very restricted class of warps. We show that using the compositional approach an equally efficient algorithm (the inverse compositional algorithm) can be derived that can be applied to any set of warps which form a group. While most warps used in computer vision form groups, there are a certain warps that do not. Perhaps most notable is the set of piecewise affine warps used in flexible appearance models (FAMs). We end this paper by extending the inverse compositional algorithm to apply to FAMs.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on},
  keywords  = {Optical sensors, computer vision, Robots, Active appearance model, additive approach, additive increment, Approximation algorithms, compositional approach, efficiency, equivalence, flexible appearance models, gradient descent, image alignment algorithms, Image motion analysis, image registration, incremental warp, inverse compositional algorithm, Jacobian matrices, Motion estimation, Nonlinear optics, piecewise affine warps, Tracking},
}

@article{liu_maximum_2016,
	title = {Maximum {Likelihood} {Estimation} of {Monocular} {Optical} {Flow} {Field} for {Mobile} {Robot} {Ego}-motion},
	issn = {1729-8806},
	doi = {10.5772/62157},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Liu, Huajun and Wang, Cailing and Lu, Jianfeng and Tang, Zhenmin and Yang, Jingyu},
	year = {2016},
	pages = {1}
}

@InProceedings{wang_action_2013,
  author    = {Wang, Heng and Schmid, Cordelia},
  title     = {Action Recognition with Improved Trajectories},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2013},
  month     = dec,
}

@InProceedings{d._pastor-moreno_optical_2015,
  author    = {Daniel Pastor-Moreno and Hyo-Sang Shin and Antony Waldock},
  title     = {Optical flow localisation and appearance mapping ({OFLAAM}) for long-term navigation},
  booktitle = {2015 International Conference on Unmanned Aircraft Systems ({ICUAS})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = jun,
  pages     = {980--988},
  doi       = {10.1109/icuas.2015.7152387},
  abstract  = {This paper presents a novel method to use optical flow navigation for long term navigation. Unlike standard SLAM approaches for augmented reality, OFLAAM is designed for Micro Air Vehicles (MAV). It uses a optical flow camera pointing downwards, a IMU and a monocular camera pointing frontwards. That configuration avoids the computational expensive mapping and tracking of the 3D features. It only maps these features in a vocabulary list by a localization module to tackle the optical flow drift and the lose of the navigation estimation. That module, based on the well established algorithm DBoW2, will be also used to close the loop and allow long-term navigation in previously visited areas. The combination of high speed optical flow navigation with a low rate localization algorithm allows fully autonomous navigation for MAV, at the same time it reduces the overall computational load. This framework is implemented in ROS (Robot Operating System) and tested attached to a laptop. A representative scenario is used to validate and analyze the performance of the system.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Unmanned Aircraft Systems (ICUAS), 2015 International Conference on},
  keywords  = {control engineering computing, image sequences, Optical imaging, Optical sensors, SLAM (robots), autonomous navigation, Vehicles, object tracking, cameras, 3D feature, Adaptive optics, aircraft navigation, algorithm DBoW2, augmented reality, autonomous aerial vehicles, computational expensive mapping and tracking, Computers, high speed optical flow navigation, High-speed optical techniques, IMU, laptop, localization module, long-term navigation, low rate localization algorithm, MAV, micro air vehicle, monocular camera pointing frontward, navigation estimation, OFLAAM, optical flow camera, optical flow drift, optical flow localisation and appearance mapping, robot operating system, ROS, standard SLAM approach},
}

@article{cummins_appearance-only_2010,
	title = {Appearance-only {SLAM} at large scale with {FAB}-{MAP} 2.0},
	volume = {30},
	issn = {0278-3649 1741-3176},
	doi = {10.1177/0278364910385483},
	journal = {The International Journal of Robotics Research},
	author = {Cummins, M. and Newman, P.},
	year = {2010},
	pages = {1100--1123},
	annote = {The following values have no corresponding Zotero field:number: 9}
}

@Article{wagner_subdimensional_2015,
  author  = {Wagner, Glenn and Choset, Howie},
  title   = {Subdimensional expansion for multirobot path planning},
  journal = {Artificial Intelligence},
  year    = {2015},
  volume  = {219},
  pages   = {1--24},
  issn    = {0004-3702},
  doi     = {10.1016/j.artint.2014.11.001},
}

@InProceedings{nissim_multi-agent_2012,
  author    = {Nissim, Raz and Brafman, Ronen I.},
  title     = {Multi-agent A* for Parallel and Distributed Systems},
  booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3},
  year      = {2012},
  series    = {AAMAS '12},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  location  = {Valencia, Spain},
  isbn      = {0-9817381-3-3, 978-0-9817381-3-0},
  pages     = {1265--1266},
  url       = {http://dl.acm.org/citation.cfm?id=2343896.2343955},
  acmid     = {2343955},
  address   = {Richland, SC},
  keywords  = {distributed search, multi-agent planning, parallel search},
  numpages  = {2},
}

@InProceedings{korf_complexity_1998,
  author    = {Korf, Richard E. and Reid, Michael},
  title     = {Complexity Analysis Admissible Heuristic Search},
  booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
  year      = {1998},
  series    = {AAAI '98/IAAI '98},
  publisher = {American Association for Artificial Intelligence},
  location  = {Madison, Wisconsin, USA},
  isbn      = {0-262-51098-7},
  pages     = {305--310},
  url       = {http://dl.acm.org/citation.cfm?id=295240.295616},
  acmid     = {295616},
  address   = {Menlo Park, CA, USA},
  numpages  = {6},
}

@article{p._e._hart_formal_1968,
	title = {A {Formal} {Basis} for the {Heuristic} {Determination} of {Minimum} {Cost} {Paths}},
	volume = {4},
	issn = {0536-1567},
	doi = {10.1109/tssc.1968.300136},
	abstract = {Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	author = {P. E. Hart and N. J. Nilsson and B. Raphael},
	year = {1968},
	keywords = {Chemical technology, Automatic control, Minimization methods, Automatic programming, Costs, Functional programming, Gradient methods, Instruction sets, Mathematical programming, Minimax techniques},
	pages = {100--107},
	annote = {The following values have no corresponding Zotero field:number: 2}
}

@article{dijkstra_note_1959,
	title = {A note on two problems in connexion with graphs},
	volume = {1},
	issn = {0945-3245},
	doi = {10.1007/bf01386390},
	journal = {Numerische Mathematik},
	author = {Dijkstra, E. W.},
	year = {1959},
	pages = {269--271},
	annote = {The following values have no corresponding Zotero field:number: 1label: Dijkstra1959work-type: journal article}
}

@Article{sharon_conflict-based_2015,
  author  = {Sharon, Guni and Stern, Roni and Felner, Ariel and Sturtevant, Nathan R.},
  title   = {Conflict-based search for optimal multi-agent pathfinding},
  journal = {Artificial Intelligence},
  year    = {2015},
  volume  = {219},
  pages   = {40--66},
  issn    = {0004-3702},
  doi     = {10.1016/j.artint.2014.11.006},
}

@article{alitappeh_multi-objective_2016,
	title = {Multi-objective multi-robot deployment in a dynamic environment},
	issn = {1432-7643 1433-7479},
	doi = {10.1007/s00500-016-2207-x},
	journal = {Soft Computing},
	author = {Alitappeh, Reza Javanmard and Jeddisaravi, Kossar and Guimarães, Frederico G.},
	year = {2016}
}

@InProceedings{j._h._peng_multi-robot_2015,
  author    = {Jung-Hao Peng and I-Hsum Li and Yi-Hsing Chien and Chen-Chien Hsu and Wei-Yen Wang},
  title     = {Multi-robot path planning based on improved {D}* {Lite} {Algorithm}},
  booktitle = {2015 {IEEE} 12th International Conference on Networking, Sensing and Control},
  year      = {2015},
  publisher = {{IEEE}},
  month     = apr,
  pages     = {350--353},
  doi       = {10.1109/icnsc.2015.7116061},
  abstract  = {This paper proposes an improved multi-robot path planning algorithm for finding the path via interacting with multiple robots. The task is to find the path with a minimum amount of computation time by using fast re-planning algorithm. To solve multi-robot path planning problem which cannot be executed in real-time, we regard other robots, exclusive the origin robot, as obstacles. Therefore, the robot uploads location information to the MySQL server to plan a safe distance between robots.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Networking, Sensing and Control (ICNSC), 2015 IEEE 12th International Conference on},
  keywords  = {Prediction algorithms, control engineering computing, path planning, Navigation, Planning, Robots, D* Lite Algorithm, D*lite, fast re-planning, Genetics, minimisation, multi-robot, multi-robot systems, multirobot path planning, MySQL server, replanning algorithm, SQL},
}

@Article{lim_uninformed_2015,
  author  = {Lim, Kai Li and Seng, Kah Phooi and Yeong, Lee Seng and Ang, Li-Minn and Ch’ng, Sue Inn},
  title   = {Uninformed pathfinding: {A} new approach},
  journal = {Expert Systems with Applications},
  year    = {2015},
  volume  = {42},
  pages   = {2722--2730},
  issn    = {0957-4174},
  doi     = {10.1016/j.eswa.2014.10.046},
  annote  = {The following values have no corresponding Zotero field:number: 5},
}

@article{gil_comparative_2009,
	title = {A comparative evaluation of interest point detectors and local descriptors for visual {SLAM}},
	volume = {21},
	issn = {0932-8092 1432-1769},
	doi = {10.1007/s00138-009-0195-x},
	journal = {Machine Vision and Applications},
	author = {Gil, Arturo and Mozos, Oscar Martinez and Ballesta, Monica and Reinoso, Oscar},
	year = {2009},
	pages = {905--920},
	annote = {The following values have no corresponding Zotero field:number: 6}
}

@InProceedings{h._lategahn_visual_2011,
  author    = {Henning Lategahn and Andreas Geiger and Bernd Kitt},
  title     = {Visual {SLAM} for autonomous ground vehicles},
  booktitle = {2011 {IEEE} International Conference on Robotics and Automation},
  year      = {2011},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {1732--1737},
  doi       = {10.1109/icra.2011.5979711},
  abstract  = {Simultaneous Localization and Mapping (SLAM) and Visual SLAM (V-SLAM) in particular have been an active area of research lately. In V-SLAM the main focus is most often laid on the localization part of the problem allowing for a drift free motion estimate. To this end, a sparse set of landmarks is tracked and their position is estimated. However, this set of landmarks (rendering the map) is often too sparse for tasks in autonomous driving such as navigation, path planning, obstacle avoidance etc. Some methods keep the raw measurements for past robot poses to address the sparsity problem often resulting in a pose only SLAM akin to laser scanner SLAM. For the stereo case, this is however impractical due to the high noise of stereo reconstructed point clouds. In this paper we propose a dense stereo V-SLAM algorithm that estimates a dense 3D map representation which is more accurate than raw stereo measurements. Thereto, we run a sparse V SLAM system, take the resulting pose estimates to compute a locally dense representation from dense stereo correspondences. This dense representation is expressed in local coordinate systems which are tracked as part of the SLAM estimate. This allows the dense part to be continuously updated. Our system is driven by visual odometry priors to achieve high robustness when tracking landmarks. Moreover, the sparse part of the SLAM system uses recently published sub mapping techniques to achieve constant runtime complexity most of the time. The improved accuracy over raw stereo measurements is shown in a Monte Carlo simulation. Finally, we demonstrate the feasibility of our method by presenting outdoor experiments of a car like robot.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  keywords  = {autonomous driving, mobile robots, road vehicles, robot vision, Visualization, Cameras, SLAM (robots), stereo image processing, navigation, path planning, object tracking, image reconstruction, image representation, simultaneous localization and mapping, Kalman filters, pose estimation, collision avoidance, Accuracy, autonomous ground vehicles, car like robot, dense 3D map representation, drift free motion estimate, landmark tracking, local coordinate system, Monte Carlo methods, Monte Carlo simulation, motion estimation, obstacle avoidance, pose estimate, position estimation, robot pose, runtime complexity, sparsity problem, stereo reconstructed point clouds, submapping technique, visual odometry, visual SLAM},
}

@article{perez_enhanced_2015,
	title = {Enhanced {Monte} {Carlo} {Localization} with {Visual} {Place} {Recognition} for {Robust} {Robot} {Localization}},
	volume = {80},
	issn = {0921-0296 1573-0409},
	doi = {10.1007/s10846-015-0198-y},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Pérez, Javier and Caballero, Fernando and Merino, Luis},
	year = {2015},
	pages = {641--656},
	annote = {The following values have no corresponding Zotero field:number: 3-4}
}

@Article{d._galvez-lopez_bags_2012,
  author    = {D. Galvez-L{\'{o}}pez and J. D. Tardos},
  title     = {Bags of {Binary} {Words} for {Fast} {Place} {Recognition} in {Image} {Sequences}},
  journal   = {IEEE Transactions on Robotics},
  year      = {2012},
  volume    = {28},
  number    = {5},
  month     = oct,
  pages     = {1188--1197},
  issn      = {1552-3098},
  doi       = {10.1109/tro.2012.2197158},
  abstract  = {We propose a novel method for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF features. For the first time, we build a vocabulary tree that discretizes a binary descriptor space and use the tree to speed up correspondences for geometrical verification. We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22 ms/frame in a sequence with 26 300 images that is one order of magnitude faster than previous approaches.},
  annote    = {The following values have no corresponding Zotero field:number: 5},
  keywords  = {mobile robots, feature extraction, Cameras, image sequences, SLAM (robots), geometry, object recognition, computer vision, simultaneous localization and mapping, Robots, accelerated segment test, Bag of binary words, bags of binary words, binary descriptor space, BRIEF features, fast place recognition, FAST\&\#x00B1, geometrical verification, Indexes, place recognition, simultaneous localization and mapping (SLAM), trees (mathematics), Vectors, visual place recognition, Vocabulary, vocabulary tree},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@article{a._j._davison_monoslam:_2007,
	title = {{MonoSLAM}: {Real}-{Time} {Single} {Camera} {SLAM}},
	volume = {29},
	issn = {0162-8828},
	doi = {10.1109/tpami.2007.1049},
	abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to structure from motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {A. J. Davison and I. D. Reid and N. D. Molton and O. Stasse},
	year = {2007},
	keywords = {feature extraction, Cameras, image sensors, Layout, SLAM (robots), Mobile robots, Real time systems, Simultaneous localization and mapping, Autonomous vehicles, Robustness, Hardware, Algorithms, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Pattern Recognition, Automated, Motion estimation, 3D trajectory, 3D/stereo scene analysis, Artificial Intelligence, Computer Systems, feature orientation estimation, humanoid robot, humanoid robots, Information Storage and Retrieval, monocular camera, MonoSLAM, Motion measurement, Numerical Analysis, Computer-Assisted, Photogrammetry, real-time single camera SLAM, Robot vision systems, Signal Processing, Computer-Assisted, tracking., Video Recording},
	pages = {1052--1067},
	annote = {The following values have no corresponding Zotero field:number: 6}
}

@InProceedings{b._williams_combining_2010,
  author    = {Brian Williams and Ian Reid},
  title     = {On combining visual {SLAM} and visual odometry},
  booktitle = {2010 {IEEE} International Conference on Robotics and Automation},
  year      = {2010},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {3494--3500},
  doi       = {10.1109/robot.2010.5509248},
  abstract  = {Sequential monocular SLAM systems perform drift free tracking of the pose of a camera relative to a jointly estimated map of landmarks. To allow real-time operation in moderately sized environments, the map is kept quite spare with usually only tens of landmarks visible in each frame. In contrast, visual odometry techniques track hundreds of visual features per frame. This leads to a very accurate estimate of the relative camera motion, but without a persistent map, the estimate tends to drift over time. We demonstrate a new monocular SLAM system which combines the benefits of these two techniques. In addition to maintaining a sparse map of landmarks in the world, our system finds as many inter-frame point matches as possible. These point matches provide additional constraints on the inter-frame motion of the camera leading to a more accurate pose estimate, and, since they are not maintained as full map landmarks, they do not cause a large increase in the computational cost. Our results in both a simulated environment and in real video demonstrate the improvement in estimation accuracy gained by the inclusion of visual odometry style observations. The constraints available from pairwise point matches are most naturally cast in the context of a camera-centric rather than world-centric frame. To that end we recast the usual world-centric EKF implementation of visual SLAM in a robo-centric frame. We show that this robo-centric visual SLAM, as expected, leads to the estimated uncertainty more closely matching the ideal uncertainty; i.e., that robo-centric visual SLAM yields a more consistent estimate than the traditional world-centric EKF algorithm.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation (ICRA), 2010 IEEE International Conference on},
  keywords  = {Cameras, Layout, SLAM (robots), camera, Simultaneous localization and mapping, image processing, Filters, distance measurement, Motion estimation, visual odometry, Geometry, interframe motion, interframe point matches, robo-centric visual SLAM, Robotics and automation, sequential monocular SLAM systems, sparse map, Uncertainty, USA Councils, Yield estimation},
}

@Article{williams_comparison_2009,
  author  = {Williams, Brian and Cummins, Mark and Neira, José and Newman, Paul and Reid, Ian and Tardós, Juan},
  title   = {A comparison of loop closing techniques in monocular {SLAM}},
  journal = {Robotics and Autonomous Systems},
  year    = {2009},
  volume  = {57},
  pages   = {1188--1197},
  issn    = {0921-8890},
  doi     = {10.1016/j.robot.2009.06.010},
  annote  = {The following values have no corresponding Zotero field:number: 12},
}

@incollection{farneback_two-frame_2003,
	address = {Berlin, Heidelberg},
	title = {Two-{Frame} {Motion} {Estimation} {Based} on {Polynomial} {Expansion}},
	isbn = {978-3-540-45103-7},
	booktitle = {Image {Analysis}: 13th {Scandinavian} {Conference}, {SCIA} 2003 {Halmstad}, {Sweden}, {June} 29 – {July} 2, 2003 {Proceedings}},
	publisher = {Springer Berlin Heidelberg},
	author = {Farnebäck, Gunnar},
	editor = {Bigun, Josef and Gustavsson, Tomas},
	year = {2003},
	pages = {363--370},
	annote = {The following values have no corresponding Zotero field:label: Farnebäck2003electronic-resource-num: 10.1007/3-540-45103-x\_50}
}

@Article{horn_determining_1981,
  author    = {Berthold K.P. Horn and Brian G. Schunck},
  title     = {Determining optical flow},
  journal   = {Artificial Intelligence},
  year      = {1981},
  volume    = {17},
  number    = {1},
  month     = aug,
  pages     = {185--203},
  issn      = {0004-3702},
  doi       = {10.1016/0004-3702(81)90024-2},
  url       = {http://www.sciencedirect.com/science/article/pii/0004370281900242},
  abstract  = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.},
  publisher = {Elsevier {BV}},
}

@InProceedings{bruce_iterative_1981,
  author    = {Lucas, Bruce D. and Kanade, Takeo},
  title     = {An Iterative Image Registration Technique with an Application to Stereo Vision},
  booktitle = {Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2},
  year      = {1981},
  series    = {IJCAI'81},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location  = {Vancouver, BC, Canada},
  pages     = {674--679},
  url       = {http://dl.acm.org/citation.cfm?id=1623264.1623280},
  acmid     = {1623280},
  address   = {San Francisco, CA, USA},
  numpages  = {6},
}

@InProceedings{e._rublee_orb:_2011,
  author    = {Ethan Rublee and Vincent Rabaud and Kurt Konolige and Gary Bradski},
  title     = {{ORB}: {An} efficient alternative to {SIFT} or {SURF}},
  booktitle = {2011 International Conference on Computer Vision},
  year      = {2011},
  publisher = {{IEEE}},
  month     = nov,
  isbn      = {1550-5499},
  pages     = {2564--2571},
  doi       = {10.1109/ICCV.2011.6126544},
  abstract  = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2011 International Conference on Computer Vision},
  keywords  = {object detection, Boats, object recognition, computer vision, image matching, SIFT, transforms, binary descriptor, BRIEF, feature matching, noise resistance, ORB, patch-tracking, smart phone, SURF, tracking},
}

@incollection{rosten_machine_2006,
	address = {Berlin, Heidelberg},
	title = {Machine {Learning} for {High}-{Speed} {Corner} {Detection}},
	isbn = {978-3-540-33833-8},
	booktitle = {Computer {Vision} – {ECCV} 2006: 9th {European} {Conference} on {Computer} {Vision}, {Graz}, {Austria}, {May} 7-13, 2006. {Proceedings}, {Part} {I}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rosten, Edward and Drummond, Tom},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	year = {2006},
	pages = {430--443},
	annote = {The following values have no corresponding Zotero field:label: Rosten2006electronic-resource-num: 10.1007/11744023\_34}
}

@PhdThesis{hans_peter_moravec_obstacle_1980,
  author      = {Hans Peter Moravec},
  title       = {Obstacle avoidance and navigation in the real world by a seeing robot rover},
  institution = {Stanford University - Computer Science Department},
  year        = {1980},
  school      = {Stanford University},
}

@article{lemaire_vision-based_2007,
	title = {Vision-{Based} {SLAM}: {Stereo} and {Monocular} {Approaches}},
	volume = {74},
	issn = {0920-5691 1573-1405},
	doi = {10.1007/s11263-007-0042-3},
	journal = {International Journal of Computer Vision},
	author = {Lemaire, Thomas and Berger, Cyrille and Jung, Il-Kyun and Lacroix, Simon},
	year = {2007},
	pages = {343--364},
	annote = {The following values have no corresponding Zotero field:number: 3}
}

@InProceedings{georg_klein_parallel_2007,
  author    = {Georg Klein and David Murray},
  title     = {Parallel {Tracking} and {Mapping} for {Small} {AR} {Workspaces}},
  booktitle = {2007 6th {IEEE} and {ACM} International Symposium on Mixed and Augmented Reality},
  year      = {2007},
  publisher = {IEEE Computer Society},
  month     = nov,
  pages     = {1--10},
  doi       = {10.1109/ismar.2007.4538852},
  address   = {1514363},
}

@Article{fortun_optical_2015,
  author  = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
  title   = {Optical flow modeling and computation: {A} survey},
  journal = {Computer Vision and Image Understanding},
  year    = {2015},
  volume  = {134},
  pages   = {1--21},
  issn    = {1077-3142},
  doi     = {10.1016/j.cviu.2015.02.008},
}

@Article{martin_a._fischler_random_1981,
  author  = {Martin A. Fischler and Robert C. Bolles},
  title   = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  journal = {Communications of the ACM},
  year    = {1981},
  volume  = {24},
  pages   = {381--395},
  issn    = {0001-0782},
  doi     = {10.1145/358669.358692},
  annote  = {The following values have no corresponding Zotero field:number: 6custom1: 358692},
}

@Article{bay_speeded-up_2008,
  author  = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
  title   = {Speeded-{Up} {Robust} {Features} ({SURF})},
  journal = {Computer Vision and Image Understanding},
  year    = {2008},
  volume  = {110},
  pages   = {346--359},
  issn    = {1077-3142},
  doi     = {10.1016/j.cviu.2007.09.014},
  annote  = {The following values have no corresponding Zotero field:number: 3},
}

@InProceedings{j._campbell_techniques_2004,
  author    = {J. Campbell and R. Sukthankar and I. Nourbakhsh},
  title     = {Techniques for evaluating optical flow for visual odometry in extreme terrain},
  booktitle = {2004 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS}) ({IEEE} Cat. No.04CH37566)},
  year      = {2004},
  volume    = {4},
  publisher = {{IEEE}},
  month     = oct,
  pages     = {3704--3711 vol.4},
  doi       = {10.1109/iros.2004.1389991},
  abstract  = {Motion vision (visual odometry, the estimation of camera egomotion) is a well researched field, yet has seen relatively limited use despite strong evidence from biological systems that vision can be extremely valuable for navigation. The limited use of such vision techniques has been attributed to a lack of good algorithms and insufficient computer power, but both of those problems were resolved as long as a decade ago. A gap presently yawns between theory and practice, perhaps due to perceptions of robot vision as less reliable and more complex than other types of sensing. We present an experimental methodology for assessing the real world precision and reliability of visual odometry techniques in both normal and extreme terrain. This paper evaluates the performance of a mobile robot equipped with a simple vision system in common outdoor and indoor environments, including grass, pavement, ice, and carpet. Our results show that motion vision algorithms can be robust and effective, and suggest a number of directions for further development.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Intelligent Robots and Systems, 2004. (IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on},
  keywords  = {mobile robots, robot vision, Cameras, image sequences, Optical sensors, Navigation, mobile robot, Machine vision, distance measurement, Image motion analysis, motion estimation, visual odometry, Biological systems, Biology computing, Biomedical optical imaging, camera egomotion estimation, motion vision algorithms, optical flow, Ultraviolet sources},
}

@InProceedings{i._dryanovski_fast_2013,
  author    = {Ivan Dryanovski and Roberto G. Valenti and Jizhong Xiao},
  title     = {Fast visual odometry and mapping from {RGB}-{D} data},
  booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
  year      = {2013},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {2305--2310},
  doi       = {10.1109/icra.2013.6630889},
  abstract  = {An RGB-D camera is a sensor which outputs color and depth and information about the scene it observes. In this paper, we present a real-time visual odometry and mapping system for RGB-D cameras. The system runs at frequencies of 30Hz and higher in a single thread on a desktop CPU with no GPU acceleration required. We recover the unconstrained 6-DoF trajectory of a moving camera by aligning sparse features observed in the current RGB-D image against a model of previous features. The model is persistent and dynamically updated from new observations using a Kalman Filter. We formulate a novel uncertainty measure for sparse RGD-B features based on a Gaussian mixture model for the filtering stage. Our registration algorithm is capable of closing small-scale loops in indoor environments online without any additional SLAM back-end techniques.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation (ICRA), 2013 IEEE International Conference on},
  keywords  = {Visualization, Gaussian mixture model, Cameras, image colour analysis, Iterative closest point algorithm, computer vision, registration algorithm, Kalman filters, Kalman filter, image registration, visual odometry, Robot vision systems, Uncertainty, 6-DoF trajectory, color image, Data models, depth image, frequency 30 Hz, Gaussian processes, RGB-D camera, small-scale loops, sparse RGD-B feature, Trajectory, visual mapping},
}

@article{maimone_two_2007,
	title = {Two years of {Visual} {Odometry} on the {Mars} {Exploration} {Rovers}},
	volume = {24},
	issn = {15564959 15564967},
	doi = {10.1002/rob.20184},
	journal = {Journal of Field Robotics},
	author = {Maimone, Mark and Cheng, Yang and Matthies, Larry},
	year = {2007},
	pages = {169--186},
	annote = {The following values have no corresponding Zotero field:number: 3}
}

@article{s._lowry_visual_2016,
	title = {Visual {Place} {Recognition}: {A} {Survey}},
	volume = {32},
	issn = {1552-3098},
	doi = {10.1109/tro.2015.2496823},
	abstract = {Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines-particularly recognition in computer vision and animal navigation in neuroscience-have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition-the role of place recognition in the animal kingdom, how a \&\#x201C;place\&\#x201D; is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description.},
	journal = {IEEE Transactions on Robotics},
	author = {S. Lowry and N, Sünderhauf and P. Newman and J. J. Leonard and D. Cox and P. Corke and M. J. Milford},
	year = {2016},
	keywords = {mobile robots, robot vision, Visualization, learning (artificial intelligence), video signal processing, Navigation, object recognition, Robot sensing systems, computer vision, Conferences, place recognition, animal kingdom, animal navigation, Animals, deep learning, long-term mobile robot autonomy, robotics context, semantic scene understanding, video description, Visual place recognition, visual place recognition research landscape, visual place recognition system, visual sensing capabilities},
	pages = {1--19},
	annote = {The following values have no corresponding Zotero field:number: 1}
}

@article{gupta_i1.4:_2015,
	title = {I1.4: {Invited} {Paper}: {Indoor} {Scene} {Understanding} from {RGB}-{D} {Images}},
	volume = {46},
	issn = {2168-0159},
	doi = {10.1002/sdtp.10285},
	abstract = {Our goal is to be able to align objects in an RGB-D image with 3D models from a library. Our pipeline for this task involves detecting and segmenting objects and estimating coarse pose using a convolutional neural network, followed by inserting the rendered model in the scene.},
	journal = {SID Symposium Digest of Technical Papers},
	author = {Gupta, Saurabh and Girshick, Ross and Arbeláez, Pablo and Malik, Jitendra},
	year = {2015},
	pages = {87--90},
	annote = {The following values have no corresponding Zotero field:number: 1}
}

@article{gupta_indoor_2015,
	title = {Indoor {Scene} {Understanding} with {RGB}-{D} {Images}: {Bottom}-up {Segmentation}, {Object} {Detection} and {Semantic} {Segmentation}},
	volume = {112},
	issn = {1573-1405},
	doi = {10.1007/s11263-014-0777-6},
	abstract = {In this paper, we address the problems of contour detection, bottom-up grouping, object detection and semantic segmentation on RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset (Silberman et al., ECCV, 2012). We propose algorithms for object boundary detection and hierarchical segmentation that generalize the \$\$gPb-ucm\$\$ g P b - u c m approach of Arbelaez et al. (TPAMI, 2011) by making effective use of depth information. We show that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We train RGB-D object detectors by analyzing and computing histogram of oriented gradients on the depth image and using them with deformable part models (Felzenszwalb et al., TPAMI, 2010). We observe that this simple strategy for training object detectors significantly outperforms more complicated models in the literature. We then turn to the problem of semantic segmentation for which we propose an approach that classifies superpixels into the dominant object categories in the NYUD2 dataset. We design generic and class-specific features to encode the appearance and geometry of objects. We also show that additional features computed from RGB-D object detectors and scene classifiers further improves semantic segmentation accuracy. In all of these tasks, we report significant improvements over the state-of-the-art.},
	journal = {International Journal of Computer Vision},
	author = {Gupta, Saurabh and Arbeláez, Pablo and Girshick, Ross and Malik, Jitendra},
	year = {2015},
	pages = {133--149},
	annote = {The following values have no corresponding Zotero field:number: 2label: Gupta2015work-type: journal article}
}

@InProceedings{w._winterhalter_accurate_2015,
  author    = {W. {Winterhalter} and F. {Fleckenstein} and B. {Steder} and L. {Spinello} and W. {Burgard}},
  title     = {Accurate indoor localization for RGB-D smartphones and tablets given 2D floor plans},
  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2015},
  month     = sep,
  pages     = {3138--3143},
  doi       = {10.1109/IROS.2015.7353811},
  keywords  = {indoor navigation;maximum likelihood estimation;mobile computing;mobile robots;particle filtering (numerical methods);path planning;pose estimation;robot vision;smart phones;indoor localization;RGB-D smart phone;2D floor plan;location-based service;particle filter;6DoF pose estimation;sensor model;likelihood estimation;Google Tango device;robot navigation;Atmospheric measurements;Particle measurements;Robot sensing systems;Monte Carlo methods;Computational modeling;Robustness;Cameras},
}

@InProceedings{y._ling_dense_2015,
  author    = {Yonggen Ling and Shaojie Shen},
  title     = {Dense visual-inertial odometry for tracking of aggressive motions},
  booktitle = {2015 {IEEE} International Conference on Robotics and Biomimetics ({ROBIO})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = dec,
  pages     = {576--583},
  doi       = {10.1109/robio.2015.7418830},
  abstract  = {We propose a sliding window-based dense visual-inertial fusion method for real-time tracking of challenging aggressive motions. Our method combines recent advances in direct dense visual odometry, inertial measurement unit (IMU) preintegration, and graph-based optimization. At the front-end, direct dense visual odometry provides camera pose tracking that is resistant to motion blur. At the back-end, a sliding window optimization-based fusion framework with efficient IMU preintegration generates smooth and high-accuracy state estimates, even with occasional visual tracking failures. A local loop closure that is integrated into the back-end further eliminates drift after extremely aggressive motions. Our system runs real-time at 25 Hz on an off-the-shelf laptop. Experimental results show that our method is able to accurately track motions with angular velocities up to 1000 degrees/s and velocities up to 4 m/s. We also compare our method with state-of-the-art systems, such as Google Tango, and show superior performance during challenging motions. We show that our method achieves reliable tracking results, even if we throw the sensor suite during experiments.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  keywords  = {Visualization, image fusion, cameras, Robot sensing systems, pose estimation, graph theory, Optimization, distance measurement, Tracking, angular velocity, aggressive motion tracking, camera pose tracking, dense visual-inertial odometry, direct dense visual odometry, Google Tango, graph-based optimization, image motion analysis, image restoration, IMU preintegration, inertial measurement unit preintegration, local loop closure, Measurement uncertainty, motion blur, optimisation, real-time tracking, Reliability, sliding window optimization-based fusion framework, sliding window-based dense visual-inertial fusion method, state estimate, state estimation, visual tracking failure},
}

@InProceedings{v._lui_image_2015,
  author    = {Vincent Lui and Tom Drummond},
  title     = {Image based optimisation without global consistency for constant time monocular visual {SLAM}},
  booktitle = {2015 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  year      = {2015},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {5799--5806},
  doi       = {10.1109/icra.2015.7140011},
  abstract  = {This paper presents a monocular visual SLAM system that does not require a globally consistent 3D model. Instead of generating a globally consistent 3D model and localising the camera from the 3D model, the system merely optimises relative pose parameters for pairs of keyframes that overlap on the scene, providing accurate local information at the expense of global consistency. During run-time, the camera is localised using only 2D measurements from nearby keyframes instead of using correspondences between 2D measurements and 3D features of a 3D model. Extensive experiments using both synthetic and real data sets were performed to evaluate the system's performance. Results show that our system is accurate and runs in real time at an average of 25 frames per second on a standard computer. Finally, we also show how useful applications can be easily developed on top of a framework without global consistency.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2015 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords  = {mobile robots, robot vision, Visualization, Cameras, image sensors, Three-dimensional displays, SLAM (robots), stereo image processing, 3D model, solid modelling, Simultaneous localization and mapping, Solid modeling, Computational efficiency, Optimization, optimisation, 2D measurements, 3D features, camera localisation, constant time monocular visual SLAM, fully autonomous robots, image based optimisation, keyframes, monocular systems, real data sets, relative pose parameter optimisation, stereo-based systems, synthetic data sets},
}

@Article{tao_simpleflow:_2012,
  author  = {Tao, Michael and Bai, Jiamin and Kohli, Pushmeet and Paris, Sylvain},
  title   = {{SimpleFlow}: {A} {Non}-iterative, {Sublinear} {Optical} {Flow} {Algorithm}},
  journal = {Computer Graphics Forum},
  year    = {2012},
  volume  = {31},
  pages   = {345--353},
  issn    = {0167-7055},
  doi     = {10.1111/j.1467-8659.2012.03013.x},
  annote  = {The following values have no corresponding Zotero field:number: 2pt1},
}

@article{sun_quantitative_2013,
	title = {A {Quantitative} {Analysis} of {Current} {Practices} in {Optical} {Flow} {Estimation} and the {Principles} {Behind} {Them}},
	volume = {106},
	issn = {0920-5691 1573-1405},
	doi = {10.1007/s11263-013-0644-x},
	journal = {International Journal of Computer Vision},
	author = {Sun, Deqing and Roth, Stefan and Black, Michael J.},
	year = {2013},
	pages = {115--137},
	annote = {The following values have no corresponding Zotero field:number: 2}
}

@InProceedings{c._forster_svo:_2014,
  author    = {Christian Forster and Matia Pizzoli and Davide Scaramuzza},
  title     = {{SVO}: {Fast} semi-direct monocular visual odometry},
  booktitle = {2014 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  year      = {2014},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {15--22},
  doi       = {10.1109/ICRA.2014.6906584},
  abstract  = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state-estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2014 IEEE International Conference on Robotics and Automation (ICRA)},
  keywords  = {control engineering computing, robot vision, Cameras, Three-dimensional displays, stereo image processing, Feature extraction, probability, Robustness, Optimization, distance measurement, Tracking, autonomous aerial vehicles, motion estimation, 3D points, consumer laptop, embedded systems, fast semidirect monocular visual odometry, GPS-denied environments, high frame-rate motion estimation, micro-aerial-vehicle state-estimation, onboard embedded computer, open-source software, outlier measurements, pixel intensities, probabilistic mapping method, subpixel precision, SVO},
}

@InProceedings{d._nister_visual_2004,
  author    = {D. Nister and O. Naroditsky and J. Bergen},
  title     = {Visual odometry},
  booktitle = {Proceedings of the 2004 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 2004. {CVPR} 2004.},
  year      = {2004},
  volume    = {1},
  publisher = {{IEEE}},
  month     = jul,
  isbn      = {1063-6919},
  pages     = {I--652--I--659 Vol.1},
  doi       = {10.1109/CVPR.2004.1315094},
  abstract  = {We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on},
  keywords  = {aerial platforms, automotive platforms, autonomous ground vehicle, camera motion, camera trajectories, Cameras, Delay estimation, feature tracker, geometric hypothesize-and-test architecture, global positioning system, handheld platforms, Head, image matching, image trajectories, inertial navigation, Layout, motion estimation, navigation, pose estimation method, Real time systems, robust estimation, Robustness, single moving camera, stereo head, Tracking, vehicles, video cameras, video rate, video signal processing, visual odometry},
}

@Article{neubert_superpixel-based_2015,
  author  = {Neubert, Peer and Sünderhauf, Niko and Protzel, Peter},
  title   = {Superpixel-based appearance change prediction for long-term navigation across seasons},
  journal = {Robotics and Autonomous Systems},
  year    = {2015},
  volume  = {69},
  pages   = {15--27},
  issn    = {0921-8890},
  doi     = {10.1016/j.robot.2014.08.005},
}

@Article{milford_place_2015,
  author        = {Michael Milford and Hanme Kim and Michael Mangan and Stefan Leutenegger and Tom Stone and Barbara Webb and Andrew J. Davison},
  title         = {Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1505.04548},
  eprint        = {1505.04548},
  url           = {http://arxiv.org/abs/1505.04548},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/MilfordKMLSWD15},
  timestamp     = {Mon, 13 Aug 2018 16:46:39 +0200},
}

@InProceedings{m._j._milford_seqslam:_2012,
  author    = {Michael J. Milford and Gordon. F. Wyeth},
  title     = {{SeqSLAM}: {Visual} route-based navigation for sunny summer days and stormy winter nights},
  booktitle = {2012 {IEEE} International Conference on Robotics and Automation},
  year      = {2012},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {1643--1649},
  doi       = {10.1109/ICRA.2012.6224623},
  abstract  = {Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these \&\#x201C;local best matches\&\#x201D;. This approach removes the need for global matching performance by the vision front-end - instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100\% precision with recall rates of up to 60\%.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  keywords  = {mobile robots, robot vision, Visualization, feature extraction, Cameras, image sequences, SLAM (robots), path planning, Navigation, object recognition, Robot sensing systems, computer vision, image matching, Vectors, Trajectory, robotics, candidate matching location, car-mounted camera datasets, coherent sequence recognition, FAB-MAP, feature-based SLAM algorithms, feature-based techniques, global matching performance, local navigation sequence, place visual recognition, route recognition, SeqSLAM, simultaneous localization-and-mapping, stormy winter nights, sunny summer days, Videos, visual route-based navigation},
}

@article{arandjelovic_netvlad:_2015,
	title = {{NetVLAD}: {CNN} architecture for weakly supervised place recognition},
	volume = {abs/1511.07247},
	journal = {CoRR},
	author = {Arandjelovic, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomás and Sivic, Josef},
	year = {2015}
}

@InProceedings{j._luo_incremental_2007,
  author    = {J. Luo and A. Pronobis and B. Caputo and P. Jensfelt},
  title     = {Incremental learning for place recognition in dynamic environments},
  booktitle = {2007 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  year      = {2007},
  publisher = {{IEEE}},
  month     = oct,
  isbn      = {2153-0858},
  pages     = {721--728},
  doi       = {10.1109/IROS.2007.4398986},
  abstract  = {Vision-based place recognition is a desirable feature for an autonomous mobile system. In order to work in realistic scenarios, visual recognition algorithms should be adaptive, i.e. should be able to learn from experience and adapt continuously to changes in the environment. This paper presents a discriminative incremental learning approach to place recognition. We use a recently introduced version of the incremental SVM, which allows to control the memory requirements as the system updates its internal representation. At the same time, it preserves the recognition performance of the batch algorithm. In order to assess the method, we acquired a database capturing the intrinsic variability of places over time. Extensive experiments show the power and the potential of the approach.},
  annote    = {The following values have no corresponding Zotero field:alt-title: 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  keywords  = {mobile robots, robot vision, image recognition, learning (artificial intelligence), Databases, support vector machines, Robustness, Lighting, USA Councils, autonomous mobile system, batch algorithm, discriminative incremental learning approach, dynamic environments, incremental SVM, Intelligent robots, Notice of Violation, Testing, Training data, vision-based place recognition, visual recognition algorithms},
}

@Article{pronobis_realistic_2010,
  author  = {Pronobis, A. and Caputo, B. and Jensfelt, P. and Christensen, H. I.},
  title   = {A realistic benchmark for visual indoor place recognition},
  journal = {Robotics and Autonomous Systems},
  year    = {2010},
  volume  = {58},
  pages   = {81--96},
  issn    = {0921-8890},
  doi     = {10.1016/j.robot.2009.07.025},
  annote  = {The following values have no corresponding Zotero field:number: 1},
}

@InProceedings{m._m._ullah_towards_2008,
  author    = {M. M. Ullah and A. Pronobis and B. Caputo and J. Luo and P. Jensfelt and H. I. Christensen},
  title     = {Towards robust place recognition for robot localization},
  booktitle = {2008 {IEEE} International Conference on Robotics and Automation},
  year      = {2008},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {530--537},
  doi       = {10.1109/ROBOT.2008.4543261},
  abstract  = {Localization and context interpretation are two key competences for mobile robot systems. Visual place recognition, as opposed to purely geometrical models, holds promise of higher flexibility and association of semantics to the model. Ideally, a place recognition algorithm should be robust to dynamic changes and it should perform consistently when recognizing a room (for instance a corridor) in different geographical locations. Also, it should be able to categorize places, a crucial capability for transfer of knowledge and continuous learning. In order to test the suitability of visual recognition algorithms for these tasks, this paper presents a new database, acquired in three different labs across Europe. It contains image sequences of several rooms under dynamic changes, acquired at the same time with a perspective and omnidirectional camera, mounted on a socket. We assess this new database with an appearance- based algorithm that combines local features with support vector machines through an ad-hoc kernel. Results show the effectiveness of the approach and the value of the database.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
  keywords  = {Europe, mobile robots, robot vision, image recognition, learning (artificial intelligence), image sequences, support vector machines, Robustness, Image databases, Spatial databases, Solid modeling, robot localization, Testing, ad-hoc kernel, appearance-based algorithm, context interpretation, continuous learning, mobile robot systems, Visual databases, visual place recognition algorithm},
}

@Article{kostavelis_robot_2016,
  author  = {Kostavelis, Ioannis and Charalampous, Konstantinos and Gasteratos, Antonios and Tsotsos, John K.},
  title   = {Robot navigation via spatial and temporal coherent semantic maps},
  journal = {Engineering Applications of Artificial Intelligence},
  year    = {2016},
  volume  = {48},
  pages   = {173--187},
  issn    = {0952-1976},
  doi     = {10.1016/j.engappai.2015.11.004},
}

@article{c._cadena_robust_2012,
	title = {Robust {Place} {Recognition} {With} {Stereo} {Sequences}},
	volume = {28},
	issn = {1552-3098},
	doi = {10.1109/TRO.2012.2189497},
	abstract = {We propose a place recognition algorithm for simultaneous localization and mapping (SLAM) systems using stereo cameras that considers both appearance and geometric information of points of interest in the images. Both near and far scene points provide information for the recognition process. Hypotheses about loop closings are generated using a fast appearance-only technique based on the bag-of-words (BoW) method. We propose several important improvements to BoWs that profit from the fact that, in this problem, images are provided in sequence. Loop closing candidates are evaluated using a novel normalized similarity score that measures similarity in the context of recent images in the sequence. In cases where similarity is not sufficiently clear, loop closing verification is carried out using a method based on conditional random fields (CRFs). We build on CRF matching with two main novelties: We use both image and 3-D geometric information, and we carry out inference on a minimum spanning tree (MST), instead of a densely connected graph. Our results show that MSTs provide an adequate representation of the problem, with the additional advantages that exact inference is possible and that the computational cost of the inference process is limited. We compare our system with the state of the art using visual indoor and outdoor data from three different locations and show that our system can attain at least full precision (no false positives) for a higher recall (fewer false negatives).},
	journal = {IEEE Transactions on Robotics},
	author = {C. Cadena and D. Galvez-L and pez and J. D. Tardos and J. Neira},
	year = {2012},
	keywords = {Visualization, Cameras, Measurement, SLAM (robots), stereo image processing, Feature extraction, object recognition, computer vision, simultaneous localization and mapping, Robustness, stereo cameras, Robots, simultaneous localization and mapping (SLAM), trees (mathematics), Vectors, 3D geometric information, appearance-only technique, Bag of words (BoW), bag-of-words, BoW method, conditional random field, conditional random fields (CRFs), CRF, loop closing verification, minimum spanning tree, MST, normalized similarity score, random processes, recognition, robust place recognition, SLAM system, stereo sequences},
	pages = {871--885},
	annote = {The following values have no corresponding Zotero field:number: 4}
}

@article{milford_vision-based_2013,
	title = {Vision-based place recognition: how low can you go?},
	volume = {32},
	doi = {10.1177/0278364913490323},
	abstract = {In this paper we use the algorithm SeqSLAM to address the question, how little and what quality of visual information is needed to localize along a familiar route? We conduct a comprehensive investigation of place recognition performance on seven datasets while varying image resolution (primarily 1 to 512 pixel images), pixel bit depth, field of view, motion blur, image compression and matching sequence length. Results confirm that place recognition using single images or short image sequences is poor, but improves to match or exceed current benchmarks as the matching sequence length increases. We then present place recognition results from two experiments where low-quality imagery is directly caused by sensor limitations; in one, place recognition is achieved along an unlit mountain road by using noisy, long-exposure blurred images, and in the other, two single pixel light sensors are used to localize in an indoor environment. We also show failure modes caused by pose variance and sequence aliasing, and discuss ways in which they may be overcome. By showing how place recognition along a route is feasible even with severely degraded image sequences, we hope to provoke a re-examination of how we develop and test future localization and mapping systems.},
	journal = {The International Journal of Robotics Research},
	author = {Milford, Michael},
	month = jun,
	year = {2013},
	pages = {766--789},
	annote = {The following values have no corresponding Zotero field:number: 7}
}

@Article{espinace_indoor_2013,
  author  = {Espinace, P. and Kollar, T. and Roy, N. and Soto, A.},
  title   = {Indoor scene recognition by a mobile robot through adaptive object detection},
  journal = {Robotics and Autonomous Systems},
  year    = {2013},
  volume  = {61},
  pages   = {932--947},
  issn    = {0921-8890},
  doi     = {10.1016/j.robot.2013.05.002},
  annote  = {The following values have no corresponding Zotero field:number: 9},
}

@InProceedings{t._saito_mobile_2013,
  author    = {Takato Saito and Yoji Kuroda},
  title     = {Mobile robot localization using multiple observations based on place recognition and {GPS}},
  booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
  year      = {2013},
  publisher = {{IEEE}},
  month     = may,
  isbn      = {1050-4729},
  pages     = {1548--1553},
  doi       = {10.1109/ICRA.2013.6630776},
  abstract  = {In this paper, we propose a mobile robot localization system using multiple observations, which show the robot's global position. One of observations is GPS observation, the other is utilized an appearance based place recognition. Using GPS observations has still some challenging problems such as multipath and signal lost under the environments there is tall buildings nearby. It affects a significant error on localization. On the other hand, appearance based place recognition methods are efficient to recognize the robot's global position. It becomes possible to use a scene database with global position information. However, it could fail to function properly in natural environments like a lawn grass or trees in a park. We solve these demerits of each observations by using these multiple observations. Our system uses not only multiple observations but also dead reckoning with Gyrodometry model. As a result, the proposed localization system have achieved robust localization. To verify the validity of proposed method, our experiments using 1600m outdoor course in different seasons were conducted.},
  annote    = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation (ICRA), 2013 IEEE International Conference on},
  keywords  = {mobile robots, Cameras, Wheels, Global Positioning System, Image recognition, image matching, natural scenes, appearance-based place recognition, dead reckoning, Detectors, global position information, GPS observation, gyrodometry model, lawn grass, natural environments, park trees, robot global position recognition, robust mobile robot localization system, scene database},
}

@Article{yang_scene_2015,
  author  = {Yang, Jinfu and Zhang, Shanshan and Wang, Guanghui and Li, Mingai},
  title   = {Scene and place recognition using a hierarchical latent topic model},
  journal = {Neurocomputing},
  year    = {2015},
  volume  = {148},
  pages   = {578--586},
  issn    = {0925-2312},
  doi     = {10.1016/j.neucom.2014.07.005},
}

@Article{franz_biomimetic_2000,
  author   = {Franz, Matthias O. and Mallot, Hanspeter A.},
  title    = {Biomimetic robot navigation},
  journal  = {Robotics and Autonomous Systems},
  year     = {2000},
  volume   = {30},
  pages    = {133--153},
  issn     = {0921-8890},
  doi      = {10.1016/S0921-8890(99)00069-X},
  abstract = {In the past decade, a large number of robots has been built that explicitly implement biological navigation behaviours. We review these biomimetic approaches using a framework that allows for a common description of biological and technical navigation behaviour. The review shows that biomimetic systems make significant contributions to two fields of research: First, they provide a real world test of models of biological navigation behaviour; second, they make new navigation mechanisms available for technical applications, most notably in the field of indoor robot navigation. While simpler insect navigation behaviours have been implemented quite successfully, the more complicated way-finding capabilities of vertebrates still pose a challenge to current systems.},
  annote   = {The following values have no corresponding Zotero field:number: 1–2},
  keywords = {Cognitive map, Landmark, Robot navigation, Spatial behaviour, Topological map},
}

@article{h._cheng_topological_2015,
	title = {Topological {Indoor} {Localization} and {Navigation} for {Autonomous} {Mobile} {Robot}},
	volume = {12},
	issn = {1545-5955},
	doi = {10.1109/TASE.2014.2351814},
	abstract = {Mobile robot typically has limited on-board resources and may be applied in different indoor environment. Thus, it is necessary that they can learn a map and navigate themselves autonomously with lightweight algorithms. A novel topological map-building-based localization and navigation method is proposed in this paper. Based on the depth curve provided by a 3D sensor, a progressive Bayesian classifier is developed to realize direct corridor type identification. Instead of extracting features from single observation, information from multi-observations are fused to achieve a more robust performance. A topological map generation and loop closing method are proposed to build the environment map through autonomous exploration. Based on the derived map and the Markov localization method, the robot can then localize itself and navigate freely in the indoor environment. Experiments are performed on a recently built mobile robot system, and the results verify the effectiveness of the proposed methodology.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {H. Cheng and H. Chen and Y. Liu},
	year = {2015},
	keywords = {mobile robots, Feature extraction, path planning, Navigation, Robot sensing systems, Indoor environments, Bayes methods, indoor navigation, 3D sensor, autonomous mobile robot, depth curve, localization and navigation, loop closing method, map-building-based localization, Markov localization, Markov localization method, Markov processes, progressive Bayesian classifier, topological indoor localization, topological map generation},
	pages = {729--738},
	annote = {The following values have no corresponding Zotero field:number: 2}
}

@Article{forster_svo:_2017,
  author     = {Forster, C. and Zhang, Z. and Gassner, M. and Werlberger, M. and Scaramuzza, D.},
  title      = {{SVO}: {Semidirect} {Visual} {Odometry} for {Monocular} and {Multicamera} {Systems}},
  journal    = {IEEE Transactions on Robotics},
  year       = {2017},
  volume     = {33},
  number     = {2},
  month      = apr,
  pages      = {249--265},
  issn       = {1552-3098},
  doi        = {10.1109/TRO.2016.2623335},
  abstract   = {Direct methods for visual odometry (VO) have gained popularity for their capability to exploit information from all intensity gradients in the image. However, low computational speed as well as missing guarantees for optimality and consistency are limiting factors of direct methods, in which established feature-based methods succeed instead. Based on these considerations, we propose a semidirect VO (SVO) that uses direct methods to track and triangulate pixels that are characterized by high image gradients, but relies on proven feature-based methods for joint optimization of structure and motion. Together with a robust probabilistic depth estimation algorithm, this enables us to efficiently track pixels lying on weak corners and edges in environments with little or high-frequency texture. We further demonstrate that the algorithm can easily be extended to multiple cameras, to track edges, to include motion priors, and to enable the use of very large field of view cameras, such as fisheye and catadioptric ones. Experimental evaluation on benchmark datasets shows that the algorithm is significantly faster than the state of the art while achieving highly competitive accuracy.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/KMUFTZNT/7782863.html/\:text/html\:PDF:PDF},
  keywords   = {Visualization, Cameras, Feature extraction, image texture, sensor fusion, Robustness, Real-time systems, Optimization, Tracking, simultaneous localization and mapping (SLAM), monocular systems, SVO, feature-based methods, high-frequency texture, image intensity gradients, multicamera systems, probabilistic depth estimation algorithm, Robot vision, semidirect visual odometry},
  shorttitle = {{SVO}},
}

@Article{engel_direct_2018,
  author   = {Engel, J. and Koltun, V. and Cremers, D.},
  title    = {Direct {Sparse} {Odometry}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2018},
  volume   = {40},
  number   = {3},
  month    = mar,
  pages    = {611--625},
  issn     = {0162-8828},
  doi      = {10.1109/TPAMI.2017.2658577},
  abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/8A2J3IGN/7898369.html/\:text/html\:PDF:PDF},
  keywords = {Visualization, Computational modeling, Cameras, Three-dimensional displays, probability, Robustness, Optimization, distance measurement, Geometry, image motion analysis, optimisation, camera motion, consistent optimization, direct sparse odometry, fully direct probabilistic model, image sampling, joint optimization, motion formulation, reference frame, sample pixels, smooth intensity variations, visual odometry method, Visual odometry, SLAM, 3D reconstruction, structure from motion},
}

@InProceedings{buczko_how_2016,
  author    = {Buczko, M. and Willert, V.},
  title     = {How to distinguish inliers from outliers in visual odometry for high-speed automotive applications},
  booktitle = {2016 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
  year      = {2016},
  month     = jun,
  pages     = {478--483},
  doi       = {10.1109/IVS.2016.7535429},
  abstract  = {In this paper, we present an outlier removal scheme for stereo-based visual odometry which is especially suited for improving high-speed pose change estimations in large-scale depth environments. First we investigate the variance of the reprojection error on the 3D position of a feature given a fixed error in pose change to conclude that a detection of outliers based on a fixed threshold on the reprojection error is inappropriate. Then we propose an optical flow dependent feature-adaptive scaling of the reprojection error to reach almost invariance to the 3D position of each feature. This feature-adaptive scaling is derived from an approximation showing the relation between longitudinal pose change of the camera, absolute value of the optical flow, and distance of the feature. Using this scaling, we develop an iterative alternating scheme to guide the separation of inliers from outliers. It optimizes the tradeoff between finding a good criterion to remove outliers based on a given pose change and improving the pose change hypothesis based on the current set of inliers. Including the new outlier removal scheme into a pure two-frame stereo-based visual odometry pipeline without applying bundle adjustment or SLAM-filtering we are currently ranked amongst the top camera-based algorithms and furthermore outperform camera and laser scanner methods in Kitti benchmark's high-speed scenarios.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/XHJ352AV/7535429.html/\:text/html\:PDF:PDF},
  keywords  = {Visualization, Cameras, Three-dimensional displays, image sequences, stereo image processing, Feature extraction, iterative methods, distance measurement, Adaptive optics, Measurement uncertainty, automotive engineering, Bars, bundle adjustment, camera-based algorithms, feature 3D position, high-speed automotive applications, iterative alternating scheme, laser scanner methods, optical flow dependent feature-adaptive scaling, outlier detection, outlier removal scheme, reprojection error, SLAM-filtering, stereo-based visual odometry, two-frame stereo-based visual odometry pipeline},
}

@InProceedings{buczko_monocular_2017,
  author    = {Buczko, M. and Willert, V.},
  title     = {Monocular {Outlier} {Detection} for {Visual} {Odometry}},
  booktitle = {2017 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
  year      = {2017},
  month     = jun,
  pages     = {739--745},
  doi       = {10.1109/IVS.2017.7995805},
  abstract  = {In this paper, we propose an optimization scheme to detect outliers in a visual odometry pipeline that is purely based on the optical flow of one monocular camera without requiring information about depth. First, we review different optical flow error measures and uncover the different sensitivity changes of the endpoint error for optical flow to the directional and the absolute error components. Then, we analyse the dependency of the direction of the flow induced by the translational components of the camera motion assuming the flow induced by the rotational camera motion components to be known. Based on a reliable estimate of the focus of expansion, we solve for a normalized directional flow error that takes the influence of the discretization error induced by the pixel size into account. Here, a circular and a squared error bound on the discretization error is investigated. Finally, we present a monocular outlier detection pipeline including the normalized directional flow error as a suitable constant threshold criterion almost invariant to the camera motion and the scene geometry. Further on, evaluations of the overall monocular outlier detection method called FERO included in a stereo visual odometry pipeline are given to compare the performance of monocular against stereoscopic outlier detection based on Kitti benchmark.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/R5XBKUM3/7995805.html/\:text/html\:PDF:PDF},
  keywords  = {Visualization, Cameras, image sequences, Optical imaging, Optical sensors, stereo image processing, cameras, distance measurement, Adaptive optics, motion estimation, monocular camera, Measurement uncertainty, optimisation, absolute error components, circular error bound, constant threshold criterion, directional error components, discretisation error, endpoint error, FERO, monocular outlier detection, normalized directional flow error, optical flow error measures, Optical variables measurement, optimization scheme, pixel size, rotational camera motion components, scene geometry, squared error bound, stereo visual odometry pipeline, translational components},
}

@Article{hartley_triangulation_1997,
  author   = {Hartley, Richard I. and Sturm, Peter},
  title    = {Triangulation},
  journal  = {Computer Vision and Image Understanding},
  year     = {1997},
  volume   = {68},
  number   = {2},
  month    = nov,
  pages    = {146--157},
  issn     = {1077-3142},
  doi      = {10.1006/cviu.1997.0547},
  url      = {http://www.sciencedirect.com/science/article/pii/S1077314297905476},
  urldate  = {2018-04-19},
  abstract = {In this paper, we consider the problem of finding the position of a point in space given its position in two images taken with cameras with known calibration and pose. This process requires the intersection of two known rays in space and is commonly known as triangulation. In the absence of noise, this problem is trivial. When noise is present, the two rays will not generally meet, in which case it is necessary to find the best point of intersection. This problem is especially critical in affine and projective reconstruction in which there is no meaningful metric information about the object space. It is desirable to find a triangulation method that is invariant to projective transformations of space. This paper solves that problem by assuming a Gaussian noise model for perturbation of the image coordinates. The triangulation problem may then be formulated as a least-squares minimization problem. In this paper a noniterative solution is given that finds the global minimum. It is shown that in certain configurations, local minima occur, which are avoided by the new method. Extensive comparisons of the new method with several other methods show that it consistently gives superior results.},
  file     = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/J2ZI3JU7/S1077314297905476.html/\:text/html\:PDF:PDF},
}

@Article{yang_challenges_2017,
  author     = {Yang, Nan and Wang, Rui and Gao, Xiang and Cremers, Daniel},
  title      = {Challenges in {Monocular} {Visual} {Odometry}: {Photometric} {Calibration}, {Motion} {Bias} and {Rolling} {Shutter} {Effect}},
  journal    = {arXiv:1705.04300 [cs]},
  year       = {2017},
  month      = may,
  note       = {arXiv: 1705.04300},
  url        = {http://arxiv.org/abs/1705.04300},
  urldate    = {2018-04-19},
  abstract   = {Monocular visual odometry (VO) has seen tremendous improvements in accuracy, robustness and efficiency, and has gained exponential popularity over recent years. Nevertheless, no comprehensive evaluations have been performed to reveal the influences of the three easily overlooked, yet very influential aspects: photometric calibration, motion bias and rolling shutter effect. In this work, we evaluate these three aspects quantitatively on the state of the art of direct, feature-based and semi-direct methods, providing the community with useful practical knowledge both for better applying existing methods and developing new algorithms of VO and SLAM. Conclusions (some of which are counterintuitive) are drawn with insightful technical and empirical analyses to all of our experiments. Possible improvements on existing methods are directed or proposed, such as a sub-pixel accuracy refinement of ORB-SLAM which boosts its performance.},
  annote     = {Comment: The first two authors contribute equally to this paper},
  file       = {html\:PDF:html\:arXiv.org Snapshot/\:C//\:/Users/Psykie/Zotero/storage/7EM7YFTJ/1705.html/\:text/html\:PDF:PDF},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  shorttitle = {Challenges in {Monocular} {Visual} {Odometry}},
}

@Article{wu_framework_2017,
  author   = {Wu, M. and Lam, S. K. and Srikanthan, T.},
  title    = {A {Framework} for {Fast} and {Robust} {Visual} {Odometry}},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2017},
  volume   = {18},
  number   = {12},
  month    = dec,
  pages    = {3433--3448},
  issn     = {1524-9050},
  doi      = {10.1109/TITS.2017.2685433},
  abstract = {Knowledge of the ego-vehicle's motion state is essential for assessing the collision risk in advanced driver assistance systems or autonomous driving. Vision-based methods for estimating the ego-motion of vehicle, i.e., visual odometry, face a number of challenges in uncontrolled realistic urban environments. Existing solutions fail to achieve a good tradeoff between high accuracy and low computational complexity. In this paper, a framework for ego-motion estimation that integrates runtime-efficient strategies with robust techniques at various core stages in visual odometry is proposed. First, a pruning method is employed to reduce the computational complexity of Kanade-Lucas-Tomasi (KLT) feature detection without compromising on the quality of the features. Next, three strategies, i.e., smooth motion constraint, adaptive integration window technique, and automatic tracking failure detection scheme, are introduced into the conventional KLT tracker to facilitate generation of feature correspondences in a robust and runtime efficient way. Finally, an early termination condition for the random sample consensus (RANSAC) algorithm is integrated with the Gauss-Newton optimization scheme to enable rapid convergence of the motion estimation process while achieving robustness. Experimental results based on the KITTI odometry data set show that the proposed technique outperforms the state-of-the-art visual odometry methods by producing more accurate ego-motion estimation in notably lesser amount of time.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/IGYFKPSC/8094876.html/\:text/html\:PDF:PDF},
  keywords = {autonomous driving, Visualization, feature extraction, image sequences, Feature extraction, driver information systems, advanced driver assistance systems, autonomous vehicles, computer vision, RANSAC algorithm, collision avoidance, Autonomous vehicles, distance measurement, Motion estimation, Tracking, motion estimation, tracking, optimisation, feature detection, feature tracking, accurate ego-motion estimation, adaptive integration window technique, ADASs, automatic tracking failure detection scheme, Collision avoidance, collision risk, computational complexity, Computational complexity, conventional KLT tracker, core stages, ego-motion, ego-vehicle motion state, fast odometry, feature correspondences, Gauss-Newton optimization scheme, Kanade-Lucas-Tomasi, KITTI odometry data, KLT feature detection, low computational complexity, motion estimation process, pruning method, random sample consensus algorithm, robust runtime efficient way, robust techniques, robust visual odometry, runtime-efficient strategies, smooth motion constraint, uncontrolled realistic urban environments, Visual odometry, visual odometry methods},
}

@InProceedings{lovegrove_accurate_2011,
  author    = {Lovegrove, S. and Davison, A. J. and Ibañez-Guzmán, J.},
  title     = {Accurate visual odometry from a rear parking camera},
  booktitle = {2011 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
  year      = {2011},
  month     = jun,
  pages     = {788--793},
  doi       = {10.1109/IVS.2011.5940546},
  abstract  = {As an increasing number of automatic safety and navigation features are added to modern vehicles, the crucial job of providing real-time localisation is predominantly performed by a single sensor, GPS, despite its well-known failings, particularly in urban environments. Various attempts have been made to supplement GPS to improve localisation performance, but these usually require additional specialised and expensive sensors. Offering increased value to vehicle OEMs, we show that it is possible to use just the video stream from a rear parking camera to produce smooth and locally accurate visual odometry in real-time. We use an efficient whole image alignment approach based on ESM, taking account of both the difficulties and advantages of the fact that a parking camera views only the road surface directly behind a vehicle. Visual odometry is complementary to GPS in offering localisation information at 30 Hz which is smooth and highly accurate locally whilst GPS is course but offers absolute measurements. We demonstrate our system in a large scale experiment covering real urban driving. We also present real-time fusion of our visual estimation with automotive GPS to generate a commodity-cost localisation solution which is smooth, accurate and drift free in global coordinates.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/9RNSFPGV/5940546.html/\:text/html\:PDF:PDF},
  keywords  = {road vehicles, Roads, traffic engineering computing, Visualization, Cameras, video signal processing, image fusion, Pixel, road safety, Vehicles, cameras, Global Positioning System, Equations, distance measurement, visual odometry, frequency 30 Hz, automotive GPS, commodity-cost localisation solution, embedded system method, ESM algorithm, image alignment approach, rear parking camera, vehicle navigation feature, vehicle safety feature, video stream, visual estimation fusion},
}

@WWW{nvidia_corporation_autonomous_nodate,
  author   = {{NVIDIA Corporation}},
  title    = {Autonomous {Car} {Development} {Platform} from {NVIDIA} {DRIVE} {PX}2},
  year     = {2018},
  url      = {https://www.nvidia.com/en-us/self-driving-cars/drive-platform/},
  language = {en-us},
  urldate  = {2018-04-20},
  abstract = {An AI car computing platform that enables automakers and their tier 1 suppliers to accelerate production of automated and autonomous vehicles.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/SBSU432I/drive-platform.html/\:text/html\:PDF:PDF},
  journal  = {NVIDIA},
}

@WWW{agarwal_ceres_nodate,
  author       = {Sameer Agarwal and Keir Mierle and Others},
  title        = {Ceres Solver},
  year         = {2019},
  url          = {http://ceres-solver.org/},
  howpublished = {\url{http://ceres-solver.org}},
}

@Article{mueggler_event-camera_2017,
  author     = {Mueggler, Elias and Rebecq, Henri and Gallego, Guillermo and Delbruck, Tobi and Scaramuzza, Davide},
  title      = {The event-camera dataset and simulator: {Event}-based data for pose estimation, visual odometry, and {SLAM}},
  journal    = {The International Journal of Robotics Research},
  year       = {2017},
  language   = {en},
  volume     = {36},
  number     = {2},
  month      = feb,
  pages      = {142--149},
  issn       = {0278-3649},
  doi        = {10.1177/0278364917691115},
  urldate    = {2018-04-20},
  abstract   = {New vision sensors, such as the dynamic and active-pixel vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called “events”) and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of ego-motion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e. rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.},
  shorttitle = {The event-camera dataset and simulator},
}

@Article{vidal_ultimate_2018,
  author     = {Vidal, A. R. and Rebecq, H. and Horstschaefer, T. and Scaramuzza, D.},
  title      = {Ultimate {SLAM}? {Combining} {Events}, {Images}, and {IMU} for {Robust} {Visual} {SLAM} in {HDR} and {High}-{Speed} {Scenarios}},
  journal    = {IEEE Robotics and Automation Letters},
  year       = {2018},
  volume     = {3},
  number     = {2},
  month      = apr,
  pages      = {994--1001},
  doi        = {10.1109/LRA.2018.2793357},
  abstract   = {Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this letter, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130\% over event-only pipelines, and 85\% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate-to the best of our knowledge-the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high dynamic range scenes. Videos of the experiments: http://rpg.ifi.uzh.ch/ultimateslam.html.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/63S9IBKL/8258997.html/\:text/html\:PDF:PDF},
  keywords   = {State estimation, mobile robots, robot vision, Cameras, image sensors, SLAM, SLAM (robots), Standards, cameras, distance measurement, autonomous aerial vehicles, IMU, motion estimation, Robot vision systems, motion blur, state estimation, aerial systems: perception and autonomy, aircraft control, autonomous quadrotor flight, bioinspired vision sensors, event camera dataset, event-only pipelines, HDR, helicopters, high dynamic range, high-speed motions, high-speed scenarios, hybrid pipeline, inertial systems, low light scenes, low-light environments, Pipelines, pixel-level brightness, robust visual SLAM, standard intensity frames, state estimation pipeline, ultimate SLAM, visual-based navigation, visual-inertial odometry, visual-inertial systems},
  shorttitle = {Ultimate {SLAM}?},
}

@Article{liu_stereo_2016,
  author   = {Liu, Y. and Xiong, R. and Wang, Y. and Huang, H. and Xie, X. and Liu, X. and Zhang, G.},
  title    = {Stereo {Visual}-{Inertial} {Odometry} {With} {Multiple} {Kalman} {Filters} {Ensemble}},
  journal  = {IEEE Transactions on Industrial Electronics},
  year     = {2016},
  volume   = {63},
  number   = {10},
  month    = oct,
  pages    = {6205--6216},
  issn     = {0278-0046},
  doi      = {10.1109/TIE.2016.2573765},
  abstract = {In this paper, we present a stereo visual-inertial odometry algorithm assembled with three separated Kalman filters, i.e., attitude filter, orientation filter, and position filter. Our algorithm carries out the orientation and position estimation with three filters working on different fusion intervals, which can provide more robustness even when the visual odometry estimation fails. In our orientation estimation, we propose an improved indirect Kalman filter, which uses the orientation error space represented by unit quaternion as the state of the filter. The performance of the algorithm is demonstrated through extensive experimental results, including the benchmark KITTI datasets and some challenging datasets captured in a rough terrain campus.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/3KQC5ZQH/7480432.html/\:text/html\:PDF:PDF},
  keywords = {robot vision, Visualization, Estimation, Sensors, stereo image processing, Kalman filters, pose estimation, Kalman filter, distance measurement, position estimation, visual-inertial odometry, benchmark KITTI datasets, Filtering algorithms, Fuses, fusion intervals, indirect Kalman filter, Information filtering, multi-sensor fusion, multiple Kalman filters ensemble, orientation error space, orientation estimation, rough terrain campus, separated Kalman filters, stereo visual-inertial odometry, unit quaternion},
}

@TechReport{tomasi_detection_1991,
  author      = {Tomasi, Carlo and Kanade, Takeo},
  title       = {Detection and {Tracking} of {Point} {Features}},
  institution = {Carnegie Mellon University},
  year        = {1991},
  language    = {en},
  number      = {CMU-CS-91-132},
  month       = apr,
  pages       = {22},
  address     = {Pittsburgh, PA},
  school      = {Carnegie Mellon University},
}

@Article{sun_robust_2018,
  author   = {Sun, K. and Mohta, K. and Pfrommer, B. and Watterson, M. and Liu, S. and Mulgaonkar, Y. and Taylor, C. J. and Kumar, V.},
  title    = {Robust {Stereo} {Visual} {Inertial} {Odometry} for {Fast} {Autonomous} {Flight}},
  journal  = {IEEE Robotics and Automation Letters},
  year     = {2018},
  volume   = {3},
  number   = {2},
  month    = apr,
  pages    = {965--972},
  doi      = {10.1109/LRA.2018.2793349},
  abstract = {In recent years, vision-aided inertial odometry for state estimation has matured significantly. However, we still encounter challenges in terms of improving the computational efficiency and robustness of the underlying algorithms for applications in autonomous flight with microaerial vehicles, in which it is difficult to use high-quality sensors and powerful processors because of constraints on size and weight. In this letter, we present a filter-based stereo visual inertial odometry that uses the multistate constraint Kalman filter. Previous work on the stereo visual inertial odometry has resulted in solutions that are computationally expensive. We demonstrate that our stereo multistate constraint Kalman filter (S-MSCKF) is comparable to state-of-the-art monocular solutions in terms of computational cost, while providing significantly greater robustness. We evaluate our S-MSCKF algorithm and compare it with state-of-the-art methods including OKVIS, ROVIO, and VINS-MONO on both the EuRoC dataset and our own experimental datasets demonstrating fast autonomous flight with a maximum speed of 17.5 m/s in indoor and outdoor environments. Our implementation of the S-MSCKF is available at https://github.com/KumarRobotics/msckf\_vio.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/4ZG4B89C/8258858.html/\:text/html\:PDF:PDF},
  keywords = {mobile robots, robot vision, Visualization, Cameras, SLAM, stereo image processing, Kalman filters, Robustness, Real-time systems, distance measurement, state estimation, Localization, aerial systems: perception and autonomy, computational efficiency, Covariance matrices, EuRoC dataset, fast autonomous flight, high-quality sensors, image filtering, Quaternions, robust stereo visual inertial odometry, S-MSCKF, stereo multistate constraint Kalman filter},
}

@Article{burri_euroc_2016,
  author   = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus W and Siegwart, Roland},
  title    = {The {EuRoC} micro aerial vehicle datasets},
  journal  = {The International Journal of Robotics Research},
  year     = {2016},
  language = {en},
  volume   = {35},
  number   = {10},
  month    = sep,
  pages    = {1157--1163},
  issn     = {0278-3649},
  doi      = {10.1177/0278364915620033},
  urldate  = {2018-04-20},
  abstract = {This paper presents visual-inertial datasets collected on-board a micro aerial vehicle. The datasets contain synchronized stereo images, IMU measurements and accurate ground truth. The first batch of datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data. It was collected in an industrial environment and contains millimeter accurate position ground truth from a laser tracking system. The second batch of datasets is aimed at precise 3D environment reconstruction and was recorded in a room equipped with a motion capture system. The datasets contain 6D pose ground truth and a detailed 3D scan of the environment. Eleven datasets are provided in total, ranging from slow flights under good visual conditions to dynamic flights with motion blur and poor illumination, enabling researchers to thoroughly test and evaluate their algorithms. All datasets contain raw sensor measurements, spatio-temporally aligned sensor data and ground truth, extrinsic and intrinsic calibrations and datasets for custom calibrations.},
}

@InProceedings{delmerico_benchmark_2018,
  author    = {Jeffrey Delmerico and Davide Scaramuzza},
  title     = {A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2018},
  publisher = {{IEEE}},
  month     = may,
  pages     = {2502--2509},
  doi       = {10.1109/ICRA.2018.8460664},
  issn      = {2577-087X},
  keywords  = {aerospace robotics;cameras;Global Positioning System;image capture;mobile robots;pose estimation;robot vision;state estimation;monocular visual-inertial odometry algorithms;state estimation algorithms;computational constraints;inertial measurement units;VIO algorithms;single-board computer systems;flying robots;pose estimation;cameras;IMUs;motion capture;global positioning systems;MSCKF;OKVIS;ROVIO;VINS-Mono;SVO-MSF;SVO-GTSAM;hardware configurations;EuRoC datasets;six degree of freedom;6 DoF;State estimation;Visualization;Optimization;Pipelines;Hardware;Robot sensing systems},
}

@Article{forster_-manifold_2017,
  author   = {Forster, C. and Carlone, L. and Dellaert, F. and Scaramuzza, D.},
  title    = {On-{Manifold} {Preintegration} for {Real}-{Time} {Visual}–{Inertial} {Odometry}},
  journal  = {IEEE Transactions on Robotics},
  year     = {2017},
  volume   = {33},
  number   = {1},
  month    = feb,
  pages    = {1--21},
  issn     = {1552-3098},
  doi      = {10.1109/TRO.2016.2597321},
  abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a posteriori bias correction in analytic form. The second contribution is to show that the preintegrated inertial measurement unit model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3-D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modeling effort leads to an accurate state estimation in real time, outperforming state-of-the-art approaches.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/8JJHZ95C/7557075.html/\:text/html\:PDF:PDF},
  keywords = {Computational modeling, Estimation, Computer vision, sensor fusion, computer vision, Real-time systems, graph theory, Optimization, distance measurement, Jacobian matrices, maximum likelihood estimation, inertial systems, computerised instrumentation, factor graph, incremental-smoothing algorithm, Manifolds, maximum a posteriori state estimation, monocular VIO pipeline evaluation, nonlinear optimization, nonlinear programming, on-manifold preintegration theory, preintegrated inertial measurement unit model, real-time visual-inertial odometry, Smoothing methods, structureless model, visual measurement, visual–inertial odometry (VIO)},
}

@Article{faessler_autonomous_2016,
  author     = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Mueggler, Elias and Pizzoli, Matia and Scaramuzza, Davide},
  title      = {Autonomous, {Vision}-based {Flight} and {Live} {Dense} 3D {Mapping} with a {Quadrotor} {Micro} {Aerial} {Vehicle}: {Autonomous}, {Vision}-based {Flight} and {Live} {Dense} 3D {Mapping}},
  journal    = {Journal of Field Robotics},
  year       = {2016},
  language   = {en},
  volume     = {33},
  number     = {4},
  month      = jun,
  pages      = {431--450},
  issn       = {1556-4959},
  doi        = {10.1002/rob.21581},
  urldate    = {2018-04-20},
  shorttitle = {Autonomous, {Vision}-based {Flight} and {Live} {Dense} 3D {Mapping} with a {Quadrotor} {Micro} {Aerial} {Vehicle}},
}

@InProceedings{mourikis_multi-state_2007,
  author    = {Mourikis, A. I. and Roumeliotis, S. I.},
  title     = {A {Multi}-{State} {Constraint} {Kalman} {Filter} for {Vision}-aided {Inertial} {Navigation}},
  booktitle = {Proceedings 2007 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
  year      = {2007},
  month     = apr,
  pages     = {3565--3572},
  doi       = {10.1109/ROBOT.2007.364024},
  abstract  = {In this paper, we present an extended Kalman filter (EKF)-based algorithm for real-time vision-aided inertial navigation. The primary contribution of this work is the derivation of a measurement model that is able to express the geometric constraints that arise when a static feature is observed from multiple camera poses. This measurement model does not require including the 3D feature position in the state vector of the EKF and is optimal, up to linearization errors. The vision-aided inertial navigation algorithm we propose has computational complexity only linear in the number of features, and is capable of high-precision pose estimation in large-scale real-world environments. The performance of the algorithm is demonstrated in extensive experimental results, involving a camera/IMU system localizing within an urban area.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/VAS97BQ4/4209642.html/\:text/html\:PDF:PDF},
  keywords  = {feature extraction, Cameras, Position measurement, Simultaneous localization and mapping, Kalman filters, pose estimation, extended Kalman filter, Large-scale systems, Solid modeling, Motion estimation, Vectors, Motion measurement, inertial navigation, Computational complexity, 3D feature position, camera pose, geometric constraints, Inertial navigation, linear computational complexity, multistate constraint Kalman filter, state vector, static feature, vision-aided inertial navigation},
}

@Article{leutenegger_keyframe-based_2015,
  author   = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
  title    = {Keyframe-based visual–inertial odometry using nonlinear optimization},
  journal  = {The International Journal of Robotics Research},
  year     = {2015},
  language = {en},
  volume   = {34},
  number   = {3},
  month    = mar,
  pages    = {314--334},
  issn     = {0278-3649},
  doi      = {10.1177/0278364914554813},
  urldate  = {2018-04-20},
  abstract = {Combining visual and inertial measurements has become popular in mobile robotics, since the two sensing modalities offer complementary characteristics that make them the ideal choice for accurate visual–inertial odometry or simultaneous localization and mapping (SLAM). While historically the problem has been addressed with filtering, advancements in visual estimation suggest that nonlinear optimization offers superior accuracy, while still tractable in complexity thanks to the sparsity of the underlying problem. Taking inspiration from these findings, we formulate a rigorously probabilistic cost function that combines reprojection errors of landmarks and inertial terms. The problem is kept tractable and thus ensuring real-time operation by limiting the optimization to a bounded window of keyframes through marginalization. Keyframes may be spaced in time by arbitrary intervals, while still related by linearized inertial terms. We present evaluation results on complementary datasets recorded with our custom-built stereo visual–inertial hardware that accurately synchronizes accelerometer and gyroscope measurements with imagery. A comparison of both a stereo and monocular version of our algorithm with and without online extrinsics estimation is shown with respect to ground truth. Furthermore, we compare the performance to an implementation of a state-of-the-art stochastic cloning sliding-window filter. This competitive reference implementation performs tightly coupled filtering-based visual–inertial odometry. While our approach declaredly demands more computation, we show its superior performance in terms of accuracy.},
}

@Article{qin_vins-mono:_2017,
  author   = {Tong Qin and Peiliang Li and Shaojie Shen},
  title    = {VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator},
  journal  = {IEEE Transactions on Robotics},
  year     = {2018},
  volume   = {34},
  number   = {4},
  month    = aug,
  pages    = {1004--1020},
  issn     = {1552-3098},
  doi      = {10.1109/TRO.2018.2853729},
  keywords = {aerospace control;autonomous aerial vehicles;closed loop systems;distance measurement;graph theory;mobile robots;optimisation;robot vision;state estimation;tightly coupled optimization-based method;visual-inertial odometry;IMU;iOS-based demonstration;microaerial-vehicle platform;closed-loop autonomous flight;loop detection module;VINS;versatile monocular visual-inertial state estimator;inertial measurement unit;graph optimization;Cameras;Optimization;Visualization;Feature extraction;Robustness;Robot sensing systems;Monocular visual-inertial systems (VINSs);state estimation;sensor fusion;simultaneous localization and mapping},
}

@InCollection{konolige_large-scale_2010,
  author    = {Konolige, Kurt and Agrawal, Motilal and Solà, Joan},
  title     = {Large-{Scale} {Visual} {Odometry} for {Rough} {Terrain}},
  booktitle = {Robotics {Research}},
  year      = {2010},
  language  = {en},
  series    = {Springer {Tracts} in {Advanced} {Robotics}},
  publisher = {Springer, Berlin, Heidelberg},
  isbn      = {978-3-642-14742-5 978-3-642-14743-2},
  pages     = {201--212},
  doi       = {10.1007/978-3-642-14743-2_18},
  urldate   = {2018-04-20},
  abstract  = {SummaryMotion estimation from stereo imagery, sometimes called visual odometry, is a well-known process. However, it is difficult to achieve good performance using standard techniques. We present the results of several years of work on an integrated system to localize a mobile robot in rough outdoor terrain using visual odometry, with an increasing degree of precision. We discuss issues that are important for real-time, high-precision performance: choice of features, matching strategies, incremental bundle adjustment, and filtering with inertial measurement sensors. Using data with ground truth from an RTK GPS system, we show experimentally that our algorithms can track motion, in off-road terrain, over distances of 10 km, with an error of less than 10 m (0.1\%).},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/N82MD5GJ/978-3-642-14743-2_18.html/\:text/html\:PDF:PDF},
}

@Article{gui_review_2015,
  author   = {Gui, Jianjun and Gu, Dongbing and Wang, Sen and Hu, Huosheng},
  title    = {A review of visual inertial odometry from filtering and optimisation perspectives},
  journal  = {Advanced Robotics},
  year     = {2015},
  language = {en},
  volume   = {29},
  number   = {20},
  month    = oct,
  pages    = {1289--1301},
  issn     = {0169-1864, 1568-5535},
  doi      = {10.1080/01691864.2015.1057616},
  urldate  = {2018-04-20},
}

@InProceedings{kerl_robust_2013,
  author    = {Kerl, C. and Sturm, J. and Cremers, D.},
  title     = {Robust odometry estimation for {RGB}-{D} cameras},
  booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
  year      = {2013},
  month     = may,
  pages     = {3748--3754},
  doi       = {10.1109/ICRA.2013.6631104},
  abstract  = {The goal of our work is to provide a fast and accurate method to estimate the camera motion from RGB-D images. Our approach registers two consecutive RGB-D frames directly upon each other by minimizing the photometric error. We estimate the camera motion using non-linear minimization in combination with a coarse-to-fine scheme. To allow for noise and outliers in the image data, we propose to use a robust error function that reduces the influence of large residuals. Furthermore, our formulation allows for the inclusion of a motion model which can be based on prior knowledge, temporal filtering, or additional sensors like an IMU. Our method is attractive for robots with limited computational resources as it runs in real-time on a single CPU core and has a small, constant memory footprint. In an extensive set of experiments carried out both on a benchmark dataset and synthetic data, we demonstrate that our approach is more accurate and robust than previous methods. We provide our software under an open source license.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/T2VWQGL6/6631104.html/\:text/html\:PDF:PDF},
  keywords  = {Convergence, RGB-D images, Sensors, cameras, filtering theory, Robustness, Software, Licenses, image registration, Jacobian matrices, IMU, motion estimation, open source license, RGB-D cameras, camera motion estimation, coarse-to-fine scheme, computational resources, constant memory footprint, CPU core, image data, motion model, nonlinear minimization, Octrees, photometric error, photometry, RGB-D frames, robust error function, robust odometry estimation, sensors, temporal filtering},
}

@InProceedings{steinbrucker_real-time_2011,
  author    = {Steinbrücker, F. and Sturm, J. and Cremers, D.},
  title     = {Real-time visual odometry from dense {RGB}-{D} images},
  booktitle = {2011 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCV} {Workshops})},
  year      = {2011},
  month     = nov,
  pages     = {719--722},
  doi       = {10.1109/ICCVW.2011.6130321},
  abstract  = {We present an energy-based approach to visual odometry from RGB-D images of a Microsoft Kinect camera. To this end we propose an energy function which aims at finding the best rigid body motion to map one RGB-D image into another one, assuming a static scene filmed by a moving camera. We then propose a linearization of the energy function which leads to a 6×6 normal equation for the twist coordinates representing the rigid body motion. To allow for larger motions, we solve this equation in a coarse-to-fine scheme. Extensive quantitative analysis on recently proposed benchmark datasets shows that the proposed solution is faster than a state-of-the-art implementation of the iterative closest point (ICP) algorithm by two orders of magnitude. While ICP is more robust to large camera motion, the proposed method gives better results in the regime of small displacements which are often the case in camera tracking applications.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/Q9BJGY7J/6130321.html/\:text/html\:PDF:PDF},
  keywords  = {Visualization, Cameras, Iterative closest point algorithm, Three dimensional displays, image processing, Equations, iterative methods, Robots, distance measurement, camera motion, coarse-to-fine scheme, camera tracking applications, dense RGB-D images, energy function, iterative closest point algorithm, Microsoft Kinect camera, realtime visual odometry, rigid body motion, Streaming media, target tracking, twist coordinates},
}

@InProceedings{elgammal_non-parametric_2000,
  author    = {Elgammal, Ahmed and Harwood, David and Davis, Larry},
  title     = {Non-parametric {Model} for {Background} {Subtraction}},
  booktitle = {Computer {Vision} — {ECCV} 2000},
  year      = {2000},
  language  = {en},
  series    = {Lecture {Notes} in {Computer} {Science}},
  publisher = {Springer, Berlin, Heidelberg},
  month     = jun,
  isbn      = {978-3-540-67686-7 978-3-540-45053-5},
  pages     = {751--767},
  doi       = {10.1007/3-540-45053-X_48},
  urldate   = {2018-04-20},
  abstract  = {Background subtraction is a method typically used to segment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the background of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detection of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates.},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/BQGWRXNZ/3-540-45053-X_48.html/\:text/html\:PDF:PDF},
}

@Article{kim_effective_2016,
  author   = {Kim, D. H. and Kim, J. H.},
  title    = {Effective {Background} {Model}-{Based} {RGB}-{D} {Dense} {Visual} {Odometry} in a {Dynamic} {Environment}},
  journal  = {IEEE Transactions on Robotics},
  year     = {2016},
  volume   = {32},
  number   = {6},
  month    = dec,
  pages    = {1565--1573},
  issn     = {1552-3098},
  doi      = {10.1109/TRO.2016.2609395},
  abstract = {This paper proposes a robust background model-based dense-visual-odometry (BaMVO) algorithm that uses an RGB-D sensor in a dynamic environment. The proposed algorithm estimates the background model represented by the nonparametric model from depth scenes and then estimates the ego-motion of the sensor using the energy-based dense-visual-odometry approach based on the estimated background model in order to consider moving objects. Experimental results demonstrate that the ego-motion is robustly obtained by BaMVO in a dynamic environment.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/X48ILDG9/7582523.html/\:text/html\:PDF:PDF},
  keywords = {robot vision, Visualization, Computational modeling, SLAM (robots), Vehicle dynamics, Robot sensing systems, Robustness, distance measurement, visual odometry, simultaneous localization and mapping (SLAM), background model-based dense visual odometry, background model-based RGB-D dense visual odometry, Background subtraction, BaMVO algorithm, depth scenes, dynamic environment, energy-based dense visual odometry, Heuristic algorithms, nonparametric model, RGB-D sensor, visual tracking},
}

@Article{takagi_fuzzy_1985,
  author   = {Takagi, T. and Sugeno, M.},
  title    = {Fuzzy identification of systems and its applications to modeling and control},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics},
  year     = {1985},
  volume   = {SMC-15},
  number   = {1},
  month    = jan,
  pages    = {116--132},
  issn     = {0018-9472},
  doi      = {10.1109/TSMC.1985.6313399},
  abstract = {A mathematical tool to build a fuzzy model of a system where fuzzy implications and reasoning are used is presented. The premise of an implication is the description of fuzzy subspace of inputs and its consequence is a linear input-output relation. The method of identification of a system using its input-output data is then shown. Two applications of the method to industrial processes are also discussed: a water cleaning process and a converter in a steel-making process.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/W2J4ZMZS/6313399.html/\:text/html\:PDF:PDF},
  keywords = {Parameter estimation, fuzzy set theory, Mathematical model, Vectors, Data models, Cognition, fuzzy identification, fuzzy model, Fuzzy sets, identification, industrial process control, input-output data, modelling, Performance analysis, process control, steel-making process, system theory, systems identification, water cleaning process},
}

@Article{sun_sequentially_2017,
  author   = {Sun, Chung-Hsun and Chen, Ying-Jen and Wang, Yin-Tien and Huang, Sheng-Kai},
  title    = {Sequentially switched fuzzy-model-based control for wheeled mobile robot with visual odometry},
  journal  = {Applied Mathematical Modelling},
  year     = {2017},
  volume   = {47},
  month    = jul,
  pages    = {765--776},
  issn     = {0307-904X},
  doi      = {10.1016/j.apm.2016.11.001},
  url      = {http://www.sciencedirect.com/science/article/pii/S0307904X16305790},
  urldate  = {2018-04-20},
  abstract = {This study focused on the Takagi–Sugeno (T–S) fuzzy-model-based control design for the differentially-driven wheeled mobile robot with visual odometry. The position and posture of the mobile robot are estimated by visual odometry. The polar kinematic model of the mobile robot is exactly converted to the T–S fuzzy model and then the fuzzy control design is synthesized to the fuzzy model. The sequentially switched fuzzy control design includes turning, forward motion as well as position and posture control modes. The stabilization is guaranteed based on the Lyapunov stability criterion. The practical constraints on the visual odometry are also satisfied in the control design. Finally, the experiment results demonstrate the effectiveness of the fuzzy-model-based control design for the mobile robot with visual odometry.},
  file     = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/YLGZXPGR/S0307904X16305790.html/\:text/html\:PDF:PDF},
  keywords = {Visual odometry, Sequentially switched control, Takagi–Sugeno (T–S) fuzzy model, Wheeled mobile robot},
}

@InProceedings{agrawal_censure:_2008,
  author     = {Agrawal, Motilal and Konolige, Kurt and Blas, Morten Rufus},
  title      = {{CenSurE}: {Center} {Surround} {Extremas} for {Realtime} {Feature} {Detection} and {Matching}},
  booktitle  = {Computer {Vision} – {ECCV} 2008},
  year       = {2008},
  language   = {en},
  series     = {Lecture {Notes} in {Computer} {Science}},
  publisher  = {Springer, Berlin, Heidelberg},
  month      = oct,
  isbn       = {978-3-540-88692-1 978-3-540-88693-8},
  pages      = {102--115},
  doi        = {10.1007/978-3-540-88693-8_8},
  urldate    = {2018-04-20},
  abstract   = {We explore the suitability of different feature detectors for the task of image registration, and in particular for visual odometry, using two criteria: stability (persistence across viewpoint change) and accuracy (consistent localization across viewpoint change). In addition to the now-standard SIFT, SURF, FAST, and Harris detectors, we introduce a suite of scale-invariant center-surround detectors (CenSurE) that outperform the other detectors, yet have better computational characteristics than other scale-space detectors, and are capable of real-time implementation.},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/N5YL8M6L/10.html/\:text/html\:PDF:PDF},
  shorttitle = {{CenSurE}},
}

@InProceedings{alahi_freak:_2012,
  author     = {Alahi, A. and Ortiz, R. and Vandergheynst, P.},
  title      = {{FREAK}: {Fast} {Retina} {Keypoint}},
  booktitle  = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  year       = {2012},
  month      = jun,
  pages      = {510--517},
  doi        = {10.1109/CVPR.2012.6247715},
  abstract   = {A large number of vision applications rely on matching keypoints across images. The last decade featured an arms-race towards faster and more robust keypoints and association algorithms: Scale Invariant Feature Transform (SIFT)[17], Speed-up Robust Feature (SURF)[4], and more recently Binary Robust Invariant Scalable Keypoints (BRISK)[I6] to name a few. These days, the deployment of vision algorithms on smart phones and embedded devices with low memory and computation complexity has even upped the ante: the goal is to make descriptors faster to compute, more compact while remaining robust to scale, rotation and noise. To best address the current requirements, we propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Keypoint (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. Our experiments show that FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are thus competitive alternatives to existing keypoints in particular for embedded applications.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/GZ6RM5ZS/6247715.html/\:text/html\:PDF:PDF},
  keywords   = {Kernel, Robustness, Humans, image matching, Noise, SIFT, transforms, Vectors, smart phone, SURF, smart phones, Detectors, computational complexity, association algorithm, binary robust invariant scalable keypoint, binary string, computation complexity, embedded application, embedded device, eye, fast retina keypoint, human visual system, keypoint descriptor, keypoint matching, Retina, scale invariant feature transform, speed-up robust feature, vision application},
  shorttitle = {{FREAK}},
}

@InProceedings{kunii_mobile_2017,
  author    = {Kunii, Y. and Kovacs, G. and Hoshi, N.},
  title     = {Mobile robot navigation in natural environments using robust object tracking},
  booktitle = {2017 {IEEE} 26th {International} {Symposium} on {Industrial} {Electronics} ({ISIE})},
  year      = {2017},
  month     = jun,
  pages     = {1747--1752},
  doi       = {10.1109/ISIE.2017.8001512},
  abstract  = {In this paper the authors introduce a method focusing on the robustness improvement of the landmark tracking system for mobile robot operation in natural environments. We extract feature points from the data obtained by a stereo vision system with CenSurE (Center Surround Extremas for Realtime Feature Detection and Matching) used as a detector, and FREAK (Fast Retina Keypoint) as a descriptor. RANSAC (RANdom SAmple Consensus) is used to remove outlier data from the feature points in order to increase precision. For self-localization, landmarks are selected from the surroundings. These landmarks are tracked by a template matching method using ZNCC (Zero-Mean Normalized Cross-Correlation) complemented with visual odometry based motion estimation. For performance purposes, this is combined with UKF (Unscented Kalman Filter) for narrowing the landmark search areas. A template update strategy suitable for long range tracking is also introduced. Finally, for increasing robustness in long range operation, we solve the issue of obscured/temporarily out of frame landmark tracking by estimating their position based on nearby visible landmarks.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/CJCAKEND/8001512.html/\:text/html\:PDF:PDF},
  keywords  = {mobile robots, Visualization, feature extraction, Three-dimensional displays, stereo image processing, Feature extraction, path planning, object tracking, Kalman filters, nonlinear filters, Robustness, image matching, Robots, Tracking, motion estimation, visual odometry, mobile robot navigation, image filtering, CenSurE, center surround extremas for realtime feature detection and matching, Correlation, fast retina keypoint descriptor, feature point extraction, FREAK, landmark tracking system, long range tracking, random sample consensus, RANSAC, self-localization, stereo vision system, template matching method, template update strategy, UKF, unscented Kalman filter, zero-mean normalized cross-correlation, ZNCC},
}

@MastersThesis{aladem_robust_2017,
  author   = {Aladem, Mohamed},
  title    = {Robust {Real}-{Time} {Visual} {Odometry} for {Autonomous} {Ground} {Vehicles}},
  year     = {2017},
  language = {English},
  month    = apr,
  url      = {https://deepblue.lib.umich.edu/handle/2027.42/136618?show=full},
  urldate  = {2018-04-20},
  abstract = {Estimating the motion of an agent, such as a self-driving vehicle or mobile robot, is an essential requirement for many modern autonomy applications. Real-time and accurate position estimates are essential for navigation, perception and control, especially in previously unknown environments. Using cameras and Visual Odometry (VO) provides an effective way to achieve such motion estimation. Visual odometry is an active area of research in computer vision and mobile robotics communities, as the problem is still a challenging one. In this thesis, a robust real-time feature-based visual odometry algorithm will be presented. The algorithm utilizes a stereo camera which enables estimation in true scale and easy startup of the system. A distinguishing aspect of the developed algorithm is its utilization of a local map consisting of sparse 3D points for tracking and motion estimation. This results in the full history of each feature being utilized for motion estimation. Hence, drift in the ego-motion estimates are greatly reduced, enabling long-term operation over prolonged distances. Furthermore, the algorithm employs Progressive Sample Consensus (PROSAC) in order to increase robustness against outliers. Extensive evaluations on the challenging KITTI and New College datasets are presented. KITTI dataset was collected by a vehicle driving in the city of Karlsruhe in Germany, and represents one of the most commonly used datasets in evaluating self-driving algorithms. The New College dataset was collected by a mobile robot traversing within New College grounds in Oxford. Moreover, experiments on custom data are performed and results are presented.},
  address  = {Dearborn, MI, USA},
  file     = {html\:PDF:html\:Robust Real-Time Visual Odometry for Autonomous Ground Vehicles/\:C//\:/Users/Psykie/Zotero/storage/GCVL9BIW/136618.html/\:text/html\:PDF:PDF},
  school   = {University of Michigan-Dearborn},
}

@Article{nister_visual_2006,
  author   = {Nistér, David and Naroditsky, Oleg and Bergen, James},
  title    = {Visual odometry for ground vehicle applications},
  journal  = {Journal of Field Robotics},
  year     = {2006},
  language = {en},
  volume   = {23},
  number   = {1},
  month    = jan,
  pages    = {3--20},
  issn     = {1556-4959, 1556-4967},
  doi      = {10.1002/rob.20103},
  urldate  = {2018-04-20},
}

@Article{martull_realistic_2012,
  author    = {Martull, Sarah and Peris, Martin and Fukui, Kazuhiro},
  title     = {Realistic CG Stereo Image Dataset with Ground Truth Disparity Maps},
  journal   = {IEICE technical report. Speech},
  year      = {2012},
  volume    = {111},
  number    = {431},
  month     = feb,
  pages     = {117--118},
  url       = {https://ci.nii.ac.jp/naid/110009481986/en/},
  publisher = {The Institute of Electronics, Information and Communication Engineers},
}

@InProceedings{chum_matching_2005,
  author    = {Chum, O. and Matas, J.},
  title     = {Matching with {PROSAC} - progressive sample consensus},
  booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
  year      = {2005},
  volume    = {1},
  month     = jun,
  pages     = {220--226 vol. 1},
  doi       = {10.1109/CVPR.2005.221},
  abstract  = {A new robust matching method is proposed. The progressive sample consensus (PROSAC) algorithm exploits the linear ordering defined on the set of correspondences by a similarity function used in establishing tentative correspondences. Unlike RANSAC, which treats all correspondences equally and draws random samples uniformly from the full set, PROSAC samples are drawn from progressively larger sets of top-ranked correspondences. Under the mild assumption that the similarity measure predicts correctness of a match better than random guessing, we show that PROSAC achieves large computational savings. Experiments demonstrate it is often significantly faster (up to more than hundred times) than RANSAC. For the derived size of the sampled set of correspondences as a function of the number of samples already drawn, PROSAC converges towards RANSAC in the worst case. The power of the method is demonstrated on wide-baseline matching problems.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/NC7F8DYA/1467271.html/\:text/html\:PDF:PDF},
  keywords  = {Computer vision, Cybernetics, Stereo vision, Robustness, image matching, Solid modeling, Motion estimation, Testing, image sampling, Image converters, Image retrieval, Pattern matching, progressive sample consensus, PROSAC algorithm, wide-baseline matching},
}

@article{liu_robust_2017,
	title = {Robust {Stereo} {Visual} {Odometry} {Using} {Improved} {RANSAC}-{Based} {Methods} for {Mobile} {Robot} {Localization}},
	volume = {17},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5677260/},
	doi = {10.3390/s17102339},
	abstract = {In this paper, we present a novel approach for stereo visual odometry with robust motion estimation that is faster and more accurate than standard RANSAC (Random Sample Consensus). Our method makes improvements in RANSAC in three aspects: first, the hypotheses are preferentially generated by sampling the input feature points on the order of ages and similarities of the features; second, the evaluation of hypotheses is performed based on the SPRT (Sequential Probability Ratio Test) that makes bad hypotheses discarded very fast without verifying all the data points; third, we aggregate the three best hypotheses to get the final estimation instead of only selecting the best hypothesis. The first two aspects improve the speed of RANSAC by generating good hypotheses and discarding bad hypotheses in advance, respectively. The last aspect improves the accuracy of motion estimation. Our method was evaluated in the KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) and the New Tsukuba dataset. Experimental results show that the proposed method achieves better results for both speed and accuracy than RANSAC.},
	number = {10},
	urldate = {2018-04-20},
	journal = {Sensors (Basel, Switzerland)},
	author = {Liu, Yanqing and Gu, Yuzhang and Li, Jiamao and Zhang, Xiaolin},
	month = oct,
	year = {2017},
	pmid = {29027935},
	pmcid = {PMC5677260}
}

@InProceedings{quiroga_dense_2014,
  author    = {Quiroga, Julian and Brox, Thomas and Devernay, Frédéric and Crowley, James},
  title     = {Dense {Semi}-rigid {Scene} {Flow} {Estimation} from {RGBD} {Images}},
  booktitle = {Computer {Vision} – {ECCV} 2014},
  year      = {2014},
  language  = {en},
  series    = {Lecture {Notes} in {Computer} {Science}},
  publisher = {Springer, Cham},
  month     = sep,
  isbn      = {978-3-319-10583-3 978-3-319-10584-0},
  pages     = {567--582},
  doi       = {10.1007/978-3-319-10584-0_37},
  urldate   = {2018-04-20},
  abstract  = {Scene flow is defined as the motion field in 3D space, and can be computed from a single view when using an RGBD sensor. We propose a new scene flow approach that exploits the local and piecewise rigidity of real world scenes. By modeling the motion as a field of twists, our method encourages piecewise smooth solutions of rigid body motions. We give a general formulation to solve for local and global rigid motions by jointly using intensity and depth data. In order to deal efficiently with a moving camera, we model the motion as a rigid component plus a non-rigid residual and propose an alternating solver. The evaluation demonstrates that the proposed method achieves the best results in the most commonly used scene flow benchmark. Through additional experiments we indicate the general applicability of our approach in a variety of different scenarios.},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/TPPHAK2I/978-3-319-10584-0_37.html/\:text/html\:PDF:PDF},
}

@InProceedings{jaimez_fast_2017,
  author    = {Jaimez, M. and Kerl, C. and Gonzalez-Jimenez, J. and Cremers, D.},
  title     = {Fast odometry and scene flow from {RGB}-{D} cameras based on geometric clustering},
  booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  year      = {2017},
  month     = may,
  pages     = {3992--3999},
  doi       = {10.1109/ICRA.2017.7989459},
  abstract  = {In this paper we propose an efficient solution to jointly estimate the camera motion and a piecewise-rigid scene flow from an RGB-D sequence. The key idea is to perform a two-fold segmentation of the scene, dividing it into geometric clusters that are, in turn, classified as static or moving elements. Representing the dynamic scene as a set of rigid clusters drastically accelerates the motion estimation, while segmenting it into static and dynamic parts allows us to separate the camera motion (odometry) from the rest of motions observed in the scene. The resulting method robustly and accurately determines the motion of an RGB-D camera in dynamic environments with an average runtime of 80 milliseconds on a multi-core CPU. The code is available for public use/test.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/SHU5CKS3/7989459.html/\:text/html\:PDF:PDF},
  keywords  = {image segmentation, Image segmentation, Cameras, Estimation, image sensors, Three-dimensional displays, image sequences, Dynamics, Robustness, multicore CPU, multiprocessing systems, motion estimation, dynamic environments, RGB-D cameras, fast odometry, camera motion estimation, geometric clustering, geometric clusters, microprocessor chips, Motion segmentation, pattern clustering, RGB-D sequence, scene flow, two-fold segmentation},
}

@InProceedings{holzmann_detailed_2016,
  author    = {Holzmann, Thomas and Fraundorfer, Friedrich and Bischof, Horst},
  title     = {A {Detailed} {Description} of {Direct} {Stereo} {Visual} {Odometry} {Based} on {Lines}},
  booktitle = {Computer {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}},
  year      = {2016},
  language  = {en},
  series    = {Communications in {Computer} and {Information} {Science}},
  publisher = {Springer, Cham},
  month     = feb,
  isbn      = {978-3-319-64869-9 978-3-319-64870-5},
  pages     = {353--373},
  doi       = {10.1007/978-3-319-64870-5_17},
  urldate   = {2018-04-20},
  abstract  = {In this paper, we propose a direct stereo visual odometry method which uses vertical lines to estimate consecutive camera poses. Therefore, it is well suited for poorly textured indoor environments where point-based methods may fail. We introduce a fast line segment detector and matcher detecting vertical lines, which occur frequently in man-made environments. We estimate the pose of the camera by directly minimizing the photometric error of the patches around the detected lines. In cases where not sufficient lines could be detected, point features are used as fallback solution. As our algorithm runs in real-time, it is well suited for robotics and augmented reality applications. In our experiments, we show that our algorithm outperforms state-of-the-art methods on poorly textured indoor scenes and delivers comparable results on well textured outdoor scenes.},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/CGSCUILC/978-3-319-64870-5_17.html/\:text/html\:PDF:PDF},
}

@InProceedings{newcombe_kinectfusion:_2011,
  author     = {Newcombe, R. A. and Izadi, S. and Hilliges, O. and Molyneaux, D. and Kim, D. and Davison, A. J. and Kohi, P. and Shotton, J. and Hodges, S. and Fitzgibbon, A.},
  title      = {{KinectFusion}: {Real}-time dense surface mapping and tracking},
  booktitle  = {2011 10th {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality}},
  year       = {2011},
  month      = oct,
  pages      = {127--136},
  doi        = {10.1109/ISMAR.2011.6092378},
  abstract   = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/HG6N7IM6/6162880.html/\:text/html\:PDF:PDF},
  keywords   = {Cameras, Iterative closest point algorithm, SLAM, Real time systems, Three dimensional displays, Simultaneous localization and mapping, Surface reconstruction, Tracking, AR, Dense Reconstruction, Depth Cameras, GPU, Image reconstruction, Real-Time, Volumetric Representation},
  shorttitle = {{KinectFusion}},
}

@article{gutierrez-gomez_dense_2016,
	title = {Dense {RGB}-{D} visual odometry using inverse depth},
	volume = {75},
	issn = {0921-8890},
	url = {http://www.sciencedirect.com/science/article/pii/S0921889015002213},
	doi = {10.1016/j.robot.2015.09.026},
	abstract = {In this paper we present a dense visual odometry system for RGB-D cameras performing both photometric and geometric error minimisation to estimate the camera motion between frames. Contrary to most works in the literature, we parametrise the geometric error by the inverse depth instead of the depth, which translates into a better fit of the distribution of the geometric error to the used robust cost functions. To improve the accuracy we propose to use a keyframe switching strategy based on a visibility criteria between frames. For the comparison of our approach with state-of-the-art approaches we use the popular datasets from the TUM for RGB-D benchmarking as well as two synthetic datasets. Our approach shows to be competitive with state-of-the-art methods in terms of drift in metres per second, even compared to methods performing loop closure too. When comparing to approaches performing pure odometry like ours, our method outperforms them in the majority of the tested datasets. Additionally we show that our approach is able to work in real time and we provide a qualitative evaluation on our own sequences showing a low drift in the 3D reconstructions. We have implemented this method within the scope of PCL (Point Cloud Library) as a branch of the code for large scale KinectFusion, where the original ICP system for odometry estimation has been completely substituted by our method. A PCL fork including the modified method is available for download.},
	urldate = {2018-04-20},
	journal = {Robotics and Autonomous Systems},
	author = {Gutierrez-Gomez, Daniel and Mayol-Cuevas, Walterio and Guerrero, J. J.},
	month = jan,
	year = {2016},
	keywords = {RGB-D, Visual odometry},
	pages = {571--583}
}

@InProceedings{handa_benchmark_2014,
  author    = {Handa, A. and Whelan, T. and McDonald, J. and Davison, A. J.},
  title     = {A benchmark for {RGB}-{D} visual odometry, 3D reconstruction and {SLAM}},
  booktitle = {2014 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  year      = {2014},
  month     = may,
  pages     = {1524--1531},
  doi       = {10.1109/ICRA.2014.6907054},
  abstract  = {We introduce the Imperial College London and National University of Ireland Maynooth (ICL-NUIM) dataset for the evaluation of visual odometry, 3D reconstruction and SLAM algorithms that typically use RGB-D data. We present a collection of handheld RGB-D camera sequences within synthetically generated environments. RGB-D sequences with perfect ground truth poses are provided as well as a ground truth surface model that enables a method of quantitatively evaluating the final map or surface reconstruction accuracy. Care has been taken to simulate typically observed real-world artefacts in the synthetic imagery by modelling sensor noise in both RGB and depth data. While this dataset is useful for the evaluation of visual odometry and SLAM trajectory estimation, our main focus is on providing a method to benchmark the surface reconstruction accuracy which to date has been missing in the RGB-D community despite the plethora of ground truth RGB-D datasets available.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/4KX99NXH/6907054.html/\:text/html\:PDF:PDF},
  keywords  = {3D reconstruction, Cameras, image colour analysis, Iterative closest point algorithm, Three-dimensional displays, image sequences, SLAM (robots), image reconstruction, Surface reconstruction, Noise, ICL-NUIM dataset, distance measurement, Trajectory, Image reconstruction, benchmark, depth data, ground truth surface model, handheld RGB-D camera sequences, Imperial College London and National University of Ireland Maynooth dataset, RGB-D sequences, RGB-D visual odometry, sensor noise modelling, SLAM trajectory estimation, surface reconstruction accuracy, synthetic imagery, visual odometry evaluation},
}

@Article{gioi_lsd:_2010,
  author     = {Gioi, R. Grompone von and Jakubowicz, J. and Morel, J. M. and Randall, G.},
  title      = {{LSD}: {A} {Fast} {Line} {Segment} {Detector} with a {False} {Detection} {Control}},
  journal    = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year       = {2010},
  volume     = {32},
  number     = {4},
  month      = apr,
  pages      = {722--732},
  issn       = {0162-8828},
  doi        = {10.1109/TPAMI.2008.300},
  abstract   = {We propose a linear-time line segment detector that gives accurate results, a controlled number of false detections, and requires no parameter tuning. This algorithm is tested and compared to state-of-the-art algorithms on a wide set of natural images.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/IRY8RZZ2/4731268.html/\:text/html\:PDF:PDF},
  keywords   = {image sensors, object detection, Humans, state-of-the-art algorithms, Algorithms, Pattern Recognition, Automated, Reproducibility of Results, a contrario detection., false detection control, Helmholtz principle, Image Processing, Computer-Assisted, Line segment detection, Linear Models, linear-time line segment detector, natural images, NFA, Visual Perception},
  shorttitle = {{LSD}},
}

@Article{furukawa_multi-view_2015,
  author     = {Furukawa, Yasutaka and Hernández, Carlos},
  title      = {Multi-{View} {Stereo}: {A} {Tutorial}},
  journal    = {Foundations and Trends® in Computer Graphics and Vision},
  year       = {2015},
  language   = {English},
  volume     = {9},
  number     = {1-2},
  month      = jun,
  pages      = {1--148},
  issn       = {1572-2740, 1572-2759},
  doi        = {10.1561/0600000052},
  url        = {https://www.nowpublishers.com/article/Details/CGV-052},
  urldate    = {2018-04-20},
  abstract   = {Multi-View Stereo: A Tutorial},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/GB5JW3QV/CGV-052.html/\:text/html\:PDF:PDF},
  shorttitle = {Multi-{View} {Stereo}},
}

@InProceedings{wang_stereo_2017,
  author    = {Wang, Rui and Schworer, Martin and Cremers, Daniel},
  title     = {Stereo DSO: Large-Scale Direct Sparse Visual Odometry With Stereo Cameras},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2017},
  month     = oct,
}

@WWW{openmp_openmp_nodate,
  author   = {{OpenMP}},
  title    = {{OpenMP}},
  year     = {2018},
  url      = {http://www.openmp.org/},
  language = {en-GB},
  urldate  = {2018-04-19},
  abstract = {Latest News     View Monthly Archives  @OpenMP\_ARB

Tweets by OpenMP\_ARB    Get all the latest API specifications, technical report drafts and proposals. All the latest books, examples and tutorials to start you on your journey. Ask the},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\V464N28C\\\\\\\\www.openmp.org.html\\\:text/html\:PDF:PDF},
  journal  = {OpenMP},
}

@InProceedings{persson_robust_2015,
  author    = {Persson, M. and Piccini, T. and Felsberg, M. and Mester, R.},
  title     = {Robust stereo visual odometry from monocular techniques},
  booktitle = {2015 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
  year      = {2015},
  month     = jun,
  pages     = {686--691},
  doi       = {10.1109/IVS.2015.7225764},
  abstract  = {Visual odometry is one of the most active topics in computer vision. The automotive industry is particularly interested in this field due to the appeal of achieving a high degree of accuracy with inexpensive sensors such as cameras. The best results on this task are currently achieved by systems based on a calibrated stereo camera rig, whereas monocular systems are generally lagging behind in terms of performance. We hypothesise that this is due to stereo visual odometry being an inherently easier problem, rather than than due to higher quality of the state of the art stereo based algorithms. Under this hypothesis, techniques developed for monocular visual odometry systems would be, in general, more refined and robust since they have to deal with an intrinsically more difficult problem. In this work we present a novel stereo visual odometry system for automotive applications based on advanced monocular techniques. We show that the generalization of these techniques to the stereo case result in a significant improvement of the robustness and accuracy of stereo based visual odometry. We support our claims by the system results on the well known KITTI benchmark, achieving the top rank for visual only systems*.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/VZMQHKEL/7225764.html/\:text/html\:PDF:PDF},
  keywords  = {Training, Visualization, Benchmark testing, Cameras, stereo image processing, KITTI benchmark, cameras, computer vision, Robustness, Optimization, Tracking, automotive applications, automotive industry, calibrated stereo camera rig, monocular visual odometry system, robust stereo visual odometry},
}

@Article{jaimez_fast_2015,
  author   = {Jaimez, M. and Gonzalez-Jimenez, J.},
  title    = {Fast {Visual} {Odometry} for 3-{D} {Range} {Sensors}},
  journal  = {IEEE Transactions on Robotics},
  year     = {2015},
  volume   = {31},
  number   = {4},
  month    = aug,
  pages    = {809--822},
  issn     = {1552-3098},
  doi      = {10.1109/TRO.2015.2428512},
  abstract = {This paper presents a new dense method to compute the odometry of a free-flying range sensor in real time. The method applies the range flow constraint equation to sensed points in the temporal flow to derive the linear and angular velocity of the sensor in a rigid environment. Although this approach is applicable to any range sensor, we particularize its formulation to estimate the 3-D motion of a range camera. The proposed algorithm is tested with different image resolutions and compared with two state-of-the-art methods: generalized iterative closest point (GICP) [1] and robust dense visual odometry (RDVO) [2]. Experiments show that our approach clearly overperforms GICP which uses the same geometric input data, whereas it achieves results similar to RDVO, which requires both geometric and photometric data to work. Furthermore, experiments are carried out to demonstrate that our approach is able to estimate fast motions at 60 Hz running on a single CPU core, a performance that has never been reported in the literature. The algorithm is available online under an open source license so that the robotic community can benefit from it.},
  annote   = {The following values have no corresponding Zotero field:number: 4},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/5KKG3APK/7119600.html/\:text/html\:PDF:PDF},
  keywords = {robot vision, Visualization, Cameras, image sensors, Optical imaging, Optical sensors, Mathematical model, public domain software, iterative methods, Robots, distance measurement, visual odometry, 3-D motion, 3-D range sensors, angular velocity, fast visual odometry, free-flying range sensor, generalized iterative closest point, GICP, linear velocity, open source license, range camera, range flow constraint equation, Range sensors, RDVO, real time, robotic community, robust dense visual odometry, sensed points, temporal flow},
}

@InProceedings{sturm_benchmark_2012,
  author    = {Sturm, J. and Engelhard, N. and Endres, F. and Burgard, W. and Cremers, D.},
  title     = {A benchmark for the evaluation of {RGB}-{D} {SLAM} systems},
  booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
  year      = {2012},
  month     = oct,
  pages     = {573--580},
  doi       = {10.1109/IROS.2012.6385773},
  abstract  = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/WSYLWP4T/6385773.html/\:text/html\:PDF:PDF},
  keywords  = {Visualization, image resolution, Cameras, image colour analysis, image sequences, SLAM (robots), object tracking, cameras, Simultaneous localization and mapping, pose estimation, distance measurement, color image, depth image, Trajectory, loop closures, automatic evaluation tools, Calibration, camera motions, cluttered indoor environment, full sensor resolution, global pose error, ground truth camera poses, ground-truth trajectory, handheld Kinect, high-speed tracking cameras, industrial hall, Microsoft Kinect, motion capture system, motion-capture system, office environment, Pioneer 3 robot, RGB-D SLAM systems, slow motions, unconstrained 6DOF motions, video frame rate, visual odometry systems},
}

@Article{fang_experimental_2015,
  author   = {Fang, Zheng and Zhang, Yu},
  title    = {Experimental {Evaluation} of {RGB}-{D} {Visual} {Odometry} {Methods}},
  journal  = {International Journal of Advanced Robotic Systems},
  year     = {2015},
  language = {en},
  volume   = {12},
  number   = {3},
  month    = mar,
  pages    = {26},
  issn     = {1729-8814},
  doi      = {10.5772/59991},
  urldate  = {2018-04-19},
  abstract = {RGB-D cameras that can provide rich 2D visual and 3D depth information are well suited to the motion estimation of indoor mobile robots. In recent years, several RGB-D visual odometry methods that process data from the sensor in different ways have been proposed. This paper first presents a brief review of recently proposed RGB-D visual odometry methods, and then presents a detailed analysis and comparison of eight state-of-the-art real-time 6DOF motion estimation methods in a variety of challenging scenarios, with a special emphasis on the trade-off between accuracy, robustness and computation speed. An experimental comparison is conducted using publicly available benchmark datasets and author-collected datasets in various scenarios, including long corridors, illumination changing environments and fast motion scenarios. Experimental results present both quantitative and qualitative differences between these methods and provide some guidelines on how to choose the right algorithm for an indoor mobile robot according to the quality of the RGB-D data and environmental characteristics.},
}

@WWW{geiger_visual_nodate,
  author  = {Geiger, Andreas},
  title   = {Visual {Odometry} / {SLAM} {Evaluation} 2012},
  year    = {2019},
  url     = {http://www.cvlibs.net/datasets/kitti/eval_odometry.php},
  urldate = {2018-04-19},
  file    = {html\:PDF:html\:The KITTI Vision Benchmark Suite/\:C//\:/Users/Psykie/Zotero/storage/L93ZFKDM/eval_odometry.html/\:text/html\:PDF:PDF},
  journal = {The KITTI Vision Benchmark Suite},
}

@InProceedings{strydom_visual_2014,
  author     = {Strydom, Reuben and Thurrowgood, Saul and Srinivasan, Mandyam V.},
  title      = {Visual odometry: autonomous {UAV} navigation using optic flow and stereo},
  booktitle  = {Proceedings of Australasian conference on robotics and automation},
  year       = {2014},
  language   = {eng},
  publisher  = {Australian Robotics and Automation Association},
  month      = jan,
  url        = {https://espace.library.uq.edu.au/view/UQ:356412},
  urldate    = {2018-04-19},
  abstract   = {Visual odometry is vital to the future of mobile robotics. In this paper, we demonstrate a method that combines information from optic flow and stereo to estimate and control the current position of a quadrotor along a pre-defined trajectory. The absolute translation in 3D is computed by combining the optic flow measurements between successive frames and stereo-based height over ground. The current 3D position, as estimated from path integration of the incremental translations, is controlled in closed loop to follow the prescribed trajectory. The performance of the system is evaluated by measuring the error between the initial and final positions in closed circuits. This error is approximately 1.7\% of the total path length.},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/TTJD44DU/UQ356412.html/\:text/html\:PDF:PDF},
  shorttitle = {Visual odometry},
}

@Article{maimone_two_2007-1,
  author     = {Maimone, Mark and Cheng, Yang and Matthies, Larry},
  title      = {Two {Years} of {Visual} {Odometry} on the {Mars} {Exploration} {Rovers}: {Field} {Reports}},
  journal    = {J. Field Robot.},
  year       = {2007},
  volume     = {24},
  number     = {3},
  month      = mar,
  pages      = {169--186},
  issn       = {1556-4959},
  doi        = {10.1002/rob.v24:3},
  urldate    = {2018-04-19},
  abstract   = {NASA's two Mars Exploration Rovers (MER) have successfully demonstrated a robotic Visual Odometry capability on another world for the first time. This provides each rover with accurate knowledge of its position, allowing it to autonomously detect and compensate for any unforeseen slip encountered during a drive. It has enabled the rovers to drive safely and more effectively in highly sloped and sandy terrains and has resulted in increased mission science return by reducing the number of days required to drive into interesting areas. The MER Visual Odometry system comprises onboard software for comparing stereo pairs taken by the pointable mast-mounted 45 deg FOV Navigation cameras (NAVCAMs). The system computes an update to the 6 degree of freedom rover pose (x, y, z, roll, pitch, yaw) by tracking the motion of autonomously selected terrain features between two pairs of 256×256 stereo images. It has demonstrated good performance with high rates of successful convergence (97\% on Spirit, 95\% on Opportunity), successfully detected slip ratios as high as 125\%, and measured changes as small as 2 mm, even while driving on slopes as high as 31 deg. Visual Odometry was used over 14\% of the first 10.7 km driven by both rovers. During the first 2 years of operations, Visual Odometry evolved from an “extra credit” capability into a critical vehicle safety system. In this paper we describe our Visual Odometry algorithm, discuss several driving strategies that rely on it (including Slip Checks, Keep-out Zones, and Wheel Dragging), and summarize its results from the first 2 years of operations on Mars. © 2006 Wiley Periodicals, Inc.},
  shorttitle = {Two {Years} of {Visual} {Odometry} on the {Mars} {Exploration} {Rovers}},
}

@Article{proenca_probabilistic_2018,
  author   = {Proença, Pedro F. and Gao, Yang},
  title    = {Probabilistic {RGB}-{D} odometry based on points, lines and planes under depth uncertainty},
  journal  = {Robotics and Autonomous Systems},
  year     = {2018},
  volume   = {104},
  month    = jun,
  pages    = {25--39},
  issn     = {0921-8890},
  doi      = {10.1016/j.robot.2018.02.018},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889017303378},
  urldate  = {2018-04-19},
  abstract = {This work proposes a robust visual odometry method for structured environments that combines point features with line and plane segments, extracted through an RGB-D camera. Noisy depth maps are processed by a probabilistic depth fusion framework based on Mixtures of Gaussians to denoise and derive the depth uncertainty, which is then propagated throughout the visual odometry pipeline. Probabilistic 3D plane and line fitting solutions are used to model the uncertainties of the feature parameters and pose is estimated by combining the three types of primitives based on their uncertainties. Performance evaluation on RGB-D sequences collected in this work and two public RGB-D datasets: TUM and ICL-NUIM show the benefit of using the proposed depth fusion framework and combining the three feature-types, particularly in scenes with low-textured surfaces, dynamic objects and missing depth measurements.},
  file     = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/9TSXUEJS/S0921889017303378.html/\:text/html\:PDF:PDF},
  keywords = {Visual odometry, Depth fusion, Depth uncertainty, Probabilistic plane and line extraction, Structured environments},
}

@Article{zhang_real-time_2017,
  author   = {Zhang, Ji and Kaess, Michael and Singh, Sanjiv},
  title    = {A real-time method for depth enhanced visual odometry},
  journal  = {Autonomous Robots},
  year     = {2017},
  language = {en},
  volume   = {41},
  number   = {1},
  month    = jan,
  pages    = {31--43},
  issn     = {0929-5593, 1573-7527},
  doi      = {10.1007/s10514-015-9525-1},
  urldate  = {2018-04-19},
  abstract = {Visual odometry can be augmented by depth information such as provided by RGB-D cameras, or from lidars associated with cameras. However, such depth information can be limited by the sensors, leaving large areas in the visual images where depth is unavailable. Here, we propose a method to utilize the depth, even if sparsely available, in recovery of camera motion. In addition, the method utilizes depth by structure from motion using the previously estimated motion, and salient visual features for which depth is unavailable. Therefore, the method is able to extend RGB-D visual odometry to large scale, open environments where depth often cannot be sufficiently acquired. The core of our method is a bundle adjustment step that refines the motion estimates in parallel by processing a sequence of images, in a batch optimization. We have evaluated our method in three sensor setups, one using an RGB-D camera, and two using combinations of a camera and a 3D lidar. Our method is rated \#4 on the KITTI odometry benchmark irrespective of sensing modality—compared to stereo visual odometry methods which retrieve depth by triangulation. The resulting average position error is 1.14 \% of the distance traveled.},
  annote   = {The following values have no corresponding Zotero field:label: Zhang2015work-type: journal article},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/8QLFK4LW/s10514-015-9525-1.html/\:text/html\:PDF:PDF},
}

@Article{stefan_threedimensional_2009,
  author   = {Stefan, May and David, Droeschel and Dirk, Holz and Stefan, Fuchs and Ezio, Malis and Andreas, Nüchter and Joachim, Hertzberg},
  title    = {Three‐dimensional mapping with time‐of‐flight cameras},
  journal  = {Journal of Field Robotics},
  year     = {2009},
  volume   = {26},
  number   = {11‐12},
  month    = sep,
  pages    = {934--965},
  issn     = {1556-4959},
  doi      = {10.1002/rob.20321},
  urldate  = {2018-04-19},
  abstract = {Abstract This article investigates the use of time?of?flight (ToF) cameras in mapping tasks for autonomous mobile robots, in particular in simultaneous localization and mapping (SLAM) tasks. Although ToF cameras are in principle an attractive type of sensor for three?dimensional (3D) mapping owing to their high rate of frames of 3D data, two features make them difficult as mapping sensors, namely, their restricted field of view and influences on the quality of range measurements by high dynamics in object reflectivity; in addition, currently available models suffer from poor data quality in a number of aspects. The paper first summarizes calibration and filtering approaches for improving the accuracy, precision, and robustness of ToF cameras independent of their intended usage. Then, several ego motion estimation approaches are applied or adapted, respectively, in order to provide a performance benchmark for registering ToF camera data. As a part of this, an extension to the iterative closest point algorithm has been developed that increases the robustness under restricted field of view and under larger displacements. Using an indoor environment, the paper provides results from SLAM experiments using these approaches in comparison. It turns out that the application of ToF cameras is feasible to SLAM tasks, although this type of sensor has a complex error characteristic. ? 2009 Wiley Periodicals, Inc.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/VEEVH3MC/rob.html/\:text/html\:PDF:PDF},
}

@InProceedings{liu_improved_2017,
  author    = {Liu, S. H. and Hsu, C. C. and Wang, W. Y. and Chen, M. Y. and Wang, Y. T.},
  title     = {Improved visual odometry system based on kinect {RGB}-{D} sensor},
  booktitle = {2017 {IEEE} 7th {International} {Conference} on {Consumer} {Electronics} - {Berlin} ({ICCE}-{Berlin})},
  year      = {2017},
  month     = sep,
  pages     = {29--30},
  doi       = {10.1109/ICCE-Berlin.2017.8210581},
  abstract  = {In conventional visual odometry (VO) systems, perspective-n-point (PnP) method and random sample consensus (RANSAC) algorithm are generally used to estimate camera poses. However, heavy computational burden is incurred, and the pose estimations are not reliable as well. Therefore, in this paper, an improved VO system is proposed, where an off-line camera calibration method is used to obtain lesser measurement errors of image features. Moreover, an improved approach of P3P algorithm is proposed to increase the efficiency of the VO system. To validate the performances of the proposed approach, several experiments are conducted based on a Kinect sensor, where accuracy of pose estimations and runtime efficiency are both improved in comparison to the conventional VO algorithms.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/W2GNKUZB/8210581.html/\:text/html\:PDF:PDF},
  keywords  = {Algorithm design and analysis, mobile robots, robot vision, Visualization, Cameras, image sensors, stereo image processing, cameras, Robot sensing systems, pose estimation, distance measurement, random sample consensus algorithm, Visual odometry, RANSAC, Calibration, calibration, conventional VO algorithms, improved visual odometry system, kinect RGB-D sensor, measurement errors, off-line camera calibration method, Perspective-3-point, perspective-n-point method},
}

@InCollection{huang_visual_2017,
  author    = {Huang, Albert S. and Bachrach, Abraham and Henry, Peter and Krainin, Michael and Maturana, Daniel and Fox, Dieter and Roy, Nicholas},
  title     = {Visual {Odometry} and {Mapping} for {Autonomous} {Flight} {Using} an {RGB}-{D} {Camera}},
  booktitle = {Robotics {Research}},
  year      = {2017},
  language  = {en},
  series    = {Springer {Tracts} in {Advanced} {Robotics}},
  publisher = {Springer, Cham},
  isbn      = {978-3-319-29362-2 978-3-319-29363-9},
  pages     = {235--252},
  doi       = {10.1007/978-3-319-29363-9_14},
  urldate   = {2018-04-19},
  abstract  = {RGB-D cameras provide both a color image and per-pixel depth estimates. The richness of their data and the recent development of low-cost sensors have combined to present an attractive opportunity for mobile robotics research. In this paper, we describe a system for visual odometry and mapping using an RGB-D camera, and its application to autonomous flight. By leveraging results from recent state-of-the-art algorithms and hardware, our system enables 3D flight in cluttered environments using only onboard sensor data. All computation and sensing required for local position control are performed onboard the vehicle, reducing the dependence on unreliable wireless links. We evaluate the effectiveness of our system for stabilizing and controlling a quadrotor micro air vehicle, demonstrate its use for constructing detailed 3D maps of an indoor environment, and discuss its limitations.},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/SXU84MFN/978-3-319-29363-9_14.html/\:text/html\:PDF:PDF},
}

@Article{an_semantic_2017,
  author   = {An, Lifeng and Zhang, Xinyu and Gao, Hongbo and Liu, Yuchao},
  title    = {Semantic segmentation–aided visual odometry for urban autonomous driving},
  journal  = {International Journal of Advanced Robotic Systems},
  year     = {2017},
  language = {en},
  volume   = {14},
  number   = {5},
  month    = sep,
  issn     = {1729-8814},
  doi      = {10.1177/1729881417735667},
  urldate  = {2018-04-19},
  abstract = {Visual odometry plays an important role in urban autonomous driving cars. Feature-based visual odometry methods sample the candidates randomly from all available feature points, while alignment-based visual odometry methods take all pixels into account. These methods hold an assumption that quantitative majority of candidate visual cues could represent the truth of motions. But in real urban traffic scenes, this assumption could be broken by lots of dynamic traffic participants. Big trucks or buses may occupy the main image parts of a front-view monocular camera and result in wrong visual odometry estimation. Finding available visual cues that could represent real motion is the most important and hardest step for visual odometry in the dynamic environment. Semantic attributes of pixels could be considered as a more reasonable factor for candidate selection in that case. This article analyzed the availability of all visual cues with the help of pixel-level semantic information and proposed a new visual odometry method that combines feature-based and alignment-based visual odometry methods with one optimization pipeline. The proposed method was compared with three open-source visual odometry algorithms on Kitti benchmark data sets and our own data set. Experimental results confirmed that the new approach provided effective improvement both on accurate and robustness in the complex dynamic scenes.},
}

@InProceedings{ros_vision-based_2015,
  author    = {Ros, G. and Ramos, S. and Granados, M. and Bakhtiary, A. and Vazquez, D. and Lopez, A. M.},
  title     = {Vision-{Based} {Offline}-{Online} {Perception} {Paradigm} for {Autonomous} {Driving}},
  booktitle = {2015 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
  year      = {2015},
  month     = jan,
  pages     = {231--238},
  doi       = {10.1109/WACV.2015.38},
  abstract  = {Autonomous driving is a key factor for future mobility. Properly perceiving the environment of the vehicles is essential for a safe driving, which requires computing accurate geometric and semantic information in real-time. In this paper, we challenge state-of-the-art computer vision algorithms for building a perception system for autonomous driving. An inherent drawback in the computation of visual semantics is the trade-off between accuracy and computational cost. We propose to circumvent this problem by following an offline-online strategy. During the offline stage dense 3D semantic maps are created. In the online stage the current driving area is recognized in the maps via a re-localization process, which allows to retrieve the pre-computed accurate semantics and 3D geometry in real-time. Then, detecting the dynamic obstacles we obtain a rich understanding of the current scene. We evaluate quantitatively our proposal in the KITTI dataset and discuss the related open challenges for the computer vision community.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/67K8RAWQ/7045892.html/\:text/html\:PDF:PDF},
  keywords  = {autonomous driving, Semantics, Cameras, Sensors, Three-dimensional displays, Vehicle dynamics, KITTI dataset, Computer vision, geometry, road safety, Vehicles, computer vision, traffic information systems, geometric information, real-time systems, 3D geometry, computer vision algorithms, future mobility, real-time system, safe driving, semantic information, vision-based offline-online perception paradigm},
}

@InProceedings{kitt_visual_2010,
  author    = {Kitt, B. and Geiger, A. and Lategahn, H.},
  title     = {Visual odometry based on stereo image sequences with {RANSAC}-based outlier rejection scheme},
  booktitle = {2010 {IEEE} {Intelligent} {Vehicles} {Symposium}},
  year      = {2010},
  month     = jun,
  pages     = {486--492},
  doi       = {10.1109/IVS.2010.5548123},
  abstract  = {A common prerequisite for many vision-based driver assistance systems is the knowledge of the vehicle's own movement. In this paper we propose a novel approach for estimating the egomotion of the vehicle from a sequence of stereo images. Our method is directly based on the trifocal geometry between image triples, thus no time expensive recovery of the 3-dimensional scene structure is needed. The only assumption we make is a known camera geometry, where the calibration may also vary over time. We employ an Iterated Sigma Point Kalman Filter in combination with a RANSAC-based outlier rejection scheme which yields robust frame-to-frame motion estimation even in dynamic environments. A high-accuracy inertial navigation system is used to evaluate our results on challenging real-world video sequences. Experiments show that our approach is clearly superior compared to other filtering techniques in terms of both, accuracy and run-time.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/55UI36QM/5548123.html/\:text/html\:PDF:PDF},
  keywords  = {Cameras, Layout, image sequences, stereo image processing, Vehicle dynamics, driver assistance systems, driver information systems, Kalman filters, Robustness, video sequences, iterative methods, distance measurement, Motion estimation, motion estimation, visual odometry, Geometry, Image sequences, inertial navigation, inertial systems, Inertial navigation, Calibration, calibration, inertial navigation system, iterated sigma point Kalman filter, RANSAC-based outlier rejection scheme, stereo image sequences, trifocal geometry, Video sequences},
}

@InProceedings{lee_online_2015,
  author    = {Lee, B. and Daniilidis, K. and Lee, D. D.},
  title     = {Online self-supervised monocular visual odometry for ground vehicles},
  booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  year      = {2015},
  month     = may,
  pages     = {5232--5238},
  doi       = {10.1109/ICRA.2015.7139928},
  abstract  = {This paper presents an online self-supervised approach to monocular visual odometry and ground classification applied to ground vehicles. We solve the motion and structure problem based on a constrained kinematic model. The true scale of the monocular scene is recovered by estimating the ground surface. We consider a general parametric ground surface model and use the Random Sample Consensus (RANSAC) algorithm for robust fitting of the parameters. The estimated ground surface provides training samples to learn a probabilistic appearance-based ground classifier in an online and self-supervised manner. The appearance-based classifier is then used to bias the RANSAC sampling to generate better hypotheses for parameter estimation of the ground surface model. Thus, without relying on any prior information, we combine geometric estimates with appearance-based classification to achieve an online self-learning scheme from monocular vision. Experimental results demonstrate that online learning improves the computational efficiency and accuracy compared to standard sampling in RANSAC. Evaluations on the KITTI benchmark dataset demonstrate the stability and accuracy of our overall methods in comparison to previous approaches.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/V8BYAACL/7139928.html/\:text/html\:PDF:PDF},
  keywords  = {road vehicles, traffic engineering computing, Visualization, image classification, Cameras, Estimation, learning (artificial intelligence), probability, Mathematical model, Image color analysis, parameter estimation, distance measurement, image motion analysis, image sampling, random sample consensus algorithm, constrained kinematic model, general parametric ground surface model, geometric estimates, ground surface estimation model, ground vehicles, KITTI benchmark dataset evaluations, Land vehicles, monocular scene, online self-learning scheme, online self-supervised monocular visual odometry, probabilistic appearance-based ground classifier, RANSAC sampling algorithm, structure problem},
}

@InProceedings{geiger_stereoscan:_2011,
  author     = {Geiger, A. and Ziegler, J. and Stiller, C.},
  title      = {{StereoScan}: {Dense} 3d reconstruction in real-time},
  booktitle  = {2011 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
  year       = {2011},
  month      = jun,
  pages      = {963--968},
  doi        = {10.1109/IVS.2011.5940405},
  abstract   = {Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/GIKWW7XE/5940405.html/\:text/html\:PDF:PDF},
  keywords   = {robot vision, Visualization, Cameras, Estimation, image sequences, stereo image processing, Real time systems, computer vision, Three dimensional displays, image reconstruction, image matching, video sequences, distance measurement, robotics, Image reconstruction, 3D maps, 3D point clouds, camera resolution, dense 3D reconstruction, feature matcher, high resolution stereo sequences, multiview linking scheme, robust visual odometry algorithm, scene analysis, Stereo image processing, stereo matching, StereoScan},
  shorttitle = {{StereoScan}},
}

@Article{sappa_monocular_2016,
  author     = {Sappa, Angel D. and Aguilera, Cristhian A. and Carvajal Ayala, Juan A. and Oliveira, Miguel and Romero, Dennis and Vintimilla, Boris X. and Toledo, Ricardo},
  title      = {Monocular visual odometry: {A} cross-spectral image fusion based approach},
  journal    = {Robotics and Autonomous Systems},
  year       = {2016},
  volume     = {85},
  month      = nov,
  pages      = {26--36},
  issn       = {0921-8890},
  doi        = {10.1016/j.robot.2016.08.005},
  url        = {http://www.sciencedirect.com/science/article/pii/S0921889016301002},
  urldate    = {2018-04-19},
  abstract   = {This manuscript evaluates the usage of fused cross-spectral images in a monocular visual odometry approach. Fused images are obtained through a Discrete Wavelet Transform (DWT) scheme, where the best setup is empirically obtained by means of a mutual information based evaluation metric. The objective is to have a flexible scheme where fusion parameters are adapted according to the characteristics of the given images. Visual odometry is computed from the fused monocular images using an off the shelf approach. Experimental results using data sets obtained with two different platforms are presented. Additionally, comparison with a previous approach as well as with monocular-visible/infrared spectra are also provided showing the advantages of the proposed scheme.},
  file       = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/STN46TP8/S0921889016301002.html/\:text/html\:PDF:PDF},
  keywords   = {Image fusion, LWIR-RGB cross-spectral imaging, Monocular visual odometry},
  shorttitle = {Monocular visual odometry},
}

@Article{engel_photometrically_2016,
  author   = {Engel, Jakob and Usenko, Vladyslav and Cremers, Daniel},
  title    = {A {Photometrically} {Calibrated} {Benchmark} {For} {Monocular} {Visual} {Odometry}},
  journal  = {arXiv:1607.02555 [cs]},
  year     = {2016},
  month    = jul,
  note     = {arXiv: 1607.02555},
  url      = {http://arxiv.org/abs/1607.02555},
  urldate  = {2018-04-19},
  abstract = {We present a dataset for evaluating the tracking accuracy of monocular visual odometry and SLAM methods. It contains 50 real-world sequences comprising more than 100 minutes of video, recorded across dozens of different environments -- ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position. This allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated. We provide exposure times for each frame as reported by the sensor, the camera response function, and dense lens attenuation factors. We also propose a novel, simple approach to non-parametric vignette calibration, which requires minimal set-up and is easy to reproduce. Finally, we thoroughly evaluate two existing methods (ORB-SLAM and DSO) on the dataset, including an analysis of the effect of image resolution, camera field of view, and the camera motion direction.},
  annote   = {Comment: * Corrected a bug in the evaluation setup, which caused the real-time results for ORB-SLAM (dashed lines in Figure 8) to be much worse than they should be. * https://vision.in.tum.de/data/datasets/mono-dataset},
  file     = {html\:PDF:html\:arXiv.org Snapshot/\:C//\:/Users/Psykie/Zotero/storage/5M8RB8MR/1607.html/\:text/html\:PDF:PDF},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@Article{kaess_isam2:_2012,
  author     = {Kaess, Michael and Johannsson, Hordur and Roberts, Richard and Ila, Viorela and Leonard, John J and Dellaert, Frank},
  title      = {{iSAM}2: {Incremental} smoothing and mapping using the {Bayes} tree},
  journal    = {The International Journal of Robotics Research},
  year       = {2012},
  language   = {en},
  volume     = {31},
  number     = {2},
  month      = feb,
  pages      = {216--235},
  issn       = {0278-3649},
  doi        = {10.1177/0278364911430419},
  urldate    = {2018-04-19},
  abstract   = {We present a novel data structure, the Bayes tree, that provides an algorithmic foundation enabling a better understanding of existing graphical model inference algorithms and their connection to sparse matrix factorization methods. Similar to a clique tree, a Bayes tree encodes a factored probability density, but unlike the clique tree it is directed and maps more naturally to the square root information matrix of the simultaneous localization and mapping (SLAM) problem. In this paper, we highlight three insights provided by our new data structure. First, the Bayes tree provides a better understanding of the matrix factorization in terms of probability densities. Second, we show how the fairly abstract updates to a matrix factorization translate to a simple editing of the Bayes tree and its conditional densities. Third, we apply the Bayes tree to obtain a completely novel algorithm for sparse nonlinear incremental optimization, named iSAM2, which achieves improvements in efficiency through incremental variable re-ordering and fluid relinearization, eliminating the need for periodic batch steps. We analyze various properties of iSAM2 in detail, and show on a range of real and simulated datasets that our algorithm compares favorably with other recent mapping algorithms in both quality and efficiency.},
  shorttitle = {{iSAM}2},
}

@InProceedings{engel_lsd-slam:_2014,
  author     = {Engel, Jakob and Schöps, Thomas and Cremers, Daniel},
  title      = {{LSD}-{SLAM}: {Large}-{Scale} {Direct} {Monocular} {SLAM}},
  booktitle  = {Computer {Vision} – {ECCV} 2014},
  year       = {2014},
  language   = {en},
  series     = {Lecture {Notes} in {Computer} {Science}},
  publisher  = {Springer, Cham},
  month      = sep,
  isbn       = {978-3-319-10604-5 978-3-319-10605-2},
  pages      = {834--849},
  doi        = {10.1007/978-3-319-10605-2_54},
  urldate    = {2018-04-19},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/3XN3NS2T/978-3-319-10605-2_54.html/\:text/html\:PDF:PDF},
  shorttitle = {{LSD}-{SLAM}},
}

@Article{daftry_semi-dense_2015,
  author  = {Daftry, Shreyansh and Dey, Debadeepta and Sandhawalia, Harsimrat and Zeng, Sam and Bagnell, J. Andrew and Hebert, Martial},
  title   = {Semi-{Dense} {Visual} {Odometry} for {Monocular} {Navigation} in {Cluttered} {Environment}},
  journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year    = {2015},
  month   = may,
  file    = {html\:PDF:html\:"Semi-Dense Visual Odometry for Monocular Navigation in Cluttered Envir" by Shreyansh Daftry, Debadeepta Dey et al./\:C//\:/Users/Psykie/Zotero/storage/EXUM64B9/1115.html/\:text/html\:PDF:PDF},
}

@InProceedings{meilland_real-time_2011,
  author    = {Maxime Meilland and Andrew Comport and Patrick Rives},
  title     = {Real-time {Dense} {Visual} {Tracking} under {Large} {Lighting} {Variations}},
  booktitle = {Procedings of the British Machine Vision Conference 2011},
  year      = {2011},
  language  = {en},
  publisher = {British Machine Vision Association},
  isbn      = {978-1-901725-43-8},
  pages     = {451--4511},
  doi       = {10.5244/C.25.45},
  url       = {http://www.bmva.org/bmvc/2011/proceedings/paper45/index.html},
  urldate   = {2018-04-19},
}

@InProceedings{newcombe_dtam:_2011,
  author     = {Newcombe, R. A. and Lovegrove, S. J. and Davison, A. J.},
  title      = {{DTAM}: {Dense} tracking and mapping in real-time},
  booktitle  = {2011 {International} {Conference} on {Computer} {Vision}},
  year       = {2011},
  month      = nov,
  pages      = {2320--2327},
  doi        = {10.1109/ICCV.2011.6126513},
  abstract   = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/MABB2KGM/6126513.html/\:text/html\:PDF:PDF},
  keywords   = {Cameras, image texture, Real time systems, object tracking, cameras, image reconstruction, Robustness, graphics processing units, Optimization, Tracking, augmented reality, Vectors, image motion analysis, video stream, Image reconstruction, concave programming, dense model, dense tracking and mapping, energy functional, GPU hardware, hand-held RGB camera, image alignment, nonconvex optimisation, photometric data term, physics-enhanced augmented reality, real-time camera reconstruction, real-time camera tracking, real-time scene interaction, textured depth map},
  shorttitle = {{DTAM}},
}

@InProceedings{alcantarilla_fast_2013,
  author    = {Pablo Alcantarilla and Jesus Nuevo and Adrien Bartoli},
  title     = {Fast {Explicit} {Diffusion} for {Accelerated} {Features} in {Nonlinear} {Scale} {Spaces}},
  booktitle = {Procedings of the British Machine Vision Conference 2013},
  year      = {2013},
  language  = {en},
  publisher = {British Machine Vision Association},
  isbn      = {978-1-901725-49-0},
  pages     = {131--1311},
  doi       = {10.5244/C.27.13},
  url       = {http://www.bmva.org/bmvc/2013/Papers/paper0013/index.html},
  urldate   = {2018-04-19},
}

@InProceedings{chien_when_2016,
  author     = {Chien, H. J. and Chuang, C. C. and Chen, C. Y. and Klette, R.},
  title      = {When to use what feature? {SIFT}, {SURF}, {ORB}, or {A}-{KAZE} features for monocular visual odometry},
  booktitle  = {2016 {International} {Conference} on {Image} and {Vision} {Computing} {New} {Zealand} ({IVCNZ})},
  year       = {2016},
  month      = nov,
  pages      = {1--6},
  doi        = {10.1109/IVCNZ.2016.7804434},
  abstract   = {Image feature-based ego-motion estimation has been dominating the development of visual odometry (VO) visual simultaneously localisation and mapping (V-SLAM) and structure-from-motion (SfM) for several years. The detection extraction or representation of image features play crucial roles when solving camera pose estimation problems in terms of accuracy and computational cost. In this paper we review three popular classes of image features namely SIFT SURF and ORB as well as the recently proposed A-KAZE features. These image features are evaluated using the KITTI benchmark dataset to conclude about reasons for deciding about the selection of a particular feature when implementing monocular visual odometry.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/RASUYNX8/7804434.html/\:text/html\:PDF:PDF},
  keywords   = {Visualization, feature extraction, Cameras, Three-dimensional displays, SLAM (robots), Feature extraction, computer vision, image representation, SIFT, transforms, motion estimation, ORB, SURF, A-KAZE feature, Dogs, ego-motion estimation, image feature representation, Maximum likelihood estimation, monocular visual odometry, oriented FAST and rotated BRIEF, scale-invariant feature transform, SfM, speeded-up robust feature, structure-from-motion, V-SLAM, visual simultaneously localization and mapping},
  shorttitle = {When to use what feature?},
}

@Article{ceriani_rawseeds_2009,
  author   = {Ceriani, Simone and Fontana, Giulio and Giusti, Alessandro and Marzorati, Daniele and Matteucci, Matteo and Migliore, Davide and Rizzi, Davide and Sorrenti, Domenico G. and Taddei, Pierluigi},
  title    = {Rawseeds ground truth collection systems for indoor self-localization and mapping},
  journal  = {Autonomous Robots},
  year     = {2009},
  language = {en},
  volume   = {27},
  number   = {4},
  month    = nov,
  pages    = {353},
  issn     = {0929-5593, 1573-7527},
  doi      = {10.1007/s10514-009-9156-5},
  urldate  = {2018-04-23},
  abstract = {A trustable and accurate ground truth is a key requirement for benchmarking self-localization and mapping algorithms; on the other hand, collection of ground truth is a complex and daunting task, and its validation is a challenging issue. In this paper we propose two techniques for indoor ground truth collection, developed in the framework of the European project Rawseeds, which are mutually independent and also independent on the sensors onboard the robot. These techniques are based, respectively, on a network of fixed cameras, and on a network of fixed laser scanners. We show how these systems are implemented and deployed, and, most importantly, we evaluate their performance; moreover, we investigate the possible fusion of their outputs.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/8E3WBUXV/s10514-009-9156-5.html/\:text/html\:PDF:PDF},
}

@InCollection{poujol_visible-thermal_2016,
  author    = {Poujol, Julien and Aguilera, Cristhian A. and Danos, Etienne and Vintimilla, Boris X. and Toledo, Ricardo and Sappa, Angel D.},
  title     = {A {Visible}-{Thermal} {Fusion} {Based} {Monocular} {Visual} {Odometry}},
  booktitle = {Robot 2015: {Second} {Iberian} {Robotics} {Conference}},
  year      = {2016},
  language  = {en},
  series    = {Advances in {Intelligent} {Systems} and {Computing}},
  publisher = {Springer, Cham},
  isbn      = {978-3-319-27145-3 978-3-319-27146-0},
  pages     = {517--528},
  doi       = {10.1007/978-3-319-27146-0_40},
  urldate   = {2018-04-23},
  abstract  = {The manuscript evaluates the performance of a monocular visual odometry approach when images from different spectra are considered, both independently and fused. The objective behind this evaluation is to analyze if classical approaches can be improved when the given images, which are from different spectra, are fused and represented in new domains. The images in these new domains should have some of the following properties: i) more robust to noisy data; ii) less sensitive to changes (e.g., lighting); iii) more rich in descriptive information, among other. In particular in the current work two different image fusion strategies are considered. Firstly, images from the visible and thermal spectrum are fused using a Discrete Wavelet Transform (DWT) approach. Secondly, a monochrome threshold strategy is considered. The obtained representations are evaluated under a visual odometry framework, highlighting their advantages and disadvantages, using different urban and semi-urban scenarios. Comparisons with both monocular-visible spectrum and monocular-infrared spectrum, are also provided showing the validity of the proposed approach.},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/VXQHWVUH/978-3-319-27146-0_40.html/\:text/html\:PDF:PDF},
}

@Article{choi_kaist_2018,
  author   = {Choi, Y. and Kim, N. and Hwang, S. and Park, K. and Yoon, J. S. and An, K. and Kweon, I. S.},
  title    = {{KAIST} {Multi}-{Spectral} {Day}/{Night} {Data} {Set} for {Autonomous} and {Assisted} {Driving}},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2018},
  volume   = {19},
  number   = {3},
  month    = mar,
  pages    = {934--948},
  issn     = {1524-9050},
  doi      = {10.1109/TITS.2018.2791533},
  abstract = {We introduce the KAIST multi-spectral data set, which covers a great range of drivable regions, from urban to residential, for autonomous systems. Our data set provides the different perspectives of the world captured in coarse time slots (day and night), in addition to fine time slots (sunrise, morning, afternoon, sunset, night, and dawn). For all-day perception of autonomous systems, we propose the use of a different spectral sensor, i.e., a thermal imaging camera. Toward this goal, we develop a multi-sensor platform, which supports the use of a co-aligned RGB/Thermal camera, RGB stereo, 3-D LiDAR, and inertial sensors (GPS/IMU) and a related calibration technique. We design a wide range of visual perception tasks including the object detection, drivable region detection, localization, image enhancement, depth estimation, and colorization using a single/multi-spectral approach. In this paper, we provide a description of our benchmark with the recording platform, data format, development toolkits, and lessons about the progress of capturing data sets.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/MGVGFK7S/8293689.html/\:text/html\:PDF:PDF},
  keywords = {autonomous driving, mobile robots, robot vision, Cameras, image colour analysis, object detection, Optical imaging, Optical sensors, stereo image processing, image fusion, image enhancement, Laser radar, cameras, optical radar, advanced driver assistance system, Dataset, Calibration, autonomous systems, calibration, 3D LiDAR, all-day perception, benchmarks, co-aligned RGB-thermal camera, coarse time slots, colorization, data format, depth estimation, development toolkits, drivable region detection, fine time slots, inertial sensors, infrared imaging, KAIST multi-sepctral, KAIST multispectral day data set, KAIST multispectral night data set, multi-spectral dataset in day and night, multi-spectral vehicle system, multisensor platform, multispectral approach, Optical distortion, recording platform, related calibration technique, RGB stereo, single spectral approach, spectral analysis, spectral sensor, thermal imaging camera, Thermal sensors, visual perception, visual perception tasks},
}

@WWW{sae_international_student_nodate,
  author  = {{SAE International}},
  title   = {Student {Events} - {Events} - {Collegiate} {Design} {Series}},
  year    = {2018},
  url     = {https://www.sae.org/attend/student-events/},
  urldate = {2018-04-29},
  file    = {html\:PDF:html\:Student Events - Events - Collegiate Design Series/\:C//\:/Users/Psykie/Zotero/storage/XZW7ADWT/student-events.html/\:text/html\:PDF:PDF},
}

@Article{valiente_garcia_visual_2012,
  author    = {Valiente García, David and Fernández Rojo, Lorenzo and Gil Aparicio, Arturo and Payá Castelló, Luis and Reinoso García, Oscar},
  title     = {Visual {Odometry} through {Appearance}- and {Feature}-{Based} {Method} with {Omnidirectional} {Images}},
  journal   = {Journal of Robotics},
  year      = {2012},
  language  = {en},
  pages     = {1--13},
  doi       = {10.1155/2012/797063},
  url       = {https://www.hindawi.com/journals/jr/2012/797063/},
  urldate   = {2018-04-30},
  abstract  = {In the field of mobile autonomous robots, visual odometry entails the retrieval of a motion transformation between two consecutive poses of the robot by means of a camera sensor solely. A visual odometry provides an essential information for trajectory estimation in problems such as Localization and SLAM (Simultaneous Localization and Mapping). In this work we present a motion estimation based on a single omnidirectional camera. We exploited the maximized horizontal field of view provided by this camera, which allows us to encode large scene information into the same image. The estimation of the motion transformation between two poses is incrementally computed, since only the processing of two consecutive omnidirectional images is required. Particularly, we exploited the versatility of the information gathered by omnidirectional images to perform both an appearance-based and a feature-based method to obtain visual odometry results. We carried out a set of experiments in real indoor environments to test the validity and suitability of both methods. The data used in the experiments consists of a large sets of omnidirectional images captured along the robot's trajectory in three different real scenarios. Experimental results demonstrate the accuracy of the estimations and the capability of both methods to work in real-time.},
  file      = {xhtml+xml\:PDF:xhtml+xml\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/FL6D6BRK/797063.html/\:application/xhtml+xml\:PDF:PDF},
  publisher = {Hindawi Limited},
  type      = {Research article},
}

@article{martinez-lao_electric_2017,
	title = {Electric vehicles in {Spain}: {An} overview of charging systems},
	volume = {77},
	issn = {1364-0321},
	url = {http://www.sciencedirect.com/science/article/pii/S1364032116310152},
	doi = {10.1016/j.rser.2016.11.239},
	abstract = {The transportation sector is characterized by a high consumption of fossil fuels and a strong environmental impact. Promoting electric vehicles is an alternative to reduce and limit them move towards the sustainability of the automobile sector. In a short period of time, world car manufacturers have built, marketed and sold a million electric vehicles, and a million drivers got used to these new low carbon advanced technologies. Comparatively, this figure represents approximately the average annual sales of conventional vehicles in Spain. The main problem is the battery autonomy, since its maximum range does not exceed 250km, a restriction that limits the trip. Spain belongs to the group of countries which have longest trip average around 80km. Then the problem is how to understand electric mobility, for that the types and modes of charging, the types of electric vehicles, and the available charging systems all interact with one another in the charging systems for electric vehicles, which will be specifically analysed. Alternative charging methods are also presented, and the agents involved in the charging process in accordance with applicable regulations are identified. The objective of this article is to analyse the charging of electric vehicles in Spain and to assess the current situation to be able to propose potential improvements or implementation strategies. This paper determines that it is necessary to develop public policies for a structured implementation of charging stations in public places and in common-use areas within large shared spaces, such as parking areas and residential areas in order to improve electric mobility in Spain. This paper also illustrates the need to legislate standards for charging electric vehicles to maximize their implementation in Spain, with the goal of implementing electric vehicles on a larger scale and ultimately allowing society to benefit from the advantages of this technology.},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Martínez-Lao, Juan and Montoya, Francisco G. and Montoya, Maria G. and Manzano-Agugliaro, Francisco},
	month = sep,
	year = {2017},
	pages = {970--983}
}

@article{ji_plug-electric_2018,
	title = {Plug-in electric vehicle charging infrastructure deployment of {China} towards 2020: {Policies}, methodologies, and challenges},
	volume = {90},
	issn = {1364-0321},
	url = {http://www.sciencedirect.com/science/article/pii/S136403211830220X},
	doi = {10.1016/j.rser.2018.04.011},
	abstract = {Plug-in electric vehicles (PEVs) are seen to be one of the solutions for China to reduce both greenhouse gas emissions and dependency on oil importation in the transport sector. Fully aware of the interaction between the market diffusion of PEVs and charging infrastructures, China's central government has issued a series of policies to promote a national charging network; this is among the largest deployment programmes in the world. Following the elaboration of China's targets for charging infrastructure deployment by 2020, there are related topics including charging pricing policies, standardisation of the charging infrastructures, renovations of the power system, and incentives for PEV adoption. A comprehensive understanding of the deployment process will help achieve the challenging goal. Therefore, related methodologies for fundamental issues in the process, which are charging technology, allocation optimisation, and smart charging management, respectively, are systematically reviewed. Furthermore, the barriers in the near-term deployment process are discussed and analysed focusing on the efficiency of these policies. Analytical results show that, while the major challenge lies in the profit expectation of the infrastructure investor sector, the design of subsidies for PEVs and chargers should be strengthened and well-balanced at the initial stage. Also, outlooks for China's future energy projections demonstrate the feasibility of providing massive PEVs with low-carbon electricity.},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Ji, Zhenya and Huang, Xueliang},
	month = jul,
	year = {2018},
	pages = {710--727}
}

@Article{liang_analysis_2008,
  author   = {Liang, C. and Chang, L. and Chen, H. H.},
  title    = {Analysis and {Compensation} of {Rolling} {Shutter} {Effect}},
  journal  = {IEEE Transactions on Image Processing},
  year     = {2008},
  volume   = {17},
  number   = {8},
  month    = aug,
  pages    = {1323--1330},
  issn     = {1057-7149},
  doi      = {10.1109/TIP.2008.925384},
  abstract = {Due to the sequential-readout structure of complementary metal-oxide semiconductor image sensor array, each scanline of the acquired image is exposed at a different time, resulting in the so-called electronic rolling shutter that induces geometric image distortion when the object or the video camera moves during image capture. In this paper, we propose an image processing technique using a planar motion model to address the problem. Unlike previous methods that involve complex 3-D feature correspondences, a simple approach to the analysis of inter- and intraframe distortions is presented. The high-resolution velocity estimates used for restoring the image are obtained by global motion estimation, Bezier curve fitting, and local motion estimation without resort to correspondence identification. Experimental results demonstrate the effectiveness of the algorithm.},
  file     = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/FQF73SZK/4549748.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/5H8G2UXA/Liang et al. - 2008 - Analysis and Compensation of Rolling Shutter Effec.pdf/\:application/pdf\:PDF:PDF},
  keywords = {Cameras, Layout, Sensor arrays, video camera, Algorithms, Image Enhancement, Image Interpretation, Computer-Assisted, Reproducibility of Results, Sensitivity and Specificity, Motion estimation, motion estimation, array signal processing, Artifacts, Bezier curve fitting, CMOS image sensors, CMOS process, CMOS technology, Complementary metal-oxide semiconductor (CMOS) sensor array, complementary metal-oxide semiconductor image sensor array, complex 3D feature correspondences, electronic rolling shutter, geometric image distortion, Image processing, Image sensors, interframe distortions, intraframe distortions, MOS devices, motion analysis, Photography, planar motion model, rolling shutter, rolling shutter effect, sequential-readout structure},
}

@WWW{flir_fujinon_nodate,
  author  = {FLIR},
  title   = {Fujinon {YV}2.8×2.8SA-2, 2.8mm-8mm, 1/3", {CS} mount {Lens}},
  year    = {2018},
  url     = {https://www.ptgrey.com/fujinon-yv28x28sa-2-hd-vari-focal-lens-3},
  urldate = {2018-09-05},
  file    = {html\:PDF:html\:Fujinon YV2.8×2.8SA-2, 2.8mm-8mm, 1/3", CS mount Lens/\:C//\:/Users/Psykie/Zotero/storage/MR2SSXS3/fujinon-yv28x28sa-2-hd-vari-focal-lens-3.html/\:text/html\:PDF:PDF},
}

@WWW{flir_blackfly_nodate,
  author   = {FLIR},
  title    = {Blackfly 1.3 {MP} {Color} {GigE} {PoE} ({Sony} {ICX}445)},
  year     = {2018},
  url      = {https://www.ptgrey.com/blackfly-13-mp-color-gige-vision-poe-sony-icx445-camera},
  urldate  = {2018-09-05},
  abstract = {The Blackfly camera line combines Power over Ethernet, highly sensitive sensors, industry-leading affordability and a host of unique features for uncompromising value. Every Blackfly model comes packed with functionality designed to maximize performance and reliability, including a 16 MByte frame buffer; LED status indicators; and an on-camera image processing pipeline that provides color interpolation, gamma, and lookup table functionality. The Blackfly camera weighs just 36 grams, measures 29x29x30mm and uses 2 watts of power.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/2PF754Y3/blackfly-13-mp-color-gige-vision-poe-sony-icx445-camera.html/\:text/html\:PDF:PDF},
  journal  = {FLIR},
}

@WWW{sick_ag_lms111-10100_nodate,
  author  = {{SICK AG}},
  title   = {{LMS}111-10100},
  year    = {2018},
  url     = {https://www.sick.com/au/en/detection-and-ranging-solutions/2d-lidar-sensors/lms1xx/lms111-10100/p/p109842},
  urldate = {2018-09-05},
  file    = {html\:PDF:html\:LMS111-10100 | Detection and ranging solutions | SICK/\:C//\:/Users/Psykie/Zotero/storage/2YJIPV9P/p109842.html/\:text/html\:PDF:PDF},
  journal = {Detection and Ranging Solutions},
}

@WWW{arduino_arduino_nodate-1,
  author  = {Arduino},
  title   = {Arduino {Nano}},
  year    = {2018},
  url     = {https://store.arduino.cc/arduino-nano},
  urldate = {2018-09-05},
  file    = {html\:PDF:html\:Arduino Nano/\:C//\:/Users/Psykie/Zotero/storage/YTFTRJDN/arduino-nano.html/\:text/html\:PDF:PDF},
  journal = {Arduino Nano},
}

@InProceedings{lim_modular_2018,
  author    = {Kai Li Lim and Thomas Drage and Roman Podolski and Gabriel Meyer-Lee and Samuel Evans-Thompson and Jason Yao-Tsu Lin and Geoffrey Channon and Mitchell Poole and Thomas Bräunl},
  title     = {A Modular Software Framework for Autonomous Vehicles},
  booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
  year      = {2018},
  month     = jun,
  pages     = {1780--1785},
  doi       = {10.1109/IVS.2018.8500474},
  issn      = {1931-0587},
  keywords  = {data handling;road vehicles;traffic engineering computing;unified software framework;navigational processing;navigation subroutine individual classes;Formula SAE vehicle;fully autonomous drives;modular software framework;autonomous vehicles;software frameworks;navigational processes;path planning;lane keeping;odometric sensors;positioning;autonomous driving;data processing;data interface;Sensors;Global Positioning System;Software;Automobiles;Autonomous vehicles;Wheels;Software algorithms},
}

@book{de_boor_practical_1978,
	title = {A practical guide to splines},
	volume = {27},
	publisher = {Springer-Verlag New York},
	author = {De Boor, Carl},
	year = {1978}
}

@book{hermite_oeuvres_1905,
	title = {{OEuvres} de {Charles} {Hermite}, publiées sous les auspices de l'{Académie} des sciences},
	url = {http://archive.org/details/oeuvresdecharles03hermuoft},
	abstract = {14},
	language = {fre},
	urldate = {2018-08-31},
	publisher = {Paris Gauthier-Villars},
	author = {Hermite, Charles and Picard, Emile and Académie des sciences, Paris},
	collaborator = {{Gerstein - University of Toronto}},
	year = {1905},
	keywords = {Mathematics}
}

@Article{kam_rviz:_2015,
  author     = {Kam, Hyeong Ryeol and Lee, Sung-Ho and Park, Taejung and Kim, Chang-Hun},
  title      = {{RViz}: {A} {Toolkit} for {Real} {Domain} {Data} {Visualization}},
  journal    = {Telecommun. Syst.},
  year       = {2015},
  volume     = {60},
  number     = {2},
  month      = oct,
  pages      = {337--345},
  issn       = {1018-4864},
  doi        = {10.1007/s11235-015-0034-5},
  urldate    = {2018-08-31},
  abstract   = {In computational science and computer graphics, there is a strong requirement to represent and visualize information in the real domain, and many visualization data structures and algorithms have been proposed to achieve this aim. Unfortunately, the dataflow model that is often selected to address this issue in visualization systems is not flexible enough to visualize newly invented data structures and algorithms because this scheme can accept only specific data structures. To address this problem, we propose a new visualization tool, RViz, which is independent of the input information data structures. Since there is no requirement for additional efforts to manage the flow networks and the interface to abstracted information is simple in RViz, any scientific information visualization algorithms are easier to implement than the dataflow model. In this paper, we provide case studies in which we have successfully implemented new data structures and related algorithms using RViz, including geometry synthesis, distance field representation, and implicit surface reconstruction. Through these cases, we show how RViz helps users visualize and understand any hidden insights in input information.},
  keywords   = {Decorator pattern, Lightweight framework, Spatial data visualization, Visualization toolkit},
  shorttitle = {{RViz}},
}

@WWW{open_source_robotics_foundation_ros/introduction_nodate,
  author  = {{Open Source Robotics Foundation}},
  title   = {{ROS}/{Introduction} - {ROS} {Wiki}},
  year    = {2019},
  url     = {http://wiki.ros.org/ROS/Introduction},
  urldate = {2018-08-29},
  file    = {html\:PDF:html\:ROS/Introduction - ROS Wiki/\:C//\:/Users/Psykie/Zotero/storage/LKRCXKU4/Introduction.html/\:text/html\:PDF:PDF},
}

@WWW{xsens_mti-g-710_nodate,
  author   = {Xsens},
  title    = {{MTi}-{G}-710},
  year     = {2019},
  url      = {https://www.xsens.com/products/mti-g-710/},
  language = {en-US},
  urldate  = {2018-09-05},
  abstract = {The MTi-G-710 is an industrial grade miniature GNSS-aided, IMU-enhanced GNSS/INS...},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/ZIVX6TSM/mti-g-710.html/\:text/html\:PDF:PDF},
  journal  = {Xsens 3D motion tracking},
}

@Misc{yu_derivation_nodate,
  author   = {Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},
  title    = {Derivation of {Extended} {Kalman} {Filtering} and {Smoothing} {Equations}},
  year     = {2004},
  date     = {2004-10-19},
  language = {en},
  month    = oct,
  abstract = {State estimation for nonlinear dynamical systems can be performed via local linearization of the nonlinearities. This Extended Kalman approach can be used for both filtering [1], [2] and smoothing. We follow the approach in [3] to derive the forward and backward Extended Kalman recursions. We assume that the reader is familiar with [3].},
  pages    = {4},
}

@article{morrell_extended_1997,
	title = {Extended {Kalman} {Filter} {Lecture} {Notes}},
	journal = {EEE 581-Spring, Arizona State University},
	author = {Morrell, Darryl},
	year = {1997}
}

@inproceedings{moore_generalized_2016,
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {A {Generalized} {Extended} {Kalman} {Filter} {Implementation} for the {Robot} {Operating} {System}},
	isbn = {978-3-319-08338-4},
	abstract = {Accurate state estimation for a mobile robot often requires the fusion of data from multiple sensors. Software that performs sensor fusion should therefore support the inclusion of a wide array of heterogeneous sensors. This paper presents a software package, robot\_localization, for the robot operating system (ROS). The package currently contains an implementation of an extended Kalman filter (EKF). It can support an unlimited number of inputs from multiple sensor types, and allows users to customize which sensor data fields are fused with the current state estimate. In this work, we motivate our design decisions, discuss implementation details, and provide results from real-world tests.},
	language = {en},
	booktitle = {Intelligent {Autonomous} {Systems} 13},
	publisher = {Springer International Publishing},
	author = {Moore, Thomas and Stouch, Daniel},
	editor = {Menegatti, Emanuele and Michael, Nathan and Berns, Karsten and Yamaguchi, Hiroaki},
	year = {2016},
	keywords = {Localization, Extended kalman filter, Robot operating system, Sensor fusion},
	pages = {335--348}
}

@TechReport{formula_student_germany_gmbh_fsg_2018,
  author      = {{Formula Student Germany GmbH}},
  title       = {{FSG} {Competition} {Handbook} 2018},
  institution = {Formula Student Germany GmbH},
  year        = {2018},
  language    = {English},
  number      = {1.1},
  month       = apr,
  pages       = {21},
  url         = {https://www.formulastudent.de/fileadmin/user_upload/all/2018/rules/FSG2018_Competition_Handbook_V1.1.pdf},
  urldate     = {2018-09-06},
  address     = {Wiesbaden, Germany},
  school      = {Formula Student Germany},
}

@Article{badrinarayanan_segnet:_2017,
  author     = {Badrinarayanan, V. and Kendall, A. and Cipolla, R.},
  title      = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
  journal    = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year       = {2017},
  volume     = {39},
  number     = {12},
  month      = dec,
  pages      = {2481--2495},
  issn       = {0162-8828},
  doi        = {10.1109/TPAMI.2016.2644615},
  abstract   = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  file       = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/EPJLJD73/7803544.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/FKI2DAFV/Badrinarayanan et al. - 2017 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf/\:application/pdf\:PDF:PDF},
  keywords   = {Training, Computer architecture, feature extraction, image classification, image resolution, image segmentation, Semantics, Image segmentation, Neural networks, image colour analysis, learning (artificial intelligence), image representation, topology, Caffe implementation, competitive inference time, Convolutional codes, convolutional layers, core trainable segmentation engine, decoder, decoder network, Decoding, DeconvNet architectures, deep convolutional encoder-decoder architecture, Deep convolutional neural networks, DeepLab-LargeFOV, dense feature maps, encoder, encoder network, FCN, gradient methods, indoor scenes, inference mechanisms, low resolution encoder feature maps, lower resolution input feature map, max-pooling step, nonlinear upsampling, pixel-wise classification layer, pixel-wise segmentation, pooling, practical deep fully convolutional neural network architecture, road scenes, SegNet, self-organising feature maps, semantic pixel-wise segmentation, stochastic gradient descent, SUN RGB-D indoor scene segmentation, SUN RGB-D indoor scene segmentation tasks, upsampling, VGG16 network},
  shorttitle = {{SegNet}},
}

@Article{fuentes-pacheco_visual_2015,
  author     = {Fuentes-Pacheco, Jorge and Ruiz-Ascencio, José and Rendón-Mancha, Juan Manuel},
  title      = {Visual simultaneous localization and mapping: a survey},
  journal    = {Artificial Intelligence Review},
  year       = {2015},
  language   = {en},
  volume     = {43},
  number     = {1},
  month      = jan,
  pages      = {55--81},
  issn       = {1573-7462},
  doi        = {10.1007/s10462-012-9365-8},
  urldate    = {2018-09-06},
  abstract   = {Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art.},
  keywords   = {Visual SLAM, Data association, Image matching, Salient feature selection, Topological and metric maps},
  shorttitle = {Visual simultaneous localization and mapping},
}

@InProceedings{geiger_are_2012,
  author     = {Geiger, A. and Lenz, P. and Urtasun, R.},
  title      = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
  booktitle  = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  year       = {2012},
  month      = jun,
  pages      = {3354--3361},
  doi        = {10.1109/CVPR.2012.6248074},
  abstract   = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
  annote     = {The following values have no corresponding Zotero field:alt-title: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  file       = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/7Q6BPQ4L/6248074.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/NQID9L2R/Geiger et al. - 2012 - Are we ready for autonomous driving The KITTI vis.pdf/\:application/pdf\:PDF:PDF},
  keywords   = {autonomous driving, robot vision, Visualization, Benchmark testing, Cameras, object detection, 3D object detection, high resolution video cameras, image sequences, KITTI vision benchmark suite, Measurement, Middlebury perform, optical flow image pairs, Optical imaging, Optical sensors, SLAM, SLAM (robots), stereo image processing, stereo visual odometry sequences, Velodyne laser scanner, video signal processing, visual recognition systems},
  shorttitle = {Are we ready for autonomous driving?},
}

@Article{mur-artal_orb-slam:_2015,
  author     = {Mur-Artal, R. and Montiel, J. M. M. and Tardós, J. D.},
  title      = {{ORB}-{SLAM}: {A} {Versatile} and {Accurate} {Monocular} {SLAM} {System}},
  journal    = {IEEE Transactions on Robotics},
  year       = {2015},
  volume     = {31},
  number     = {5},
  month      = oct,
  pages      = {1147--1163},
  issn       = {1552-3098},
  doi        = {10.1109/TRO.2015.2463671},
  abstract   = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  file       = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/9QKXJQPM/7219438.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/KZIJ8VHV/Mur-Artal et al. - 2015 - ORB-SLAM A Versatile and Accurate Monocular SLAM .pdf/\:application/pdf\:PDF:PDF},
  keywords   = {Visualization, Computational modeling, Cameras, SLAM (robots), Feature extraction, Simultaneous localization and mapping, Real-time systems, monocular vision, Optimization, simultaneous localization and mapping (SLAM), recognition, feature-based monocular simultaneous localization and mapping system, Lifelong mapping, localization, ORB-SLAM system, survival of the fittest strategy},
  shorttitle = {{ORB}-{SLAM}},
}

@Article{mur-artal_orb-slam2:_2017,
  author     = {Mur-Artal, R. and Tardós, J. D.},
  title      = {{ORB}-{SLAM}2: {An} {Open}-{Source} {SLAM} {System} for {Monocular}, {Stereo}, and {RGB}-{D} {Cameras}},
  journal    = {IEEE Transactions on Robotics},
  year       = {2017},
  volume     = {33},
  number     = {5},
  month      = oct,
  pages      = {1255--1262},
  issn       = {1552-3098},
  doi        = {10.1109/TRO.2017.2705103},
  abstract   = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  file       = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/TFKBMQ7K/7946260.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/K4DGZCXK/Mur-Artal and Tardós - 2017 - ORB-SLAM2 An Open-Source SLAM System for Monocula.pdf/\:application/pdf\:PDF:PDF},
  keywords   = {mobile robots, robot vision, Cameras, SLAM (robots), Feature extraction, path planning, cameras, mapping, Simultaneous localization and mapping, Kalman filters, stereo cameras, Optimization, distance measurement, motion estimation, simultaneous localization and mapping (SLAM), Trajectory, lightweight localization mode, Localization, map points, monocular cameras, open-source SLAM system, ORB-SLAM, RGB-D, RGB-D cameras, simultaneous localization and mapping system, SLAM community, stereo, Tracking loops, zero-drift localization},
  shorttitle = {{ORB}-{SLAM}2},
}

@Article{coughlan_manhattan_2003,
  author     = {Coughlan, James M. and Yuille, A. L.},
  title      = {Manhattan {World}: {Orientation} and {Outlier} {Detection} by {Bayesian} {Inference}},
  journal    = {Neural Computation},
  year       = {2003},
  volume     = {15},
  number     = {5},
  month      = may,
  pages      = {1063--1088},
  issn       = {0899-7667},
  doi        = {10.1162/089976603765202668},
  urldate    = {2018-10-05},
  abstract   = {This letter argues that many visual scenes are based on a “Manhattan” three-dimensional grid that imposes regularities on the image statistics. We construct a Bayesian model that implements this assumption and estimates the viewer orientation relative to the Manhattan grid. For many images, these estimates are good approximations to the viewer orientation (as estimated manually by the authors). These estimates also make it easy to detect outlier structures that are unaligned to the grid. To determine the applicability of the Manhattan world model, we implement a null hypothesis model that assumes that the image statistics are independent of any three-dimensional scene structure. We then use the log-likelihood ratio test to determine whether an image satisfies the Manhattan world assumption. Our results show that if an image is estimated to be Manhattan, then the Bayesian model's estimates of viewer direction are almost always accurate (according to our manual estimates), and vice versa.},
  file       = {html\:PDF:html\:Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/DKIC3KGF/Coughlan and Yuille - 2003 - Manhattan World Orientation and Outlier Detection.pdf/\:application/pdf/\;Snapshot/\:C//\:/Users/Psykie/Zotero/storage/XGGMM7SR/089976603765202668.html/\:text/html\:PDF:PDF},
  shorttitle = {Manhattan {World}},
}

@InProceedings{censi_accurate_2007,
  author    = {Censi, A.},
  title     = {An accurate closed-form estimate of {ICP}'s covariance},
  booktitle = {Proceedings 2007 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
  year      = {2007},
  month     = apr,
  pages     = {3167--3172},
  doi       = {10.1109/ROBOT.2007.363961},
  abstract  = {Existing methods for estimating the covariance of the ICP (iterative closest/corresponding point) algorithm are either inaccurate or are computationally too expensive to be used online. This paper proposes a new method, based on the analysis of the error function being minimized. It considers that the correspondences are not independent (the same measurement being used in more than one correspondence), and explicitly utilizes the covariance matrix of the measurements, which are not assumed to be independent either. The validity of the approach is verified through extensive simulations: it is more accurate than previous methods and its computational load is negligible. The ill-posedness of the surface matching problem is explicitly tackled for under-constrained situations by performing an observability analysis; in the analyzed cases the method still provides a good estimate of the error projected on the observable manifold.},
  file      = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/S5MFFMVK/4209579.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/JSD984LF/Censi - 2007 - An accurate closed-form estimate of ICP's covarian.pdf/\:application/pdf\:PDF:PDF},
  keywords  = {Iterative closest point algorithm, Robot sensing systems, Simultaneous localization and mapping, estimation theory, iterative methods, Robotics and automation, iterative closest point algorithm, Performance analysis, closed-form estimate, covariance matrices, covariance matrix, Error analysis, ICP covariance estimation, Impedance matching, Iterative algorithms, iterative corresponding point algorithm, Iterative methods, observability analysis, Random variables, surface matching},
}

@Article{bresenham_algorithm_1965,
  author   = {Bresenham, J. E.},
  title    = {Algorithm for computer control of a digital plotter},
  journal  = {IBM Systems Journal},
  year     = {1965},
  volume   = {4},
  number   = {1},
  pages    = {25--30},
  issn     = {0018-8670},
  doi      = {10.1147/sj.41.0025},
  abstract = {The algorithm can be programmed without the use of multiplication or division. It was found that 333 core locations were sufficient for an IBM 1401 program (used to control an IBM 1627). The average computation time between successive incrementations was approximately 1.5 milliseconds.},
  file     = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/NCVX5HGN/5388473.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/DXH6KC7F/Bresenham - 1965 - Algorithm for computer control of a digital plotte.pdf/\:application/pdf\:PDF:PDF},
}

@WWW{sti_engineering_pty_ltd_rfinnovations_nodate,
  author  = {{STI Engineering Pty Ltd}},
  title   = {{RFInnovations} {RFI}-9256 900MHz {High} {Speed} {Data} {Radio}},
  year    = {2018},
  url     = {http://www.rfinnovations.com.au/Uploads/Images/900MHz%20Data%20Radio%20Modem(2).pdf},
  urldate = {2018-10-05},
  journal = {RFInnovations},
}

@WWW{ubiquiti_networks_picostation2hp_nodate,
  author  = {{Ubiquiti Networks}},
  title   = {{PicoStation}2HP {Datasheet}},
  year    = {2018},
  url     = {https://dl.ubnt.com/pico2hp_ds.pdf},
  urldate = {2018-10-05},
  file    = {pdf\:PDF:pdf\:pico2hp_ds.pdf/\:C//\:/Users/Psykie/Zotero/storage/C2JIXRXP/pico2hp_ds.pdf/\:application/pdf\:PDF:PDF},
}

@WWW{adept_technology_inc._pioneer_nodate,
  author  = {{Adept Technology, Inc.}},
  title   = {Pioneer 3-{AT}},
  year    = {2018},
  url     = {http://www.mobilerobots.com/Libraries/Downloads/Pioneer3AT-P3AT-RevA.sflb.ashx},
  urldate = {2018-10-05},
  file    = {pdf\:PDF:pdf\:Pioneer3AT-P3AT-RevA.sflb.pdf/\:C//\:/Users/Psykie/Zotero/storage/UKMA9FRT/Pioneer3AT-P3AT-RevA.sflb.pdf/\:application/pdf\:PDF:PDF},
}

@WWW{logitech_quickcam_nodate,
  author  = {Logitech},
  title   = {{QuickCam}® {Orbit} {AF}},
  year    = {2018},
  url     = {https://support.logitech.com/en_us/product/quickcam-sphere-af/specs},
  urldate = {2018-10-05},
  file    = {html\:PDF:html\:QuickCam® Orbit AF - Logitech Support/\:C//\:/Users/Psykie/Zotero/storage/35YGKCQV/specs.html/\:text/html\:PDF:PDF},
}

@WWW{qstarz_international_co._ltd._bt-q818xt_nodate,
  author  = {{QStarz International Co., Ltd.}},
  title   = {{BT}-{Q}818XT},
  year    = {2018},
  url     = {http://www.qstarz.com/Products/GPS%20Products/BT-Q818XT-F.htm},
  urldate = {2018-10-05},
  file    = {html\:PDF:html\:BT-Q818XT/\:C//\:/Users/Psykie/Zotero/storage/J9QQ8U29/BT-Q818XT-F.html/\:text/html\:PDF:PDF},
}

@WWW{hokuyo_automatic_co._ltd._scanning_nodate,
  author  = {{Hokuyo Automatic Co., Ltd.}},
  title   = {Scanning {Rangefinder} {Distance} {Data} {Output}/{URG}-04LX-{UG}01},
  year    = {2018},
  url     = {https://www.hokuyo-aut.jp/search/single.php?serial=166},
  urldate = {2018-10-05},
  file    = {html\:PDF:html\:Scanning Rangefinder Distance Data Output/URG-04LX-UG01 Product Details | HOKUYO AUTOMATIC CO., LTD./\:C//\:/Users/Psykie/Zotero/storage/2G7Q8294/single.html/\:text/html\:PDF:PDF},
  journal = {Product Details},
}

@Article{quan_tightly-coupled_2018,
  author   = {Quan, Meixiang and Piao, Songhao and Tan, Minglang and Huang, Shi-Sheng},
  title    = {Tightly-coupled {Monocular} {Visual}-odometric {SLAM} using {Wheels} and a {MEMS} {Gyroscope}},
  journal  = {arXiv:1804.04854 [cs]},
  year     = {2018},
  month    = apr,
  note     = {arXiv: 1804.04854},
  url      = {http://arxiv.org/abs/1804.04854},
  urldate  = {2018-10-05},
  abstract = {In this paper, we present a novel tightly-coupled probabilistic monocular visual-odometric Simultaneous Localization and Mapping algorithm using wheels and a MEMS gyroscope, which can provide accurate, robust and long-term localization for the ground robot moving on a plane. Firstly, we present an odometer preintegration theory that integrates the wheel encoder measurements and gyroscope measurements to a local frame. The preintegration theory properly addresses the manifold structure of the rotation group SO(3) and carefully deals with uncertainty propagation and bias correction. Then the novel odometer error term is formulated using the odometer preintegration model and it is tightly integrated into the visual optimization framework. Furthermore, we introduce a complete tracking framework to provide different strategies for motion tracking when (1) both measurements are available, (2) visual measurements are not available, and (3) wheel encoder experiences slippage, which leads the system to be accurate and robust. Finally, the proposed algorithm is evaluated by performing extensive experiments, the experimental results demonstrate the superiority of the proposed system.},
  annote   = {Comment: 13 pages, 31 figures},
  file     = {html\:PDF:html\:arXiv//\:1804.04854 PDF/\:C//\:/Users/Psykie/Zotero/storage/6CXA4MNQ/Quan et al. - 2018 - Tightly-coupled Monocular Visual-odometric SLAM us.pdf/\:application/pdf/\;arXiv.org Snapshot/\:C//\:/Users/Psykie/Zotero/storage/XIFJ6GJV/1804.html/\:text/html\:PDF:PDF},
  keywords = {Computer Science - Robotics},
}

@Article{koch_multi-robot_2016,
  author   = {Koch, Philipp and May, Stefan and Schmidpeter, Michael and Kühn, Markus and Pfitzner, Christian and Merkl, Christian and Koch, Rainer and Fees, Martin and Martin, Jon and Ammon, Daniel and Nüchter, Andreas},
  title    = {Multi-{Robot} {Localization} and {Mapping} {Based} on {Signed} {Distance} {Functions}},
  journal  = {Journal of Intelligent \& Robotic Systems},
  year     = {2016},
  language = {en},
  volume   = {83},
  number   = {3},
  month    = sep,
  pages    = {409--428},
  issn     = {1573-0409},
  doi      = {10.1007/s10846-016-0375-7},
  urldate  = {2018-10-04},
  abstract = {This publication describes a 2D Simultaneous Localization and Mapping approach applicable to multiple mobile robots. The presented strategy uses data of 2D LIDAR sensors to build a dynamic representation based on Signed Distance Functions. Novelties of the approach are a joint map built in parallel instead of occasional merging of smaller maps and the limited drift localization which requires no loop closure detection. A multi-threaded software architecture performs registration and data integration in parallel allowing for drift-reduced pose estimation of multiple robots. Experiments are provided demonstrating the application with single and multiple robot mapping using simulated data, public accessible recorded data, two actual robots operating in a comparably large area as well as a deployment of these units at the Robocup rescue league.},
  file     = {pdf\:PDF:pdf\:Springer Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/92H86S2R/Koch et al. - 2016 - Multi-Robot Localization and Mapping Based on Sign.pdf/\:application/pdf\:PDF:PDF},
  keywords = {SLAM, Mobile robotics, Multi-robot, Rescue robotics},
}

@Article{jung_development_2015,
  author    = {Jung, Jaehoon and Yoon, Sanghyun and Ju, Sungha and Heo, Joon and Jung, Jaehoon and Yoon, Sanghyun and Ju, Sungha and Heo, Joon},
  title     = {Development of {Kinematic} 3D {Laser} {Scanning} {System} for {Indoor} {Mapping} and {As}-{Built} {BIM} {Using} {Constrained} {SLAM}},
  journal   = {Sensors},
  year      = {2015},
  language  = {en},
  volume    = {15},
  number    = {10},
  month     = oct,
  pages     = {26430--26456},
  doi       = {10.3390/s151026430},
  url       = {https://www.mdpi.com/1424-8220/15/10/26430},
  urldate   = {2018-10-04},
  abstract  = {The growing interest and use of indoor mapping is driving a demand for improved data-acquisition facility, efficiency and productivity in the era of the Building Information Model (BIM). The conventional static laser scanning method suffers from some limitations on its operability in complex indoor environments, due to the presence of occlusions. Full scanning of indoor spaces without loss of information requires that surveyors change the scanner position many times, which incurs extra work for registration of each scanned point cloud. Alternatively, a kinematic 3D laser scanning system, proposed herein, uses line-feature-based Simultaneous Localization and Mapping (SLAM) technique for continuous mapping. Moreover, to reduce the uncertainty of line-feature extraction, we incorporated constrained adjustment based on an assumption made with respect to typical indoor environments: that the main structures are formed of parallel or orthogonal line features. The superiority of the proposed constrained adjustment is its reduction for uncertainties of the adjusted lines, leading to successful data association process. In the present study, kinematic scanning with and without constrained adjustment were comparatively evaluated in two test sites, and the results confirmed the effectiveness of the proposed system. The accuracy of the 3D mapping result was additionally evaluated by comparison with the reference points acquired by a total station: the Euclidean average distance error was 0.034 m for the seminar room and 0.043 m for the corridor, which satisfied the error tolerance for point cloud acquisition (0.051 m) according to the guidelines of the General Services Administration for BIM accuracy.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file      = {pdf\:PDF:pdf\:Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/YS7AYW2L/Jung et al. - 2015 - Development of Kinematic 3D Laser Scanning System .pdf/\:application/pdf\:PDF:PDF},
  keywords  = {SLAM, constrained least squares adjustment, laser scanner, line feature, point clouds},
}

@inproceedings{abdulgalil_multi-robot_2019,
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Multi-robot {SLAM}: {An} {Overview} and {Quantitative} {Evaluation} of {MRGS} {ROS} {Framework} for {MR}-{SLAM}},
	isbn = {978-3-319-78452-6},
	shorttitle = {Multi-robot {SLAM}},
	abstract = {In recent years, multi-robot systems (MRS) have received attention from researchers in academia, government laboratories and industry. This research activity has borne fruit in tackling some of the challenging problems that are still open. One is multi-robot simultaneous localization and mapping (MR-SLAM). This paper provides an overview of the latest trends in tackling the problem of (MR-SLAM) focusing on Robot Operating System (ROS) enabled package designed to solve this problem through enabling the robots to share their maps and merge them over a WiFi network. This package had some out-of-date dependencies and worked with some packages that no longer exist. The package has been modified to handle these dependencies. The C-SLAM package was then tested with 2 robots using Gmapping and Hector SLAM packages. Quantitative metrics were used to evaluate the accuracy of the generated maps by comparing it to the ground truth map, including Map Score and Occupied/Free cells ratio.},
	language = {en},
	booktitle = {Robot {Intelligence} {Technology} and {Applications} 5},
	publisher = {Springer International Publishing},
	author = {Abdulgalil, Mahmoud A. and Nasr, Mahmoud M. and Elalfy, Mohamed H. and Khamis, Alaa and Karray, Fakhri},
	editor = {Kim, Jong-Hwan and Myung, Hyun and Kim, Junmo and Xu, Weiliang and Matson, Eric T and Jung, Jin-Woo and Choi, Han-Lim},
	year = {2019},
	keywords = {SLAM, ROS, Data association, C-SLAM, Map merging, Multi-robot SLAM, Occupancy grid map},
	pages = {165--183}
}

@Article{saeedi_multiple-robot_2016,
  author     = {Saeedi, Sajad and Trentini, Michael and Seto, Mae and Li, Howard},
  title      = {Multiple-{Robot} {Simultaneous} {Localization} and {Mapping}: {A} {Review}},
  journal    = {Journal of Field Robotics},
  year       = {2016},
  language   = {en},
  volume     = {33},
  number     = {1},
  month      = jan,
  pages      = {3--46},
  issn       = {1556-4967},
  doi        = {10.1002/rob.21620},
  urldate    = {2018-10-04},
  abstract   = {Simultaneous localization and mapping (SLAM) in unknown GPS-denied environments is a major challenge for researchers in the field of mobile robotics. Many solutions for single-robot SLAM exist; however, moving to a platform of multiple robots adds many challenges to the existing problems. This paper reviews state-of-the-art multiple-robot systems, with a major focus on multiple-robot SLAM. Various issues and problems in multiple-robot SLAM are introduced, current solutions for these problems are reviewed, and their advantages and disadvantages are discussed.},
  copyright  = {© 2015 Wiley Periodicals, Inc.},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/IMBH3D9S/rob.html/\:text/html\:PDF:PDF},
  shorttitle = {Multiple-{Robot} {Simultaneous} {Localization} and {Mapping}},
}

@Article{best_dec-mcts:_2018,
  author    = {Graeme Best and Oliver M Cliff and Timothy Patten and Ramgopal R Mettu and Robert Fitch},
  title     = {Dec-MCTS: Decentralized planning for multi-robot active perception},
  journal   = {The International Journal of Robotics Research},
  year      = {2019},
  volume    = {38},
  number    = {2-3},
  month     = mar,
  pages     = {316--337},
  doi       = {10.1177/0278364918755924},
  eprint    = {https://doi.org/10.1177/0278364918755924},
  abstract  = { We propose a decentralized variant of Monte Carlo tree search (MCTS) that is suitable for a variety of tasks in multi-robot active perception. Our algorithm allows each robot to optimize its own actions by maintaining a probability distribution over plans in the joint-action space. Robots periodically communicate a compressed form of their search trees, which are used to update the joint distribution using a distributed optimization approach inspired by variational methods. Our method admits any objective function defined over robot action sequences, assumes intermittent communication, is anytime, and is suitable for online replanning. Our algorithm features a new MCTS tree expansion policy that is designed for our planning scenario. We extend the theoretical analysis of standard MCTS to provide guarantees for convergence rates to the optimal payoff sequence. We evaluate the performance of our method for generalized team orienteering and online active object recognition using real data, and show that it compares favorably to centralized MCTS even with severely degraded communication. These examples demonstrate the suitability of our algorithm for real-world active perception with multiple robots. },
  publisher = {{SAGE} Publications},
}

@Article{garzon_multirobot_2016,
  author    = {Garzón, Mario and Valente, João and Roldán, Juan Jesús and Cancar, Leandro and Barrientos, Antonio and Cerro, Jaime Del},
  title     = {A {Multirobot} {System} for {Distributed} {Area} {Coverage} and {Signal} {Searching} in {Large} {Outdoor} {Scenarios}*},
  journal   = {Journal of Field Robotics},
  year      = {2016},
  language  = {en},
  volume    = {33},
  number    = {8},
  month     = dec,
  pages     = {1087--1106},
  issn      = {1556-4967},
  doi       = {10.1002/rob.21636},
  urldate   = {2018-10-04},
  abstract  = {This work presents a complete multirobot solution for signal searching tasks in large outdoor scenarios. An evaluation of two different coverage path-planning strategies according to field size and shape is presented. A signal location system developed to simulate mines or chemical source detections is also described. The solution presented is a pioneer in evaluating multimaster robotics operative system architectures with a fleet of robots in real scenarios. This solution minimizes the use of communications bandwidth required for full operation. Finally, field results are provided, and the advantages of the implemented solution are analyzed.},
  copyright = {© 2015 Wiley Periodicals, Inc.},
  file      = {html\:PDF:html\:Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/LXY3MQXR/Garzón et al. - 2016 - A Multirobot System for Distributed Area Coverage .pdf/\:application/pdf/\;Snapshot/\:C//\:/Users/Psykie/Zotero/storage/QY2RQKDN/rob.html/\:text/html\:PDF:PDF},
}

@Article{luft_recursive_2018,
  author   = {Luft, Lukas and Schubert, Tobias and Roumeliotis, Stergios I. and Burgard, Wolfram},
  title    = {Recursive decentralized localization for multi-robot systems with asynchronous pairwise communication},
  journal  = {The International Journal of Robotics Research},
  year     = {2018},
  language = {en},
  month    = mar,
  pages    = {0278364918760698},
  issn     = {0278-3649},
  doi      = {10.1177/0278364918760698},
  urldate  = {2018-10-04},
  abstract = {This paper provides a fully decentralized algorithm for collaborative localization based on the extended Kalman filter. The major challenge in decentralized collaborative localization is to track inter-robot dependencies, which is particularly difficult when sustained synchronous communication between the robots cannot be guaranteed. Current approaches suffer from the need for particular communication schemes, extensive bookkeeping of measurements, overly conservative assumptions, or the restriction to specific measurement models. This paper introduces a localization algorithm that is able to approximate the inter-robot correlations while fulfilling all of the following conditions: communication is limited to two robots that obtain a relative measurement, the algorithm is recursive in the sense that it does not require storage of measurements and each robot maintains only the latest estimate of its own pose, and it supports generic measurement models. The fact that the proposed approach can handle these particularly difficult conditions ensures that it is applicable to a wide range of multi-robot scenarios. We provide mathematical details on our approximation. Extensive experiments carried out using real-world datasets demonstrate the improved performance of our method compared with several existing approaches.},
  file     = {pdf\:PDF:pdf\:SAGE PDF Full Text/\:C//\:/Users/Psykie/Zotero/storage/2V4I4PAQ/Luft et al. - 2018 - Recursive decentralized localization for multi-rob.pdf/\:application/pdf\:PDF:PDF},
}

@InProceedings{ortiz_task_2005,
  author    = {Ortiz, Charles L. and Vincent, Régis and Morisset, Benoit},
  title     = {Task {Inference} and {Distributed} {Task} {Management} in the {Centibots} {Robotic} {System}},
  booktitle = {Proceedings of the {Fourth} {International} {Joint} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
  year      = {2005},
  series    = {{AAMAS} '05},
  publisher = {ACM},
  isbn      = {978-1-59593-093-4},
  pages     = {860--867},
  doi       = {10.1145/1082473.1082604},
  urldate   = {2018-10-04},
  abstract  = {We describe the Centibots system, a very large scale distributed robotic system, consisting of more than 100 robots, that has been successfully deployed in large, unknown indoor environments, over extended periods of time (i.e., durations corresponding to several power cycles). Unlike most multiagent systems, the set of tasks about which teams must collaborate is not given a priori. We first describe a task inference algorithm that identifies potential team commitments that collectively balance constraints such as reachability, sensor coverage, and communication access. We then describe a dispatch algorithm for task distribution and management that assigns resources depending on either task density or replacement requirements stemming from failures or power shortages. The targeted deployment environments are expected to lack a supporting communication infrastructure; robots manage their own network and reason about the concomitant localization constraints necessary to maintain team communication. Finally, we present quantitative results in terms of a "search and rescue problem" and discuss the team-oriented aspects of the system in the context of prevailing theories of multiagent collaboration.},
  address   = {New York, NY, USA},
  keywords  = {autonomous robots and robot teams, mobile agents, teamwork},
}

@Article{saqib_smart_2017,
  author   = {Saqib, Mohd. and Hussain, Md. Muzakkir and Alam, Mohammad Saad and Beg, M. M. Sufyan and Sawant, Amol},
  title    = {Smart {Electric} {Vehicle} {Charging} {Through} {Cloud} {Monitoring} and {Management}},
  journal  = {Technology and Economics of Smart Grids and Sustainable Energy},
  year     = {2017},
  language = {en},
  volume   = {2},
  number   = {1},
  month    = oct,
  pages    = {18},
  issn     = {2199-4706},
  doi      = {10.1007/s40866-017-0035-4},
  urldate  = {2018-09-21},
  abstract = {Smart charging system of electric vehicle using cloud based monitoring and management is demonstrated in this work. xEVs (electric plugin hybrid, battery electric vehicles) Charging Management System is crucial for the dynamic demands of charging infrastructure, namely perspectives from automakers, electricity providers, vehicle owners and charging service providers. Through dedicated interface, the developed system is capable of providing real time information to xEVs users regarding nearest charging station with minimum queuing delay, with minimum charging cost through a secured online accessing mechanism for accessing Sate of the Charge (SOC) of the xEV’s battery being charged. The system not only provide an execution framework for the xEVs users but also provide an optimal energy trading solution to all entities involved in a smart charging infrastructure such as charging station, aggregators, smart grid etc. The work also explains the cloud enabled bidding strategies that look for day-ahead and term-ahead markets. The aggregators will use the smart decisions undertaken by cloud analytics to execute their bidding strategies in way to maximize the profit. Further, the work also assesses the possible cyber security aspects of such architectures along with providing possible solutions.},
  file     = {pdf\:PDF:pdf\:Springer Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/YNCMFS9T/Saqib et al. - 2017 - Smart Electric Vehicle Charging Through Cloud Moni.pdf/\:application/pdf\:PDF:PDF},
  keywords = {Cloud computing, Cloud management, Electric Vehicles (EVs), Power market operation, Smart grid},
}

@Article{rodrigues_third-party_2018,
  author   = {Rodrigues, Andreia P.M. and Pimentel, Carina M.O. and Matias, João C.O.},
  title    = {Third-party logistics continuous improvement through integrated fleet management: the case of a transport company},
  journal  = {International Journal of Logistics Systems and Management},
  year     = {2018},
  volume   = {31},
  number   = {1},
  month    = jan,
  pages    = {39--68},
  issn     = {1742-7967},
  doi      = {10.1504/IJLSM.2018.094190},
  urldate  = {2018-09-21},
  abstract = {In the transport sector, there is a gap that arises in managing fleets with vehicles of different brands, making it ineffective to choose the use of telematics system that each brand offers. This article presents a solution based on a system that uses a single service which is aggregating all fleet of vehicles, irrespective of brand, and allowing their control any time. Thus, it results in improved efficiency on the routes based on a greater consistency of data as similar driving data are obtained for all vehicles. This article uses a case study that shows possibility to reduce travelling costs, allowing the generation of the not greater quality services and hence improving customer satisfaction.},
  annote   = {doi: 10.1504/IJLSM.2018.094190},
}

@Article{touati_investigation_2016,
  author   = {Touati, Farid and Al-Hitmi, M. A. and Chowdhury, Noor Alam and Hamad, Jehan Abu and San Pedro Gonzales, Antonio J. R.},
  title    = {Investigation of solar {PV} performance under {Doha} weather using a customized measurement and monitoring system},
  journal  = {Renewable Energy},
  year     = {2016},
  volume   = {89},
  month    = apr,
  pages    = {564--577},
  issn     = {0960-1481},
  doi      = {10.1016/j.renene.2015.12.046},
  url      = {http://www.sciencedirect.com/science/article/pii/S0960148115305504},
  urldate  = {2018-09-20},
  abstract = {A key factor in future eco-friendly and reliable energy systems for network stability is energy prediction of renewable resources. However, the climate-induced performance degradation represents a major drawback to solar making them unreliable for unattended or remote plants. We propose a customized cost-effective solution that allows monitoring and predicting the performance of PV systems, fostering thus efficient planning and control strategies for network stability. More specifically, we design an in-house system featuring a buck–boost converter enhanced with a maximum power point tracker (MPPT) serving as a PV load, sensors' system that collects climatic parameters including dust, wireless Radio, and a LabVIEW based monitoring and recording station. We have presented thoroughly all the system design steps. The system was deployed in Doha over a long period of time under harsh environment. It allows accessing and processing of recorded data ubiquitously over the web or from a database. Based on the recorded data, the power output for a given PV technology under any environment can be analyzed. Results predict that the maximum power output from a Poly-crystalline panel decreases by around 30\% for a dust exposure of five months. This would suggest cleaning needs and advise on cleaning frequencies of PV panels.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/JT9KPZ8K/Touati et al. - 2016 - Investigation of solar PV performance under Doha w.pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/AIB53ZJF/S0960148115305504.html/\:text/html\:PDF:PDF},
  keywords = {Effect of harsh environment, P–V and I–V curves, PV efficiency, PV performance prediction, PV soiling, Wireless monitoring},
}

@inproceedings{manghani_internet_2018,
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {An {Internet} of {Things} to {Maximum} {Power} {Point} {Tracking} {Approach} of {Solar} {PV} {Array}},
	isbn = {978-981-10-4286-7},
	abstract = {Internet of Things (IoT) is one of the emerging trends and has great potential to bring advances in electrical engineering and renewable energy. It is estimated that by 2020, IoT will be a \$20 trillion industry. Hence, this paper proposes the method of incorporating IoT in solar energy and photovoltaic research at the laboratory level. The proposed method includes IoT’s divine application in (a) MPPT tracking; (b) partial shading; (c) security switch for turning ON Arduino’s serial monitor; and finally, (d) building up the direct communication between professor (sitting in the cabin) and the research scholar performing experiment (in the laboratory which is at some particular distance from the professor’s cabin). The whole setup is based on the newly developed Indian microcontroller-BOLT (An Internet of Things platform) launched on 21 February 2016.},
	language = {en},
	booktitle = {Advances in {Smart} {Grid} and {Renewable} {Energy}},
	publisher = {Springer Singapore},
	author = {Manghani, Himanshu and Prasanth Ram, J. and Rajasekar, N.},
	editor = {SenGupta, Sabyasachi and Zobaa, Ahmed F. and Sherpa, Karma Sonam and Bhoi, Akash Kumar},
	year = {2018},
	keywords = {Internet of Things, Maximum power point tracking, Partial shading},
	pages = {401--409}
}

@Article{shaukat_survey_2018,
  author   = {Shaukat, N. and Khan, B. and Ali, S. M. and Mehmood, C. A. and Khan, J. and Farid, U. and Majid, M. and Anwar, S. M. and Jawad, M. and Ullah, Z.},
  title    = {A survey on electric vehicle transportation within smart grid system},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2018},
  volume   = {81},
  month    = jan,
  pages    = {1329--1349},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2017.05.092},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032117307190},
  urldate  = {2018-09-20},
  abstract = {The electrification of hybrid electric vehicle reduces the reliance of transportation on fossil fuels and reduces Green House Gas emissions. The economic and environmental benefits of the hybrid electric vehicles are greatly reshaping the modern transportation sector. The transportation electrification (TE) brings various challenges to the Smart Grid (SG), such as power quality, reliability, and control. Thus, there is a need to explore and reveal the key enabling technologies for TE. Moreover, the intermittent nature of Renewable Energy Resources (RER) based generation demands for efficient, reliable, flexible, dynamic, and distributed energy storage technologies. The Electrical Vehicles (EVs) storage battery is the promising solution in accommodating RER based generation within SG. The most efficient feature of transportation sector is Vehicle to Grid (V2G) concept that will help in storing the surplus energy and feeding back this energy to the main grid during period of high demands. The storage technology is an integral part of the SG that helps in attaining the proper utilization of RER. In this paper, our goal is to explore the TE sector and its impact on economy, reliability and eco-friendly system. We reviewed the V2G technology and their implementation challenges. We further reviewed various energy storage technologies deployed in EVs within SG, considering attention to their influence on the environment. Moreover, this paper presented a detailed overview of the on board and off board charging infrastructure and communication necessities for EV. The paper also investigated the current issues and challenges of energy storage technologies in EVs. The technical and economic benefits of storage technologies are also considered. Our analysis reviews the role of EVs in decarbonizing the atmosphere. Lastly, the survey explains the current regulation, Standard, and interfacing issues within SG.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/UU6EHFFU/Shaukat et al. - 2018 - A survey on electric vehicle transportation within.pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/HIXJFWB6/S1364032117307190.html/\:text/html\:PDF:PDF},
  keywords = {Charging infrastructure, Electrical vehicles, Energy storage technology, Global warming, Hybrid electrical vehicles, Plug in hybrid electric vehicles, Smart Grid, Vehicle-to-Grid (V2G)},
}

@Article{zhou_incentive-based_2018,
  author   = {Yaqin Zhou and Rajnish Kumar and Shaojie Tang},
  title    = {Incentive-based distributed scheduling of {Electric} {Vehicle} charging under uncertainty},
  journal  = {IEEE Transactions on Power Systems},
  year     = {2019},
  volume   = {34},
  number   = {1},
  month    = jan,
  pages    = {3--11},
  issn     = {0885-8950},
  doi      = {10.1109/TPWRS.2018.2868501},
  abstract = {We consider a power system with\$N\$charging stations, each of which has local renewable generation and electric vehicle (EV) arrivals with deadline constraints for their requests. To reduce the adverse impact brought by uncoordinated charging, the utility company requires to adopt advanced load management to control the load supplied to stations but without collecting many details. The major challenge is to incentivize them to coordinate charging for its system-wide optimization while meeting all charging requests, regarding inherent uncertainties. We model a game that aims to minimize the total electricity price at the utility company meanwhile maximize the payoff of each station. We combineϵ-Nash equilibrium and Lyapunov optimization to design a low-complexity distributed scheduling scheme of EV charging. It achieves at most\$O(1/V)\$more than the optimal cost where\$V\$is a controllable parameter that balances between the cost and average fulfillment ratio of charging requests. The payoff of each station is around\$O(1/V)\$less than the optimal payoff if using a small\$V\$. Simulation results under real-world traces show that, compared with a state-of-the-art scheduling algorithm, our algorithm reduces the peak-to-average ratio and charging cost respectively by\$94.32\%\$and\$44.21\%\$, and increases the payoff by\$52.27\%\$averagely},
  file     = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/HW34FUWE/8454323.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/V923D8PT/Zhou et al. - 2018 - Incentive-based distributed scheduling of Electric.pdf/\:application/pdf\:PDF:PDF},
  keywords = {Optimization, Charging stations, Companies, distributed scheduling, Electric vehicle charging, EV charging, Game theory, Games, Lyapunov optimization, Renewable energy sources},
}

@Article{zhang_factors_2018,
  author   = {Zhang, Qi and Li, Hailong and Zhu, Lijing and Campana, Pietro Elia and Lu, Huihui and Wallin, Fredrik and Sun, Qie},
  title    = {Factors influencing the economics of public charging infrastructures for {EV} – {A} review},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2018},
  volume   = {94},
  month    = oct,
  pages    = {500--509},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2018.06.022},
  url      = {http://www.sciencedirect.com/science/article/pii/S136403211830460X},
  urldate  = {2018-09-20},
  abstract = {Growing concerns about energy conservation and the environmental impacts of greenhouse gas emissions over the world have promoted the development of the electric vehicles (EVs) market. However, one of the biggest barriers in the development of the EV market is the lack of the public charging infrastructure. This paper reviews the factors that can directly and indirectly influence the economics of the public charging infrastructure. The knowledge gaps, barriers and opportunities in the development of the charging infrastructure have been identified and analyzed. In order to promote the development of the public charging infrastructure, more research efforts should be paid on the impacts of psychological factors of customers and the technical development of charging infrastructures and EV batteries. The government support has been proved to play an important role, so that how the government policy can be tailored for the development of the charging infrastructure market should receive more attentions. In addition, the charging price as an endogenous factor should be considered more carefully in modelling the charging infrastructure market. New business models are also urgently needed to accelerate the future development of the public charging infrastructure.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/GDIFGDGW/Zhang et al. - 2018 - Factors influencing the economics of public chargi.pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/LG36V8SN/S136403211830460X.html/\:text/html\:PDF:PDF},
  keywords = {Public charging infrastructure, Electric vehicle (EV), Economics, Influencing factor, Investment},
}

@Article{amjad_review_2018,
  author     = {Amjad, Muhammad and Ahmad, Ayaz and Rehmani, Mubashir Husain and Umer, Tariq},
  title      = {A review of {EVs} charging: {From} the perspective of energy optimization, optimization approaches, and charging techniques},
  journal    = {Transportation Research Part D: Transport and Environment},
  year       = {2018},
  volume     = {62},
  month      = jul,
  pages      = {386--417},
  issn       = {1361-9209},
  doi        = {10.1016/j.trd.2018.03.006},
  url        = {http://www.sciencedirect.com/science/article/pii/S1361920917306120},
  urldate    = {2018-09-19},
  abstract   = {Introduction of electric vehicles (EVs) or plug-in electric vehicles (PEVs) in the road transportation can significantly reduce the carbon emission. Hence, the demand of EVs is likely to increase in the near future. Large penetration of EVs will also ultimately result into high loads on the existing power grids. The controlled charging of EVs can have a significant impact on the power grid load, voltage, frequency, and power losses. In this paper, we have provided a comprehensive review of various energy optimization approaches used for EVs charging. Energy optimization approaches used for EVs not only enhance the battery life but also contribute in regulating the voltage and frequency. During EVs charging, various objective functions such as supporting the renewable energy sources, minimization of the peak load, energy cost, and maximization of the aggregator profit have also been studied from optimization perspectives. The controlled and an optimized EVs charging enhances the performance of EVs batteries and conserves the energy in the system by minimizing the load and power losses. The different EVs charging approaches such as centralized and distributed suited for different objective functions have also been studied and compared with respect to various optimization approaches.},
  file       = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/GV82NJ3X/Amjad et al. - 2018 - A review of EVs charging From the perspective of .pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/XIJXS3S9/S1361920917306120.html/\:text/html\:PDF:PDF},
  keywords   = {EV charging, Battery life, Centralized EVs charging, Distributed EVs charging, Optimization methods, Optimization objectives},
  shorttitle = {A review of {EVs} charging},
}

@Article{amadeo_information-centric_2016,
  author     = {Amadeo, M. and Campolo, C. and Molinaro, A.},
  title      = {Information-centric networking for connected vehicles: a survey and future perspectives},
  journal    = {IEEE Communications Magazine},
  year       = {2016},
  volume     = {54},
  number     = {2},
  month      = feb,
  pages      = {98--104},
  issn       = {0163-6804},
  doi        = {10.1109/MCOM.2016.7402268},
  abstract   = {In the connected vehicle ecosystem, a high volume of information-rich and safety-critical data will be exchanged by roadside units and onboard transceivers to improve the driving and traveling experience. However, poor-quality wireless links and the mobility of vehicles highly challenge data delivery. The IP address-centric model of the current Internet barely works in such extremely dynamic environments and poorly matches the localized nature of the majority of vehicular communications, which typically target specific road areas (e.g., in the proximity of a hazard or a point of interest) regardless of the identity/address of a single vehicle passing by. Therefore, a paradigm shift is advocated from traditional IP-based networking toward the groundbreaking information- centric networking. In this article, we scrutinize the applicability of this paradigm in vehicular environments by reviewing its core functionalities and the related work. The analysis shows that, thanks to features like named content retrieval, innate multicast support, and in-network data caching, information-centric networking is positioned to meet the challenging demands of vehicular networks and their evolution. Interoperability with the standard architectures for vehicular applications along with synergies with emerging computing and networking paradigms are debated as future research perspectives.},
  file       = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/M6E746Y2/7402268.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/F6M567T6/Amadeo et al. - 2016 - Information-centric networking for connected vehic.pdf/\:application/pdf\:PDF:PDF},
  keywords   = {Vehicle dynamics, Safety, Intelligent vehicles, Automotive electronics, connected vehicle ecosystem, Connected vehicles, content retrieval, in-network data caching, information-centric networking, innate multicast support, Internet, IP address-centric model, IP networks, Network security, onboard transceivers, roadside units, vehicular ad hoc networks, vehicular communications},
  shorttitle = {Information-centric networking for connected vehicles},
}

@Article{siegel_survey_2018,
  author   = {Siegel, J. E. and Erb, D. C. and Sarma, S. E.},
  title    = {A {Survey} of the {Connected} {Vehicle} {Landscape}—{Architectures}, {Enabling} {Technologies}, {Applications}, and {Development} {Areas}},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2018},
  volume   = {19},
  number   = {8},
  month    = aug,
  pages    = {2391--2406},
  issn     = {1524-9050},
  doi      = {10.1109/TITS.2017.2749459},
  abstract = {This paper summarizes the state of the art in connected vehicles-from the need for vehicle data and applications thereof, to enabling technologies, challenges, and identified opportunities. Connectivity is increasing around the world and its expansion to vehicles is no exception. With improvements in connectivity, sensing, and computation, the future will see vehicles used as development platforms capable of generating rich data, acting based on inference, and effecting great change in transportation, the human-vehicle dynamic, the environment, and the economy. Connected vehicle technologies have already been used to improve fleet safety and efficiency, with emerging technologies additionally allowing data to be used to inform aspects of vehicle design, ownership, and use. While the demand for connected vehicles and its enabling technology has progressed significantly in recent years, there remain challenges to connected and collaborative vehicle application deployment before the full potential of connected cars may be realized. From extensibility and scalability to privacy and security, this paper informs the reader about key enabling technologies, opportunities, and challenges in the connected vehicle landscape.},
  file     = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/K6PA6Q28/8058008.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/AAK5GZ6Z/Siegel et al. - 2018 - A Survey of the Connected Vehicle Landscape—Archit.pdf/\:application/pdf\:PDF:PDF},
  keywords = {Automobiles, traffic engineering computing, Standards, road safety, automobiles, intelligent transportation systems, automotive electronics, Safety, automotive engineering, automotive applications, Connected vehicles, collaborative vehicle application deployment, connected cars, connected vehicle application deployment, controller area network, data privacy, fleet safety, human-vehicle dynamic, Intelligent sensors, privacy, road transportation, security, security of data, telematics, V2B, V2I, V2R, V2V, V2X, vehicle data, vehicle design, vehicle dynamics, vehicle-to-everything, vehicle-to-infrastructure, vehicle-to-roadside, vehicle-to-vehicle, vehicular ad-hoc networks},
}

@Patent{rybak_systems_2017,
  author      = {Rybak, Ihor Bohdan and Wood, Daniel and Murray, Judson and Janes, Andy and Pichette, Matthew},
  title       = {Systems and methods for extraction and telemetry of vehicle operational data from an internal automotive network},
  number      = {US9659417B2},
  year        = {2017},
  month       = may,
  url         = {https://patents.google.com/patent/US9659417B2/en},
  urldate     = {2018-09-19},
  abstract    = {Systems, methods, and related computer programs are provided wherein vehicle operation data is extracted from an internal automotive network. In an embodiment, a method comprises: i) obtaining data available on the internal automotive network via iterative interrogation; ii) analyzing the obtained data to identify a set of candidate data values having at least one common feature within a suitable proximity margin; and iii) heuristically selecting a candidate data value best matching one or more selection criteria to identify a true value. These systems and methods allow data to be extracted from proprietary and non-proprietary busses in the internal automotive network.},
  annote      = {Classifications G07C5/0858: Registering performance data using electronic data carriers wherein the data carrier is removable G07C5/085: Registering performance data using electronic data carriers},
  assignee    = {VINVOX INTERNATIONAL CORP.},
  file        = {pdf\:PDF:pdf\:Fulltext PDF/\:C//\:/Users/Psykie/Zotero/storage/BMYKY2M5/Rybak et al. - 2017 - Systems and methods for extraction and telemetry o.pdf/\:application/pdf\:PDF:PDF},
  keywords    = {vehicle, data, device, internal network, network bus},
  language    = {en},
  nationality = {US},
}

@InProceedings{chung_design_2013,
  author    = {Chung, C. and Shepelev, A. and Qiu, C. and Chu, C. and Gadh, R.},
  title     = {Design of {RFID} mesh network for {Electric} {Vehicle} smart charging infrastructure},
  booktitle = {2013 {IEEE} {International} {Conference} on {RFID}-{Technologies} and {Applications} ({RFID}-{TA})},
  year      = {2013},
  month     = sep,
  pages     = {1--6},
  doi       = {10.1109/RFID-TA.2013.6694512},
  abstract  = {With an increased number of Electric Vehicles (EVs) on the roads, charging infrastructure is gaining an ever-more important role in simultaneously meeting the needs of the local distribution grid and of EV users. This paper proposes a mesh network RFID system for user identification and charging authorization as part of a smart charging infrastructure providing charge monitoring and control. The Zigbee-based mesh network RFID provides a cost-efficient solution to identify and authorize vehicles for charging and would allow EV charging to be conducted effectively while observing grid constraints and meeting the needs of EV drivers.},
  file      = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/G8KXXGQU/6694512.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/XB4IH9PW/Chung et al. - 2013 - Design of RFID mesh network for Electric Vehicle s.pdf/\:application/pdf\:PDF:PDF},
  keywords  = {electric vehicles, Databases, Monitoring, Authorization, charging authorization, electric vehicle smart charging infrastructure, Electrical vehicle charging, EV users, IEEE 802.11 Standards, Logic gates, mesh network RFID system, power distribution control, radiofrequency identification, RFID, Servers, smart grids, user identification, Wireless LAN, wireless mesh network, Zigbee},
}

@InProceedings{desai_internet_2017,
  author    = {Desai, M. and Phadke, A.},
  title     = {Internet of {Things} based vehicle monitoring system},
  booktitle = {2017 {Fourteenth} {International} {Conference} on {Wireless} and {Optical} {Communications} {Networks} ({WOCN})},
  year      = {2017},
  month     = feb,
  pages     = {1--3},
  doi       = {10.1109/WOCN.2017.8065840},
  abstract  = {Advances in technologies and availability of economical open source hardware systems are setting a new trend in system designing. Use of technologies like Internet of Things (IoT) can ease the process of data collection and analysis. The main objective of the paper is to describe a system which can monitor or track the location and vehicle parameters of different test vehicles from a centralized place for research and development purposes and to store data of testing parameters of those vehicles on the server for further analysis and records. System design will be generalized for monitoring different parameters like Location, Vehicle speed, Engine compartment temperature, Fuel consumption and many more. Proposed system uses Open source controller and GPS/GSM/GPRS module for data transfer application.},
  file      = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/W4IIBZ6B/8065840.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/4TC8KBPF/Desai and Phadke - 2017 - Internet of Things based vehicle monitoring system.pdf/\:application/pdf\:PDF:PDF},
  keywords  = {road vehicles, intelligent transportation systems, Global Positioning System, Research and development, Monitoring, Testing, Internet of Things, Servers, Arduino, cellular radio, computerised monitoring, data collection, data transfer application, economical open source hardware systems, Engines, GPS-GSM-GPRS module, GPS/GSM/GPRS technology, IoT, microcontrollers, Open source controller, packet radio networks, Vehicle Monitoring, vehicle monitoring system},
}

@inproceedings{calonder_brief:_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{BRIEF}: {Binary} {Robust} {Independent} {Elementary} {Features}},
	isbn = {978-3-642-15561-1},
	shorttitle = {{BRIEF}},
	abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L 2 norm as is usually done.As a result, BRIEF is very fast both to build and to match. We compare it against SURF and U-SURF on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	year = {2010},
	pages = {778--792}
}

@Article{saputra_visual_2018,
  author     = {Saputra, Muhamad Risqi U. and Markham, Andrew and Trigoni, Niki},
  title      = {Visual {SLAM} and {Structure} from {Motion} in {Dynamic} {Environments}: {A} {Survey}},
  journal    = {ACM Computing Surveys},
  year       = {2018},
  volume     = {51},
  number     = {2},
  month      = feb,
  pages      = {37:1--37:36},
  issn       = {0360-0300},
  doi        = {10.1145/3177853},
  urldate    = {2018-09-12},
  abstract   = {In the last few decades, Structure from Motion (SfM) and visual Simultaneous Localization and Mapping (visual SLAM) techniques have gained significant interest from both the computer vision and robotic communities. Many variants of these techniques have started to make an impact in a wide range of applications, including robot navigation and augmented reality. However, despite some remarkable results in these areas, most SfM and visual SLAM techniques operate based on the assumption that the observed environment is static. However, when faced with moving objects, overall system accuracy can be jeopardized. In this article, we present for the first time a survey of visual SLAM and SfM techniques that are targeted toward operation in dynamic environments. We identify three main problems: how to perform reconstruction (robust visual SLAM), how to segment and track dynamic objects, and how to achieve joint motion segmentation and reconstruction. Based on this categorization, we provide a comprehensive taxonomy of existing approaches. Finally, the advantages and disadvantages of each solution class are critically discussed from the perspective of practicality and robustness.},
  file       = {pdf\:PDF:pdf\:ACM Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/8EGFZJ49/Saputra et al. - 2018 - Visual SLAM and Structure from Motion in Dynamic E.pdf/\:application/pdf\:PDF:PDF},
  keywords   = {3D reconstruction, visual odometry, visual SLAM, deep learning, dynamic environments, 3D tracking, dynamic object segmentation, motion segmentation, Structure from motion},
  shorttitle = {Visual {SLAM} and {Structure} from {Motion} in {Dynamic} {Environments}},
}

@Article{bresson_simultaneous_2017,
  author     = {Bresson, G. and Alsayed, Z. and Yu, L. and Glaser, S.},
  title      = {Simultaneous {Localization} and {Mapping}: {A} {Survey} of {Current} {Trends} in {Autonomous} {Driving}},
  journal    = {IEEE Transactions on Intelligent Vehicles},
  year       = {2017},
  volume     = {2},
  number     = {3},
  month      = sep,
  pages      = {194--220},
  issn       = {2379-8904},
  doi        = {10.1109/TIV.2017.2749181},
  abstract   = {In this paper, we propose a survey of the Simultaneous Localization And Mapping (SLAM) field when considering the recent evolution of autonomous driving. The growing interest regarding self-driving cars has given new directions to localization and mapping techniques. In this survey, we give an overview of the different branches of SLAM before going into the details of specific trends that are of interest when considered with autonomous applications in mind. We first present the limits of classical approaches for autonomous driving and discuss the criteria that are essential for this kind of application. We then review the methods where the identified challenges are tackled. We mostly focus on approaches building and reusing long-term maps in various conditions (weather, season, etc.). We also go through the emerging domain of multivehicle SLAM and its link with self-driving cars. We survey the different paradigms of that field (centralized and distributed) and the existing solutions. Finally, we conclude by giving an overview of the various large-scale experiments that have been carried out until now and discuss the remaining challenges and future orientations.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/CAASQK98/8025618.html/\:text/html\:PDF:PDF},
  keywords   = {autonomous driving, mobile robots, SLAM, SLAM (robots), Vehicle driving, automobiles, mapping, Simultaneous localization and mapping, self-driving cars, place recognition, localization, autonomous applications, Autonomous automobiles, Autonomous vehicle, drift, large-scale experiments, Mapping field, Moblie robots, multi-vehicle, multivehicle SLAM, Robot localization, Simultaneous Localization and Mapping, survey},
  shorttitle = {Simultaneous {Localization} and {Mapping}},
}

@Article{bodin_slambench2:_2018,
  author     = {Bodin, Bruno and Wagstaff, Harry and Saeedi, Sajad and Nardi, Luigi and Vespa, Emanuele and Mayer, John H. and Nisbet, Andy and Luján, Mikel and Furber, Steve and Davison, Andrew J. and Kelly, Paul H. J. and O'Boyle, Michael},
  title      = {{SLAMBench}2: {Multi}-{Objective} {Head}-to-{Head} {Benchmarking} for {Visual} {SLAM}},
  journal    = {arXiv:1808.06820 [cs]},
  year       = {2018},
  month      = aug,
  note       = {arXiv: 1808.06820},
  url        = {http://arxiv.org/abs/1808.06820},
  urldate    = {2018-09-12},
  abstract   = {SLAM is becoming a key component of robotics and augmented reality (AR) systems. While a large number of SLAM algorithms have been presented, there has been little effort to unify the interface of such algorithms, or to perform a holistic comparison of their capabilities. This is a problem since different SLAM applications can have different functional and non-functional requirements. For example, a mobile phonebased AR application has a tight energy budget, while a UAV navigation system usually requires high accuracy. SLAMBench2 is a benchmarking framework to evaluate existing and future SLAM systems, both open and close source, over an extensible list of datasets, while using a comparable and clearly specified list of performance metrics. A wide variety of existing SLAM algorithms and datasets is supported, e.g. ElasticFusion, InfiniTAM, ORB-SLAM2, OKVIS, and integrating new ones is straightforward and clearly specified by the framework. SLAMBench2 is a publicly-available software framework which represents a starting point for quantitative, comparable and validatable experimental research to investigate trade-offs across SLAM systems.},
  file       = {html\:PDF:html\:arXiv//\:1808.06820 PDF/\:C//\:/Users/Psykie/Zotero/storage/X966YZNY/Bodin et al. - 2018 - SLAMBench2 Multi-Objective Head-to-Head Benchmark.pdf/\:application/pdf/\;arXiv.org Snapshot/\:C//\:/Users/Psykie/Zotero/storage/2DYZDIBG/1808.html/\:text/html\:PDF:PDF},
  keywords   = {Computer Science - Robotics},
  shorttitle = {{SLAMBench}2},
}

@InProceedings{drage_lidar_2015,
  author    = {Drage, T. and Churack, T. and Bräunl, T.},
  title     = {{LIDAR} {Road} {Edge} {Detection} by {Heuristic} {Evaluation} of {Many} {Linear} {Regressions}},
  booktitle = {2015 {IEEE} 18th {International} {Conference} on {Intelligent} {Transportation} {Systems}},
  year      = {2015},
  month     = sep,
  pages     = {2465--2470},
  doi       = {10.1109/ITSC.2015.397},
  abstract  = {This article presents a novel algorithm for the reliable detection of road edges via multi-layer LIDAR sensing on a moving vehicle for improved safety. Results are presented showing the operation and performance of the system in several outdoor scenarios. The algorithm is then integrated with orientation and position measurements, giving improved reliability and extension to mapping.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/SGZ4SKKL/7313489.html/\:text/html\:PDF:PDF},
  keywords  = {regression analysis, optical radar, autonomous vehicles, computer vision, Conferences, edge detection, heuristic evaluation, Intelligent transportation systems, LIDAR, LIDAR road edge detection, linear regressions, mapping, multilayer LIDAR, optical images, position measurements, reliable detection, telecommunication network reliability},
}

@Article{dissanayake_solution_2001,
  author   = {Dissanayake, M. W. M. G. and Newman, P. and Clark, S. and Durrant-Whyte, H. F. and Csorba, M.},
  title    = {A solution to the simultaneous localization and map building ({SLAM}) problem},
  journal  = {IEEE Transactions on Robotics and Automation},
  year     = {2001},
  volume   = {17},
  number   = {3},
  month    = jun,
  pages    = {229--241},
  issn     = {1042-296X},
  doi      = {10.1109/70.938381},
  abstract = {The simultaneous localization and map building (SLAM) problem asks if it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and then to incrementally build a map of this environment while simultaneously using this map to compute absolute vehicle location. Starting from estimation-theoretic foundations of this problem, the paper proves that a solution to the SLAM problem is indeed possible. The underlying structure of the SLAM problem is first elucidated. A proof that the estimated map converges monotonically to a relative map with zero uncertainty is then developed. It is then shown that the absolute accuracy of the map and the vehicle location reach a lower bound defined only by the initial vehicle uncertainty. Together, these results show that it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and, using relative observations only, incrementally build a perfect map of the world and to compute simultaneously a bounded estimate of vehicle location. The paper also describes a substantial implementation of the SLAM algorithm on a vehicle operating in an outdoor environment using millimeter-wave radar to provide relative map observations. This implementation is used to demonstrate how some key issues such as map management and data association can be handled in a practical environment. The results obtained are cross-compared with absolute locations of the map landmarks obtained by surveying. In conclusion, the paper discusses a number of key issues raised by the solution to the SLAM problem including suboptimal map-building algorithms and map management.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/5KE636S4/938381.html/\:text/html\:PDF:PDF},
  keywords = {mobile robots, autonomous vehicle, path planning, Navigation, Mobile robots, Remotely operated vehicles, Simultaneous localization and mapping, filtering theory, Millimeter wave radar, estimation theory, Uncertainty, state estimation, covariance matrices, absolute accuracy, absolute vehicle location, Environmental management, estimation-theoretic foundations, Helium, map management, Mechatronics, millimeter-wave radar, Motion planning, outdoor environment, perfect map, relative map, simultaneous localization and map building problem, SLAM problem, suboptimal map-building algorithms, unknown environment, unknown location},
}

@Article{aqel_review_2016,
  author     = {Aqel, Mohammad O. A. and Marhaban, Mohammad H. and Saripan, M. Iqbal and Ismail, Napsiah Bt.},
  title      = {Review of visual odometry: types, approaches, challenges, and applications},
  journal    = {SpringerPlus},
  year       = {2016},
  language   = {en},
  volume     = {5},
  number     = {1},
  month      = oct,
  pages      = {1897},
  issn       = {2193-1801},
  doi        = {10.1186/s40064-016-3573-7},
  urldate    = {2018-10-06},
  abstract   = {Accurate localization of a vehicle is a fundamental challenge and one of the most important tasks of mobile robots. For autonomous navigation, motion tracking, and obstacle detection and avoidance, a robot must maintain knowledge of its position over time. Vision-based odometry is a robust technique utilized for this purpose. It allows a vehicle to localize itself robustly by using only a stream of images captured by a camera attached to the vehicle. This paper presents a review of state-of-the-art visual odometry (VO) and its types, approaches, applications, and challenges. VO is compared with the most common localization sensors and techniques, such as inertial navigation systems, global positioning systems, and laser sensors. Several areas for future research are also highlighted.},
  keywords   = {Visual odometry, Global positioning system, Image stream, Inertial navigation system, Localization sensors},
  shorttitle = {Review of visual odometry},
}

@Article{maase_performance_2018,
  author     = {Maase, Simone and Dilrosun, Xiomara and Kooi, Martijn and van den Hoed, Robert},
  title      = {Performance of {Electric} {Vehicle} {Charging} {Infrastructure}: {Development} of an {Assessment} {Platform} {Based} on {Charging} {Data}},
  journal    = {World Electric Vehicle Journal},
  year       = {2018},
  language   = {en},
  volume     = {9},
  number     = {2},
  month      = jul,
  pages      = {25},
  doi        = {10.3390/wevj9020025},
  url        = {https://www.mdpi.com/2032-6653/9/2/25},
  urldate    = {2018-10-08},
  abstract   = {Developers of charging infrastructure, be it public or private parties, are highly dependent on accurate utilization data in order to make informed decisions where and when to expand charging points. The Amsterdam University of Applied Sciences, in close cooperation with the municipalities of Amsterdam, Rotterdam, The Hague, Utrecht, and the Metropolitan Region of Amsterdam Electric, developed both the back- and front-end of a charging infrastructure assessment platform that processes and represents real-life charging data. Charging infrastructure planning and design methods described in the literature use geographic information system data, traffic flow data of non-EV vehicles, or geographical distributions of, for example, refueling stations for combustion engine vehicles. Only limited methods apply real-life charging data. Rolling out public charging infrastructure is a balancing act between stimulating the transition to zero-emission transport by enabling (candidate) EV drivers to charge, and limiting costly investments in public charging infrastructure. Five key performance indicators for charging infrastructure utilization are derived from literature, workshops, and discussions with practitioners. The paper describes the Data Warehouse architecture designed for processing large amounts of charging data, and the web-based assessment platform by which practitioners get access to relevant knowledge and information about the current performance of existing charging infrastructure represented by the key performance indicators developed. The platform allows stakeholders in the decision-making process of charging point installation to make informed decisions on where and how to expand the already existing charging infrastructure. The results are generalizable beyond the case study regions in the Netherlands and can serve the roll-out of charging infrastructure, both public and semi-public, all over the world.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  file       = {html\:PDF:html\:Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/LYZXZS7Y/Maase et al. - 2018 - Performance of Electric Vehicle Charging Infrastru.pdf/\:application/pdf/\;Snapshot/\:C//\:/Users/Psykie/Zotero/storage/UMEGFTKU/25.html/\:text/html\:PDF:PDF},
  keywords   = {EV charging, decision support, infrastructure, key performance indicator, municipal government, public EV charging infrastructure, simulation, user behavior, zero-emission transport},
  shorttitle = {Performance of {Electric} {Vehicle} {Charging} {Infrastructure}},
}

@article{zhou_dynamic_2018,
	title = {Dynamic {Objects} {Segmentation} for {Visual} {Localization} in {Urban} {Environments}},
	url = {http://arxiv.org/abs/1807.02996},
	abstract = {Visual localization and mapping is a crucial capability to address many challenges in mobile robotics. It constitutes a robust, accurate and cost-effective approach for local and global pose estimation within prior maps. Yet, in highly dynamic environments, like crowded city streets, problems arise as major parts of the image can be covered by dynamic objects. Consequently, visual odometry pipelines often diverge and the localization systems malfunction as detected features are not consistent with the precomputed 3D model.},
	language = {en},
	urldate = {2018-10-09},
	journal = {arXiv:1807.02996 [cs]},
	author = {Zhou, Guoxiang and Bescos, Berta and Dymczyk, Marcin and Pfeiffer, Mark and Neira, José and Siegwart, Roland},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.02996},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics}
}

@Article{boeing_wambot:_2012,
  author   = {Boeing, Adrian and Boulton, Mark and Bräunl, Thomas and Frisch, Brian and Lopes, Sam and Morgan, Aidan and Ophelders, Frank and Pangeni, Sushil and Reid, Robert and Vinsen, Kevin and Garel, Nicolas and Lee, Chang Su and Masek, Martin and Attwood, Anthony and Fazio, Michael and Gandossi, Adam},
  title    = {{WAMbot}: {Team} {MAGICian}'s entry to the {Multi} {Autonomous} {Ground}-robotic {International} {Challenge} 2010},
  journal  = {Journal of Field Robotics},
  year     = {2012},
  volume   = {29},
  number   = {5},
  month    = jul,
  pages    = {707--728},
  issn     = {1556-4959},
  doi      = {10.1002/rob.21434},
  urldate  = {2018-10-31},
  abstract = {Abstract This article describes ?WAMbot,'' an autonomous unmanned ground vehicle and the Team MAGICian solution to the 2010 Multi Autonomous Ground-robotic International Challenge (MAGIC). The system architecture, hardware, sensing, planning, navigation, mapping systems, and user interface were specifically designed to address the demands of the large-scale distributed reconnaissance and surveillance task presented in the challenge. The resulting system was selected as a finalist and ultimately placed 4th in MAGIC 2010. ? 2012 Wiley Periodicals, Inc.},
  annote   = {doi: 10.1002/rob.21434},
}

@InProceedings{r._reid_cooperative_2013,
  author    = {Robert Reid and Andrew Cann and Calum Meiklejohn and Liam Poli and Adrian Boeing and Thomas Bräunl},
  title     = {Cooperative multi-robot navigation, exploration, mapping and object detection with {ROS}},
  booktitle = {2013 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
  year      = {2013},
  month     = jun,
  isbn      = {1931-0587},
  pages     = {1083--1088},
  doi       = {10.1109/IVS.2013.6629610},
  keywords  = {control engineering computing, mobile robots, robot vision, Visualization, Object recognition, Cameras, object detection, Navigation, object recognition, Robot kinematics, Simultaneous localization and mapping, public domain software, graph theory, ROS, multi-robot systems, centralized global planner, competitive down-selection process, cooperative multirobot navigation, decentralized local navigation, global maps, intuitive graph-based visual object recognition pipeline, large-scale mapping stack, large-scale urban environments, MAGIC 2010, Multi Autonomous Ground-robotic International Challenge in 2010, multirobot exploration, multirobot mapping, navigation stack, open source robot operating system software framework, operating systems (computers), WAMbot system},
}

@inproceedings{a._boeing_real-time_2012,
	title = {Real-time tactical motion planning and obstacle avoidance for multi-robot cooperative reconnaissance},
	isbn = {1062-922X},
	doi = {10.1109/ICSMC.2012.6378270},
	booktitle = {2012 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {{A. Boeing} and {S. Pangeni} and {T. Bräunl} and {C. S. Lee}},
	month = oct,
	year = {2012},
	keywords = {mobile robots, Dynamics, navigation, Vehicle dynamics, path planning, Navigation, Vehicles, Robot kinematics, mobile robot, collision avoidance, unmanned ground vehicles, multi-robot systems, obstacle avoidance, Collision avoidance, vehicle dynamics, remotely operated vehicles, dynamic vehicle control, hierarchical architecture, Intelligence Surveillance and Reconnaissance mission, ISR mission, large urban environment, Multi Autonomous Ground-robotic International Challenge, multirobot cooperative reconnaissance, multivehicle tactical planning, realtime tactical motion planning, team MAGICian, trajectory generation, UGV},
	pages = {3117--3122}
}

@inproceedings{a._boeing_cooperative_2011,
	title = {Cooperative multi-robot navigation and mapping of unknown terrain},
	isbn = {2158-219X},
	doi = {10.1109/RAMECH.2011.6070488},
	booktitle = {2011 {IEEE} 5th {International} {Conference} on {Robotics}, {Automation} and {Mechatronics} ({RAM})},
	author = {{A. Boeing} and {T. Bräunl} and {R. Reid} and {A. Morgan} and {K. Vinsen}},
	month = sep,
	year = {2011},
	keywords = {mobile robots, navigation, path planning, Navigation, Lasers, Robot sensing systems, Planning, Robot kinematics, public domain software, urban environment, multi-robot systems, cooperative multirobot navigation, Automation, cooperative mapping, cooperative multirobot mapping, cooperative navigation, cooperative reconnaissance, cooperative systems, down-selection processes, hardware equipment, MAGIC 2010 robotics challenge, mobile robot design, off-the-shelf hardware, open source software, robot team, sensor equipment, software contributions, software equipment, surveillance, surveillance tasks, terrain mapping, unknown terrain},
	pages = {234--238}
}

@inproceedings{r._reid_large-scale_2011,
	title = {Large-scale multi-robot mapping in {MAGIC} 2010},
	isbn = {2158-219X},
	doi = {10.1109/RAMECH.2011.6070489},
	booktitle = {2011 {IEEE} 5th {International} {Conference} on {Robotics}, {Automation} and {Mechatronics} ({RAM})},
	author = {{R. Reid} and {T. Bräunl}},
	month = sep,
	year = {2011},
	keywords = {mobile robots, SLAM (robots), Real time systems, Robot kinematics, Simultaneous localization and mapping, graph theory, Optimization, multi-robot systems, optimisation, MAGIC 2010, decentralised, distributed, distributed mapping system, fusion operations, globally optimised metric maps, GPU map correlation, GPU map fusion, graph-based SLAM, graphics processing unit, Graphics processing unit, hybrid decentralised system, large-scale decentralised multirobot mapping system, Large-scale multi-robot mapping, large-scale multirobot graph-based nonlinear map optimisation, large-scale multirobot mapping, map matching, multiautonomous ground-robotics international challenge, multivariable systems, simultaneous localisation \& mapping (SLAM), WAMbot},
	pages = {239--244}
}

@inproceedings{s._lopes_autonomous_2011,
	title = {Autonomous exploration of unknown terrain for groups of mobile robots},
	isbn = {1931-0587},
	doi = {10.1109/IVS.2011.5940455},
	booktitle = {2011 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {{S. Lopes} and {B. Frisch} and {A. Boeing} and {K. Vinsen} and {T. Bräunl}},
	month = jun,
	year = {2011},
	keywords = {mobile robots, Measurement, Pixel, Accuracy, Robots, multi-robot systems, sensors, WAMbot, Autonomous navigation, autonomous unknown terrain exploration, Cost function, Floods, intelligent robots, MAGIC2010 robot challenge, multi-robot exploration, Pioneer 3 AT robots, sensor systems, Space exploration},
	pages = {157--162}
}

@phdthesis{reid_large-scale_2016,
	address = {Perth, Australia},
	type = {{PhD} {Thesis}},
	title = {Large-{Scale} {Simultaneous} {Localization} and {Mapping} for {Teams} of {Mobile} {Robots}},
	url = {http://robotics.ee.uwa.edu.au/theses/2016-SLAM-Reid-PhD.pdf},
	language = {English},
	urldate = {2018-10-31},
	school = {The University of Western Australia},
	author = {Reid, Robert},
	month = jul,
	year = {2016}
}

@InProceedings{k._l._lim_implementation_2017,
  author    = {Kai Li Lim and Thomas Drage and Thomas Bräunl},
  title     = {Implementation of semantic segmentation for road and lane detection on an autonomous ground vehicle with {LIDAR}},
  booktitle = {2017 {IEEE} {International} {Conference} on {Multisensor} {Fusion} and {Integration} for {Intelligent} {Systems} ({MFI})},
  year      = {2017},
  publisher = {{IEEE}},
  month     = nov,
  pages     = {429--434},
  doi       = {10.1109/MFI.2017.8170358},
  abstract  = {While current implementations of LIDAR-based autonomous driving systems are capable of road following and obstacle avoidance, they are still unable to detect road lane markings, which is required for lane keeping during autonomous driving sequences. In this paper, we present an implementation of semantic image segmentation to enhance a LIDAR-based autonomous ground vehicle for road and lane marking detection, in addition to object perception and classification. To achieve this, we installed and calibrated a low-cost monocular camera onto a LIDAR-fitted Formula-SAE Electric car as our test bench. Tests were performed first on video recordings of local roads to verify the feasibility of semantic segmentation, and then on the Formula-SAE car with LIDAR readings. Results from semantic segmentation confirmed that the road areas in each video frame were properly segmented, and that road edges and lane markers can be classified. By combining this information with LIDAR measurements for road edges and obstacles, distance measurements for each segmented object can be obtained, thereby allowing the vehicle to be programmed to drive autonomously within the road lanes and away from road edges.},
  file      = {pdf\:PDF:pdf\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/35X9ZR4M/8170358.html/\:text/html/\;IEEE Xplore Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/R3EREEZL/Lim et al. - 2017 - Implementation of semantic segmentation for road a.pdf/\:application/pdf\:PDF:PDF},
  keywords  = {Automobiles, mobile robots, Roads, traffic engineering computing, feature extraction, image classification, image segmentation, Semantics, Cameras, object detection, Image edge detection, road traffic, Laser radar, road detection, automobiles, cameras, optical radar, Robot sensing systems, edge detection, autonomous ground vehicle, collision avoidance, distance measurement, obstacle avoidance, autonomous driving sequences, autonomous driving systems, distance measurements, Formula-SAE Electric car, lane markers classification, lane marking detection, LIDAR measurements, LIDAR readings, low-cost monocular camera, object segmentation, remotely operated vehicles, road edges classification, road lane markings, semantic image segmentation, video recordings},
}

@WWW{chen_yunchih/orb-slam2-gpu2016-final_2019,
  author    = {Chen, Yunchih},
  title     = {yunchih/{ORB}-{SLAM}2-{GPU}2016-final},
  year      = {2019},
  url       = {https://github.com/yunchih/ORB-SLAM2-GPU2016-final},
  note      = {original-date: 2016-05-09T22:52:27Z},
  month     = feb,
  urldate   = {2019-02-25},
  copyright = {View license},
}

@WWW{chargestar_chargestar_nodate,
  author   = {{ChargeStar}},
  title    = {{ChargeStar}},
  year     = {2018},
  url      = {https://www.chargestar.com.au/},
  language = {en-US},
  urldate  = {2019-04-10},
  abstract = {EV Charge Stations \& Supplies {\textbar} Charge Station Access Swipe Cards (RFID) {\textbar} EV Control Systems {\textbar} EV Users Web Interface {\textbar} Tel: (08) 6102 1285},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\NMXB6U6C\\\\\\\\www.chargestar.com.au.html\\\:text/html\:PDF:PDF},
  journal  = {ChargeStar},
}

@WWW{guy_python_2019,
  author    = {Guy, J. J.},
  title     = {Python module to create heatmaps.},
  year      = {2019},
  url       = {https://github.com/jjguy/heatmap},
  note      = {original-date: 2012-09-18T02:02:44Z},
  month     = mar,
  urldate   = {2019-04-04},
  copyright = {MIT},
}

@WWW{asial_corporation_jpgraph_nodate,
  author  = {{Asial Corporation}},
  title   = {{JpGraph}},
  year    = {2018},
  url     = {https://jpgraph.net/},
  urldate = {2019-04-04},
  file    = {html\:PDF:html\:JpGraph - Most powerful PHP-driven charts/\:C//\:/Users/Psykie/Zotero/storage/PE7CYAGS/jpgraph.net.html/\:text/html\:PDF:PDF},
}

@TechReport{sma_solar_technology_ag_sunny_nodate,
  author      = {{SMA Solar Technology AG}},
  title       = {Sunny {Webbox}},
  institution = {SMA Solar Technology AG},
  year        = {2018},
  type        = {Datasheet},
  url         = {http://files.sma.de/dl/2585/WEBBOX-DEN102530.pdf},
  urldate     = {2019-04-03},
  school      = {SMA Solar Technology AG},
}

@TechReport{sma_solar_technology_ag_sunny_2017,
  author      = {{SMA Solar Technology AG}},
  title       = {Sunny {Tripower} 5000TL-12000TL},
  institution = {SMA Solar Technology AG},
  year        = {2017},
  type        = {Datasheet},
  language    = {en},
  month       = may,
  pages       = {6},
  url         = {https://files.sma.de/dl/17781/STP12000TL-DAU1723-V10web.pdf},
  urldate     = {2019-04-03},
  abstract    = {Perfectly suited to the design of the traditional residential PV system up to the higher power outage range.},
  file        = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/P22ZQ8VL/sunny-tripower-5000tl-12000tl.html/\:text/html\:PDF:PDF},
  school      = {SMA Solar Technology AG},
}

@Report{national_transport_commission_carbon_2014,
  author      = {{National Transport Commission}},
  title       = {Carbon {Dioxide} {Emissions} from {New} {Australian} {Vehicles} 2013},
  type        = {Information paper},
  institution = {National Transport Commission},
  year        = {2014},
  language    = {en},
  month       = may,
  pages       = {68},
  url         = {https://www.ntc.gov.au/Media/Reports/(6B1DD6CF-FB2C-B934-74A8-47971CB09050).pdf},
  urldate     = {2019-04-03},
  file        = {html\:PDF:html\:NTC - National Transport Commission/\:C//\:/Users/Psykie/Zotero/storage/I3FXIJIE/light-vehicle-emissions.html/\:text/html\:PDF:PDF},
  school      = {National Transport Commission},
}

@Report{u-blox_ag_eva-m8_2016,
  author   = {{u-blox AG}},
  title    = {{EVA}-{M}8 series},
  type     = {Data sheet},
  year     = {2016},
  language = {en-US},
  number   = {UBX-160007405, R02},
  month    = jul,
  pages    = {32},
  url      = {https://www.u-blox.com/sites/default/files/EVA-M8-FW3_DataSheet_%28UBX-16014189%29.pdf},
  urldate  = {2019-04-03},
  abstract = {Cost-efficient u-blox M8 GNSS SiPs},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/8FAHY5RB/eva-m8-series.html/\:text/html\:PDF:PDF},
  school   = {u-blox},
}

@TechReport{yiu_armv8-m_2015,
  author      = {Yiu, Joseph},
  title       = {{ARMv}8-{M} {Architecture} {Technical} {Overview}},
  institution = {ARM},
  year        = {2015},
  type        = {White {Paper}},
  language    = {en},
  month       = nov,
  pages       = {16},
  url         = {https://community.arm.com/cfs-file/__key/telligent-evolution-components-attachments/01-2142-00-00-00-00-66-90/Whitepaper-_2D00_-ARMv8_2D00_M-Architecture-Technical-Overview.pdf},
  urldate     = {2019-04-03},
  abstract    = {The next generation of Cortex-M processors are powered by Armv8-M architecture. This blog is a technical overview of enhancements in the new architecture and an introduction to TrustZone for Armv8-M.},
  file        = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/VCAWNTUL/whitepaper-armv8-m-architecture-technical-overview.html/\:text/html\:PDF:PDF},
  school      = {ARM Limited},
}

@WWW{astra_telematics_limited_at240_nodate,
  author   = {{Astra Telematics}},
  title    = {{AT}240 3G {UMTS} / {GNSS} {IP}67 waterproof vehicle tracking device},
  year     = {2018},
  url      = {https://gps-telematics.co.uk/products/at240-waterproof-vehicle-tracking-device/},
  language = {en-GB},
  urldate  = {2019-04-03},
  abstract = {AT240 3G UMTS / GNSS IP67 waterproof vehicle tracking device with integrated antennas, the AT240 is simple to install and inconspicuous in appearance.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/HNNCNGLL/at240-waterproof-vehicle-tracking-device.html/\:text/html\:PDF:PDF},
  number   = {N},
  type     = {Datasheet},
}

@WWW{astra_telematics_limited_at110_nodate,
  author   = {{Astra Telematics}},
  title    = {{AT}110 {GPS}-{GPRS} vehicle tracking device},
  year     = {2018},
  url      = {https://gps-telematics.co.uk/products/at110-gps-gprs-fleet-management-applications/},
  language = {en-GB},
  urldate  = {2019-04-03},
  abstract = {AT110 GPS-GPRS vehicle tracking device is aimed at fleet management applications, wide range of features make it equally appealing in other applications.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/DPDN5GLR/at110-gps-gprs-fleet-management-applications.html/\:text/html\:PDF:PDF},
  number   = {A},
  type     = {Datasheet},
}

@WWW{nxp_semiconductors_mifare_nodate,
  author   = {{NXP Semiconductors}},
  title    = {{MIFARE} {DESFire} {EV}1},
  year     = {2018},
  url      = {https://www.mifare.net/en/products/chip-card-ics/mifare-desfire/mifare-desfire-ev1/},
  language = {en},
  urldate  = {2019-04-03},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/QXBSCTIG/mifare-desfire-ev1.html/\:text/html\:PDF:PDF},
}

@WWW{xiamen_four-faith_communication_technology_co._ltd._f2414_nodate,
  author  = {{Xiamen Four-Faith Communication Technology Co., Ltd.}},
  title   = {F2414 {IP} {MODEM}({DTU})},
  year    = {2018},
  url     = {https://en.four-faith.com/f2414-wcdma-ip-modem.html},
  urldate = {2019-04-03},
  file    = {html\:PDF:html\:Difference Modem Between WCDMA,GPRS,GSM,HSPA,HSDPA,HSUPA,UTMS/\:C//\:/Users/Psykie/Zotero/storage/FEQLAQXN/f2414-wcdma-ip-modem.html/\:text/html\:PDF:PDF},
}

@WWW{google_developers_google_nodate,
  author   = {{Google Developers}},
  title    = {Google {Visualization} {API} {Reference} {\textbar} {Charts}},
  year     = {2019},
  url      = {https://developers.google.com/chart/interactive/docs/reference},
  language = {en},
  urldate  = {2019-04-03},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/ZXSIHLKR/reference.html/\:text/html\:PDF:PDF},
  journal  = {Google Developers},
}

@WWW{tritium_pty_ltd_veefil-rt_nodate,
  author  = {{Tritium}},
  title   = {{VEEFIL}-{RT} 50KW {DC} {FAST} {CHARGER}},
  year    = {2019},
  url     = {https://www.tritium.com.au/product/productitem?url=veefil-rt-50kw-dc-fast-charger},
  urldate = {2019-04-03},
  file    = {html\:PDF:html\:Product - Tritium | Offering Energy Freedom/\:C//\:/Users/Psykie/Zotero/storage/STIH3VV2/productitem.html/\:text/html\:PDF:PDF},
}

@Article{speidel_driving_2014,
  author   = {Speidel, Stuart and Bräunl, Thomas},
  title    = {Driving and charging patterns of electric vehicles for energy usage},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2014},
  volume   = {40},
  month    = dec,
  pages    = {97--110},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2014.07.177},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032114006297},
  urldate  = {2019-04-03},
  abstract = {This paper presents findings from the Western Australian Electric Vehicle Trial (2010–2012) and the ongoing Electric vehicle (EV) charging research network in Perth. The University of Western Australia is collecting the data from eleven locally converted EVs and 23 charging stations. The data confirms most charging is conducted at business and home locations (55\%), while charging stations were only used for 33\% of charging events. The EV charging power over time-of-day and aggregated over all charging stations closely resembles a solar PV curve, which means that EV charging stations can ideally be offset by solar PV. Another important finding is that EVs spend significantly more time at a charging station than what is technically required for the charging process. Also on average, EVs have more than 50\% battery charge remaining when they plug in. This tells us parking spaces are in higher demand than Level-2 charging facilities.},
  file     = {html\:PDF:html\:ScienceDirect Full Text PDF/\:C//\:/Users/Psykie/Zotero/storage/W2LMLNXF/Speidel and Bräunl - 2014 - Driving and charging patterns of electric vehicles.pdf/\:application/pdf/\;ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/LCF8MGCH/S1364032114006297.html/\:text/html\:PDF:PDF},
  keywords = {Electric vehicle charging, Electricity Grid, Home charging, Pool vehicles},
}

@InProceedings{speidel_analysis_2012,
  author    = {Speidel, S. and Jabeen, F. and Olaru, D. and Harries, D. and Bräunl, T.},
  title     = {Analysis of {Western} {Australian} electric vehicle and charging station trials},
  booktitle = {35th 2012 Australasian Transport Research Forum (ATRF)},
  year      = {2012},
  month     = sep,
  url       = {https://trid.trb.org/view/1224141},
  urldate   = {2019-04-03},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/NJX4WIKY/1224141.html/\:text/html\:PDF:PDF},
}

@WWW{elaadnl_elaad_nodate,
  author  = {ElaadNL},
  title   = {Elaad {NL}},
  year    = {2019},
  url     = {https://www.elaad.nl/},
  urldate = {2019-04-03},
  file    = {html\:PDF:html\:Home - Elaad NL\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\VNDIQEIU\\\\\\\\www.elaad.nl.html\\\:text/html\:PDF:PDF},
}

@Article{commission_iec_2014,
  author  = {{International Electrotechnical Commission}},
  title   = {{IEC} 62196-3: 2014},
  journal = {Plugs, Socket-Outlets, Vehicle Connectors and Vehicle Inlets–Conductive Charging of Electric Vehicles–Part},
  year    = {2014},
  volume  = {3},
}

@inproceedings{anegawa_development_2010,
	title = {Development of quick charging system for electric vehicle},
	booktitle = {Proc. {World} {Energy} {Congress}},
	publisher = {Tokyo Electric Power Company},
	author = {Anegawa, Takafumi},
	year = {2010}
}

@WWW{synergy_solar_nodate,
  author   = {Synergy},
  title    = {Solar \& battery},
  year     = {2018},
  url      = {https://www.synergy.net.au:443/Solar-and-battery},
  language = {en},
  urldate  = {2019-04-02},
  abstract = {Our SolarReturn solution gives you access to a range of high quality, high efficiency solar system packages, coupled with our long history and dedication to great customer service. Click here to find out how you can maximise your savings and minimise your energy costs.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/FR3DFEQD/Solar-and-battery.html/\:text/html\:PDF:PDF},
  journal  = {Synergy},
}

@WWW{solaredge_technologies_inc._solaredge_nodate,
  author  = {{SolarEdge Technologies Inc.}},
  title   = {{SolarEdge}},
  year    = {2019},
  url     = {https://www.solaredge.com/aus/},
  urldate = {2019-04-02},
  file    = {html\:PDF:html\:SolarEdge | A World Leader in Smart Energy | A World Leader in Smart Energy/\:C//\:/Users/Psykie/Zotero/storage/6S26KRNI/aus.html/\:text/html\:PDF:PDF},
}

@WWW{gpsgate_ab_gpsgate_nodate,
  author  = {{GpsGate AB}},
  title   = {{GpsGate}},
  year    = {2019},
  url     = {https://gpsgate.com/},
  urldate = {2019-04-02},
  file    = {html\:PDF:html\:GpsGate - GPS Tracking your way/\:C//\:/Users/Psykie/Zotero/storage/Q82IT7TE/gpsgate.com.html/\:text/html\:PDF:PDF},
}

@WWW{traccar_ltd_traccar_nodate,
  author  = {{Traccar Ltd}},
  title   = {Traccar},
  year    = {2019},
  url     = {https://www.traccar.org/},
  urldate = {2019-04-02},
  file    = {html\:PDF:html\:GPS Tracking Software - Free and Open Source System - Traccar\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\HXXQ23JT\\\\\\\\www.traccar.org.html\\\:text/html\:PDF:PDF},
}

@WWW{geotelematic_solutions_inc._opengts_nodate,
  author  = {{GeoTelematic Solutions, Inc.}},
  title   = {{OpenGTS}},
  year    = {2019},
  url     = {http://www.opengts.org/},
  urldate = {2019-04-02},
  file    = {html\:PDF:html\:GPS Tracking\\\\\\\: Open-Source GPS Tracking System - OpenGTS\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\5GDRIP69\\\\\\\\www.opengts.org.html\\\:text/html\:PDF:PDF},
}

@WWW{go_electric_stations_s.r.l.s_go_nodate,
  author   = {{Go Electric Stations S.r.l.s}},
  title    = {Go {Electric} {Stations}},
  year     = {2019},
  url      = {https://goelectricstations.com/},
  language = {en},
  urldate  = {2019-04-02},
  abstract = {Go Electric Stations - - EV-World - Discover charge points worldwide},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/2XWB6K2G/goelectricstations.com.html/\:text/html\:PDF:PDF},
}

@WWW{chargepoint_inc_chargepoint_nodate,
  author   = {{ChargePoint Inc}},
  title    = {{ChargePoint}},
  year     = {2019},
  url      = {https://www.chargepoint.com},
  language = {en-au},
  urldate  = {2019-04-02},
  abstract = {ChargePoint is the world's largest network of electric vehicle (EV) charging stations in the US, Europe, Australia. Join the EV revolution for a greener tomorrow!},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/Q2CUWC7L/en-au.html/\:text/html\:PDF:PDF},
  journal  = {ChargePoint},
}

@Article{girshick_region-based_2016,
  author   = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
  title    = {Region-{Based} {Convolutional} {Networks} for {Accurate} {Object} {Detection} and {Segmentation}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2016},
  volume   = {38},
  number   = {1},
  month    = jan,
  pages    = {142--158},
  issn     = {0162-8828},
  doi      = {10.1109/TPAMI.2015.2437384},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/DPHMMY6E/7112511.html/\:text/html\:PDF:PDF},
  keywords = {Training, Object recognition, image segmentation, semantic segmentation, Image segmentation, object detection, Feature extraction, Support vector machines, deep learning, Detectors, object segmentation, canonical PASCAL VOC Challenge datasets, convolutional codes, convolutional networks, detection, high-capacity convolutional networks, image coding, mAP, mean average precision, Object detection, Proposals, region-based convolutional networks, source code, source coding, transfer learning},
}

@inproceedings{a._rodriguez-serrano_communication_2013,
	title = {A communication system from {EV} to {EV} {Service} {Provider} based on {OCPP} over a wireless network},
	isbn = {1553-572X},
	doi = {10.1109/IECON.2013.6700020},
	abstract = {The progressive market penetration of Electrical Vehicles (EVs) expected for next decades demands the development of new ICT services and tools, and Wireless Sensor Networks (WSN) are an attractive option for communication that allow fast and cost-effective deployments. Standardization is also an important issue, as communication standards for EVs are still in a state of relative immaturity, even though some initiatives for interoperability are receiving wide acceptance, as it is the case of the OCPP protocol, which is becoming a de facto standard of ever increasing use. This paper presents the development of a communication network for EV charging stations using IEEE 802.15.4 and the OCPP protocol over SOAP/TCP technology. A system demonstrator has also been built, in collaboration with a dozen partners in the frame of the “Internet of Energy (IoE)” project, as part of the European ARTEMIS Joint Technology Initiative (JTI).},
	booktitle = {{IECON} 2013 - 39th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	author = {{Á. Rodríguez-Serrano} and {A. Torralba} and {E. Rodríguez-Valencia} and {J. Tarifa-Galisteo}},
	month = nov,
	year = {2013},
	keywords = {electric vehicles, Standards, Vehicles, vehicular ad hoc networks, Logic gates, Zigbee, communication system, electrical vehicles, European ARTEMIS joint technology initiative, EV (Electric Vehicle), EV to EV service provider, EVSE (EV Supply Equipment), EVSP (EV Service Provider), ICT services, IEEE 802.15.4, Internet of energy, IoE, JTI, OCPP (Open Charging Point Protocol), OCPP protocol, Protocols, Simple object access protocol, SOAP/TCP technology, transport protocols, Wireless communication, wireless sensor networks, Wireless Sensor Networks, WSN},
	pages = {5434--5438}
}

@Report{international_energy_agency_global_2018,
  author   = {{International Energy Agency}},
  title    = {Global {EV} {Outlook} 2018},
  type     = {Report},
  year     = {2017},
  date     = {2017-05},
  language = {English},
  month    = may,
  pages    = {141},
  url      = {https://activatecp.com/wp-content/uploads/2018/06/global_ev_outlook_2018.pdf},
  urldate  = {2019-04-23},
  address  = {Paris},
  school   = {International Energy Agency},
}

@WWW{linxio_linxio_nodate,
  author   = {{Linxio}},
  title    = {Linxio},
  year     = {2019},
  url      = {https://linxio.com/},
  language = {en-US},
  urldate  = {2019-04-23},
  abstract = {Specialising in GPS trackers to suit any budget and industry in Australia. Find out more about Linxio's best in class GPS Management Software today.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/LJDT2FUM/linxio.com.html/\:text/html\:PDF:PDF},
  journal  = {linxio.com},
}

@WWW{verizon_fleetmatics_nodate,
  author  = {{Verizon}},
  title   = {Fleetmatics},
  year    = {2019},
  url     = {https://www.verizonconnect.com/au/fleetmatics/},
  urldate = {2019-04-23},
  file    = {html\:PDF:html\:Fleetmatics REVEAL and WORK have become Verizon Connect/\:C//\:/Users/Psykie/Zotero/storage/QRZJ3GG7/fleetmatics.html/\:text/html\:PDF:PDF},
}

@WWW{netstar_ezy2c_nodate,
  author   = {{NETSTAR}},
  title    = {{EZY}2c {GPS} {Tracking}},
  year     = {2019},
  url      = {https://www.ezy2c.com.au/},
  urldate  = {2019-04-23},
  abstract = {EZY2C provides the most advanced GPS tracking \& reporting solutions at an affordable price. Our GPS Tracking systems provide location, unlimited history, real-time alerts, routing \& messaging. EZY2c GPS tracker is considered one of the most reliable for tracking Vehicle Positioning, Fleet Management, Devices, \& more.},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\9XQSYV6S\\\\\\\\www.ezy2c.com.au.html\\\:text/html\:PDF:PDF},
  journal  = {EZY2c GPS Tracking},
}

@InProceedings{jabeen_acceptability_2012,
  author     = {Jabeen, F. and Olaru, D. and Smith, B. and Bräunl, T. and Speidel, S.},
  title      = {Acceptability of electric vehicles: findings from a driver survey},
  booktitle  = {Australasian Transport Research Forum (ATRF), 35th, 2012, Perth, Western Australia, Australia},
  year       = {2012},
  month      = sep,
  url        = {https://trid.trb.org/view/1224115},
  urldate    = {2019-04-23},
  address    = {Perth, Australia},
  file       = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/8BV22AUX/1224115.html/\:text/html\:PDF:PDF},
  shorttitle = {Acceptability of electric vehicles},
}

@InProceedings{jabeen_electric_2013,
  author    = {Jabeen, Fakhra and Olaru, Doina and Smith, Brett and Bräunl, Thomas and Speidel, Stuart},
  title     = {Electric vehicle battery charging behaviour: findings from a driver survey},
  booktitle = {Australasian Transport Research Forum (ATRF), 36th, 2013, Brisbane, Queensland, Australia},
  year      = {2013},
  address   = {Brisbane},
}

@article{magana_gafu:_2015,
	title = {{GAFU}: {Using} a {Gamification} {Tool} to {Save} {Fuel}},
	volume = {7},
	issn = {1939-1390},
	shorttitle = {{GAFU}},
	url = {http://ieeexplore.ieee.org/document/7091134/},
	doi = {10.1109/MITS.2015.2408152},
	number = {2},
	urldate = {2019-04-23},
	journal = {IEEE Intelligent Transportation Systems Magazine},
	author = {Magana, V. Corcoba and Munoz-Organero, M.},
	year = {2015},
	pages = {58--70}
}

@Article{mullan_technical_2012,
  author   = {Mullan, Jonathan and Harries, David and Bräunl, Thomas and Whitely, Stephen},
  title    = {The technical, economic and commercial viability of the vehicle-to-grid concept},
  journal  = {Energy Policy},
  year     = {2012},
  language = {en},
  volume   = {48},
  month    = sep,
  pages    = {394--406},
  issn     = {0301-4215},
  doi      = {10.1016/j.enpol.2012.05.042},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0301421512004545},
  urldate  = {2019-04-23},
}

@InProceedings{hill_tracking_2012,
  author    = {Hill, G.A. and Blythe, P.T. and Suresh, V.},
  title     = {Tracking and managing real world electric vehicle power usage and supply},
  booktitle = {9th {IET} {Data} {Fusion} \& {Target} {Tracking} {Conference} ({DF}\&{TT} 2012): {Algorithms} \& {Applications}},
  year      = {2012},
  language  = {en},
  publisher = {IET},
  isbn      = {978-1-84919-624-6},
  pages     = {15--15},
  doi       = {10.1049/cp.2012.0415},
  urldate   = {2019-04-23},
  address   = {London, UK},
}

@WWW{australian_government_green_nodate,
  author  = {{Australian Government}},
  title   = {Green {Vehicle} {Guide} {Home}},
  year    = {2019},
  url     = {http://www.greenvehicleguide.gov.au/},
  urldate = {2019-04-23},
  file    = {html\:PDF:html\:Green Vehicle Guide Home\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\JMJXVLXX\\\\\\\\www.greenvehicleguide.gov.au.html\\\:text/html\:PDF:PDF},
}

@TechReport{van_namen_green_2011,
  author      = {van Namen, Kate and Tieu, Anle and Olden, Peter},
  title       = {Green {House} {Gas} {Emissions} from {Households} in {Western} {Australia}},
  institution = {SMEC Australia},
  year        = {2011},
  date        = {2011-08},
  type        = {Report},
  language    = {English},
  number      = {3006127},
  month       = aug,
  pages       = {47},
  url         = {http://www.vegetableclimate.com/wp-content/uploads/2013/10/reduced-size-AT_LS_P_GreenHouseGasEmissions.pdf},
  address     = {Perth},
  school      = {SMEC Australia Ltd},
}

@article{anair_state_2012,
	title = {State of charge},
	volume = {10},
	url = {http://esci-ksp.org/wp/wp-content/uploads/2012/04/State-of-Charge.pdf},
	journal = {Union of Concerned Scientists},
	author = {Anair, Don and Mahmassani, Amine},
	year = {2012}
}

@TechReport{department_of_industry_innovation_and_science_australian_2016,
  author   = {{Department of Industry, Innovation {and} Science}},
  title    = {Australian {Energy} {Update} 2016},
  year     = {2016},
  type     = {Report},
  language = {English},
  month    = sep,
  pages    = {32},
  url      = {https://archive.industry.gov.au/Office-of-the-Chief-Economist/Publications/Documents/aes/2016-australian-energy-statistics.pdf},
  address  = {Canberra},
  school   = {Department of Industry, Innovation and Science},
}

@Article{energy_supply_association_of_australia_sparking_2013,
  author        = {{Energy Supply Association of Australia}},
  title         = {Sparking an electric vehicle debate in Australia},
  journal       = {Discussion paper},
  year          = {2013},
  language      = {English},
  note          = {Cover title},
  catalogue-url = { https://trove.nla.gov.au/work/188053391 },
  publisher     = {Melbourne Energy Supply Association of Australia},
  subjects      = { Electric vehicles; Motor vehicle industry; Market share; Consumer behaviour; Trends; Statistics; Interstate comparisons; Australia overseas comparisons },
  type          = {Article},
}

@Unpublished{roberts_media_2012,
  author    = {Roberts, Natalie},
  title     = {Media {Release}: {ABMARC} releases key findings from their electric \& hybrid vehicles report},
  year      = {2012},
  language  = {English},
  month     = oct,
  url       = {http://www.abmarc.com.au/7_news_&_downloads/downloads/assets/ABMARC%20EV%20and%20Hybrid%20Report%20-%20Media%20Release.pdf},
  publisher = {ABMARC},
}

@TechReport{kinghorn_forecast_2011,
  author      = {Kinghorn, Robert and Kua, Dominic},
  title       = {Forecast {Uptake} and {Economic} {Evaluation} of {Electric} {Vehicles} in {Victoria}},
  institution = {AECOM},
  year        = {2011},
  type        = {Report},
  language    = {English},
  number      = {60149263},
  month       = may,
  pages       = {105},
  url         = {http://www.ehcar.net/library/rapport/rapport002.pdf},
  address     = {Melbourne},
  school      = {AECOM Australia Pty Ltd},
}

@TechReport{aecom_australia_economic_2009,
  author      = {{AECOM Australia}},
  title       = {Economic {Viability} of {Electric} {Vehicles}},
  institution = {AECOM},
  year        = {2009},
  type        = {Report},
  language    = {English},
  number      = {60099409},
  month       = sep,
  pages       = {102},
  url         = {https://www.environment.nsw.gov.au/resources/climatechange/ElectricVehiclesReport.pdf},
  address     = {Sydney},
  school      = {AECOM Australia Pty Ltd},
}

@Report{clean_energy_council_clean_2019,
  author      = {{Clean Energy Council}},
  title       = {Clean {Energy} {Australia} {Report} 2019},
  type        = {Report},
  institution = {Clean Energy Council Australia},
  year        = {2019},
  date        = {2019-04-04},
  language    = {en},
  month       = apr,
  pages       = {82},
  url         = {https://assets.cleanenergycouncil.org.au/documents/resources/reports/clean-energy-australia/clean-energy-australia-report-2019.pdf},
  urldate     = {2019-05-04},
  address     = {Melbourne},
  school      = {Clean Energy Council},
}

@WWW{noauthor_jquery_2019,
  author    = {jQuery},
  title     = {{jQuery} {JavaScript} {Library}},
  year      = {2019},
  url       = {https://github.com/jquery/jquery},
  note      = {original-date: 2009-04-03T15:20:14Z},
  month     = may,
  urldate   = {2019-05-01},
  copyright = {MIT},
  publisher = {jQuery},
}

@WWW{vanderkam_interactive_2019,
  author     = {Vanderkam, Dan},
  title      = {Interactive visualizations of time series using {JavaScript} and the {HTML} canvas tag: danvk/dygraphs},
  year       = {2019},
  url        = {https://github.com/danvk/dygraphs},
  note       = {original-date: 2009-11-24T10:26:21Z},
  month      = apr,
  urldate    = {2019-05-01},
  copyright  = {MIT},
  shorttitle = {Interactive visualizations of time series using {JavaScript} and the {HTML} canvas tag},
}

@inproceedings{kour_real-time_2014,
	title = {Real-time segmentation of on-line handwritten arabic script},
	booktitle = {Frontiers in {Handwriting} {Recognition} ({ICFHR}), 2014 14th {International} {Conference} on},
	publisher = {IEEE},
	author = {Kour, George and Saabne, Raid},
	year = {2014},
	pages = {417--422}
}

@inproceedings{kour_fast_2014,
	title = {Fast classification of handwritten on-line {Arabic} characters},
	booktitle = {Soft {Computing} and {Pattern} {Recognition} ({SoCPaR}), 2014 6th {International} {Conference} of},
	publisher = {IEEE},
	author = {Kour, George and Saabne, Raid},
	year = {2014},
	pages = {312--318}
}

@article{hadash_estimate_2018,
	title = {Estimate and {Replace}: {A} {Novel} {Approach} to {Integrating} {Deep} {Neural} {Networks} with {Existing} {Applications}},
	journal = {arXiv preprint arXiv:1804.09028},
	author = {Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
	year = {2018}
}

@article{alvarez_combining_2014,
	title = {Combining {Priors}, {Appearance}, and {Context} for {Road} {Detection}},
	volume = {15},
	issn = {1524-9050},
	doi = {10.1109/TITS.2013.2295427},
	number = {3},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Álvarez, Jose M. and López, Antonio M. and Gevers, Theo and Lumbreras, Felipe},
	year = {2014},
	keywords = {3-D scene layout, 3D scene layout, autonomous driving, Cameras, car collision warning, computer vision, contextual cues, contextual information, free road surface ahead detection, generative model, geographic information systems, geographical information, Global Positioning System, horizon lines, Illuminant invariance, Image color analysis, imaging conditions, lane markings, Lighting, low-level cues, low-level features, moving vehicle, object detection, road appearance, road detection, road geometry, road homogeneity, road prior, road prior online estimation, road scene understanding, road vehicles, Roads, Shape, structured roads, traffic engineering computing, uniform lighting conditions, vanishing point, vanishing points, Vehicles, vision-based road detection methods},
	pages = {1168--1178},
	annote = {The following values have no corresponding Zotero field:number: 3}
}

@WWW{comma.ai_self_2016,
  author = {{comma.ai}},
  title  = {Self coloring books},
  year   = {2016},
  url    = {https://commaai.blogspot.com/2016/07/self-coloring-books.html},
  type   = {Unpublished {Work}},
}

@article{costante_exploring_2016,
	title = {Exploring {Representation} {Learning} {With} {CNNs} for {Frame}-to-{Frame} {Ego}-{Motion} {Estimation}},
	volume = {1},
	issn = {2377-3766},
	doi = {10.1109/LRA.2015.2505717},
	number = {1},
	journal = {IEEE Robotics and Automation Letters},
	author = {Costante, G. and Mancini, M. and Valigi, P. and Ciarfuglia, T. A.},
	year = {2016},
	keywords = {Adaptive optics, Computer architecture, Estimation, Feature extraction, Optical imaging, Robustness, Visual Learning, Visual-Based Navigation, Visualization},
	pages = {18--25},
	annote = {The following values have no corresponding Zotero field:number: 1}
}

@WWW{daimler_ag._autonomous_2017,
  author = {{Daimler AG.}},
  title  = {Autonomous {Driving} - {Mobility} of the future},
  year   = {2017},
  url    = {https://www.daimler.com/innovation/autonomous-driving/},
  type   = {Unpublished {Work}},
}

@inproceedings{fardi_multi-modal_2003,
	title = {Multi-modal detection and parameter-based tracking of road borders with a laser scanner},
	doi = {10.1109/IVS.2003.1212890},
	booktitle = {{IEEE} {IV}2003 {Intelligent} {Vehicles} {Symposium}. {Proceedings} ({Cat}. {No}.03TH8683)},
	author = {Fardi, B. and Scheunert, U. and Cramer, H. and Wanielik, G.},
	year = {2003},
	keywords = {Chemical lasers, Chemical technology, computerised navigation, data fusion, Detection algorithms, driver assistance systems, Frequency, Image edge detection, image processing algorithm, image segmentation, Kalman filter, Kalman filters, laser beam applications, Laser fusion, laser scanner system, laser signal, Mechanical sensors, multi modal detection, optical scanners, optical tracking, range signal, Reflectivity, reflectivity signals, road borders detection, road borders tracking, road vehicles, sensor fusion, Signal processing algorithms},
	pages = {95--99},
	annote = {The following values have no corresponding Zotero field:alt-title: IEEE IV2003 Intelligent Vehicles Symposium. Proceedings (Cat. No.03TH8683)}
}

@WWW{flir_systems_flir_2017-1,
  author = {{FLIR® Systems}},
  title  = {{FLIR} {Systems} - {The} {World}'s {Sixth} {Sense}},
  year   = {2017},
  url    = {http://www.flir.com},
  type   = {Unpublished {Work}},
}

@article{fraundorfer_visual_2012,
	title = {Visual {Odometry} : {Part} {II}: {Matching}, {Robustness}, {Optimization}, and {Applications}},
	volume = {19},
	issn = {1070-9932},
	doi = {10.1109/MRA.2012.2182810},
	number = {2},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Fraundorfer, F. and Scaramuzza, D.},
	year = {2012},
	keywords = {aerial application, cameras, Computer vision, digital camera, distance measurement, driving autonomous car, easier-to-manufacture solution, egomotion estimation, error propagation, Estimation, Feature extraction, ground application, Lidar-based system, loop closures, motion drift, motion estimation, Odemtry, Optimization, radar sensor, robot vision, robotic system, Robust control, robust estimation, SLAM (robots), SLAM algorithm, space application, Tutorials, underwater application, visual odometry, VIsualization, VO pipeline},
	pages = {78--90},
	annote = {The following values have no corresponding Zotero field:number: 2}
}

@article{guo_road_2015,
	title = {Road {Edge} {Recognition} {Using} the {Stripe} {Hough} {Transform} {From} {Millimeter}-{Wave} {Radar} {Images}},
	volume = {16},
	issn = {1524-9050},
	doi = {10.1109/TITS.2014.2342875},
	number = {2},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Guo, K. Y. and Hoare, E. G. and Jasteh, D. and Sheng, X. Q. and Gashinova, M.},
	year = {2015},
	keywords = {Adaptive cruise control (ACC), automotive imaging system, feature extraction, Gray-scale, Hough transform (HT), Hough transforms, Image edge detection, Laser radar, lighting condition, millimeter-wave (MMW) radars, millimeter-wave radar image, MMW radar image, object recognition, optical camera, radar imaging, ranging capability, road edge recognition, road feature recognition, road features, road path geometry extraction, road traffic, Roads, stripe Hough transform, traffic engineering computing, weather condition},
	pages = {825--833},
	annote = {The following values have no corresponding Zotero field:number: 2}
}

@inproceedings{jia_mobile_2011,
	title = {Mobile robot 3D map building based on laser ranging and stereovision},
	isbn = {2152-7431},
	doi = {10.1109/ICMA.2011.5986248},
	booktitle = {2011 {IEEE} {International} {Conference} on {Mechatronics} and {Automation}},
	author = {Jia, S. and Cui, W. and Li, X. and Shen, H. and Sheng, J.},
	year = {2011},
	keywords = {3D map building, 3D spatial model, Bayes methods, Bayesian filter, Bayesian methods, Bayesian rule, binocular stereo vision sensor, Buildings, dynamic occupancy grid map modeling technique, environment perception, feature extraction, filtering theory, image texture, image texture extraction, Laser modes, laser range finder, laser ranging, local map temporal integration approach, map building, measurement uncertainty, mobile robot, mobile robots, Pioneer robot, Robot sensing systems, robot vision, stereo image processing, Stereo vision, stereovision, Three dimensional displays},
	pages = {1774--1779},
	annote = {The following values have no corresponding Zotero field:alt-title: 2011 IEEE International Conference on Mechatronics and Automation}
}

@inproceedings{jing_3-d_2006,
	title = {A 3-{D} {Real}-{Time} {Road} {Edge} {Detection} {System} for {Automated} {Smart} {Car} {Control}},
	doi = {10.1109/ICNSC.2006.1673255},
	booktitle = {2006 {IEEE} {International} {Conference} on {Networking}, {Sensing} and {Control}},
	author = {Jing, Li and Liu, R. and Xuemin, Chen and Huichun, Xing and Ying, Wang and Chienping, Kao and Li, Xiao},
	year = {2006},
	keywords = {3D real-time road-trail edge detection system, automated highways, automated smart car control, Automatic control, automobiles, Control systems, CW radar, edge detection, FM radar, four-dimensional profile detection system, Frequency, frequency-modulated continuous-wave radars, Image edge detection, Meteorological radar, mobile robots, optical images, Radar detection, Radar imaging, radar technology, Radar tracking, Real time systems, road material, Roads, three-dimensional road images},
	pages = {837--841},
	annote = {The following values have no corresponding Zotero field:alt-title: 2006 IEEE International Conference on Networking, Sensing and Control}
}

@article{kroeger_fast_2016,
	title = {Fast {Optical} {Flow} using {Dense} {Inverse} {Search}},
	volume = {abs/1603.03590},
	url = {http://arxiv.org/abs/1603.03590},
	journal = {CoRR},
	author = {Kroeger, Till and Timofte, Radu and Dai, Dengxin and Gool, Luc J. Van},
	year = {2016}
}

@Article{li_place_2016,
  author  = {Li, Qin and Li, Ke and You, Xiong and Bu, Shuhui and Liu, Zhenbao},
  title   = {Place recognition based on deep feature and adaptive weighting of similarity matrix},
  journal = {Neurocomputing},
  year    = {2016},
  issn    = {0925-2312},
  doi     = {10.1016/j.neucom.2016.03.029},
}

@inproceedings{liu_mobile_2012,
	title = {Mobile robot instant indoor map building and localization using 2D laser scanning data},
	isbn = {2325-0909},
	doi = {10.1109/ICSSE.2012.6257203},
	booktitle = {2012 {International} {Conference} on {System} {Science} and {Engineering} ({ICSSE})},
	author = {Liu, Y. and Sun, Y.},
	year = {2012},
	keywords = {2D laser scanning data, Buildings, EKF-based pose estimation, grid based representation, indoor environment, Indoor environments, instant indoor map building, instant map building, Kalman filters, laser ranging, laser scanning, Lasers, least square algorithm, least squares approximations, localizatoin, metric-based iterative closest point technique, Mobile communication, mobile robot, mobile robot map building, mobile robots, omnidirectional map building, pose estimation, robot autonomous localization system, Robot kinematics, scanning data matching, semistructured indoor environment, SLAM (robots)},
	pages = {339--344},
	annote = {The following values have no corresponding Zotero field:alt-title: 2012 International Conference on System Science and Engineering (ICSSE)}
}

@inproceedings{lukierski_rapid_2015,
	title = {Rapid free-space mapping from a single omnidirectional camera},
	doi = {10.1109/ECMR.2015.7324222},
	booktitle = {Mobile {Robots} ({ECMR}), 2015 {European} {Conference} on},
	author = {Lukierski, R. and Leutenegger, S. and Davison, A. J.},
	year = {2015},
	keywords = {2D occupancy map, 3D semidense reconstruction, Cameras, computer vision, feature-based mapping, feature-based visual SLAM procedure, global free-space reasoning, image matching, image reconstruction, image sequences, intelligent guided navigation, matching step, mobile robots, omni image sequence, rapid free-space mapping, robot, robot vision, Robot vision systems, robots, simultaneous localisation and mapping, single omnidirectional camera, SLAM, SLAM (robots), Standards, Three-dimensional displays, visibility reasoning},
	pages = {1--8},
	annote = {The following values have no corresponding Zotero field:alt-title: Mobile Robots (ECMR), 2015 European Conference on}
}

@inproceedings{mccarthy_performance_2004,
	title = {Performance of optical flow techniques for indoor navigation with a mobile robot},
	volume = {5},
	isbn = {1050-4729},
	doi = {10.1109/ROBOT.2004.1302525},
	booktitle = {Robotics and {Automation}, 2004. {Proceedings}. {ICRA} '04. 2004 {IEEE} {International} {Conference} on},
	author = {McCarthy, C. and Bames, N.},
	year = {2004},
	keywords = {Australia, Biomedical optical imaging, Computer science, Image motion analysis, Image sequences, Mobile robots, Navigation, Optical filters, Optical sensors, Software engineering},
	pages = {5093--5098},
	annote = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation, 2004. Proceedings. ICRA '04. 2004 IEEE International Conference on}
}

@inproceedings{mendes_exploiting_2016,
	title = {Exploiting fully convolutional neural networks for fast road detection},
	doi = {10.1109/ICRA.2016.7487486},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Mendes, C. C. T. and Frémont, V. and Wolf, D. F.},
	year = {2016},
	keywords = {autonomous navigation systems, autonomous vehicle, Benchmark testing, Computer architecture, contextual window sizes, convolutional neural network architecture, image classification, maneuvers, mobile robots, motion control, network-in-network architecture, neural net architecture, Neural networks, neurocontrollers, NiN architecture, object detection, path planning, road area, road vehicles, Roads, robot vision, Standards, Training, visual road detection, Visualization},
	pages = {3174--3179},
	annote = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE International Conference on Robotics and Automation (ICRA)}
}

@inproceedings{nikolova_segmentation_2000,
	title = {Segmentation of a road from a vehicle-mounted radar and accuracy of the estimation},
	doi = {10.1109/IVS.2000.898356},
	booktitle = {Proceedings of the {IEEE} {Intelligent} {Vehicles} {Symposium} 2000 ({Cat}. {No}.00TH8511)},
	author = {Nikolova, M. and Hero, A.},
	year = {2000},
	keywords = {automated highways, Automatic control, automatic road following, Backscatter, collision avoidance, Cramer Rao bound, Fisher information matrix, image segmentation, maneuver control, Millimeter wave radar, radar backscatter, Radar imaging, Radar scattering, Roads, Robustness, segmentation, vehicle-mounted radar, Vehicles},
	pages = {284--289},
	annote = {The following values have no corresponding Zotero field:alt-title: Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511)}
}

@WWW{nvidia_corporation_embedded_2017-1,
  author = {{NVIDIA Corporation}},
  title  = {Embedded {Systems}},
  year   = {2017},
  url    = {http://www.nvidia.com/object/embedded-systems-dev-kits-modules.html},
  type   = {Unpublished {Work}},
}

@WWW{nvidia_corporation_nvidia_2017-1,
  author = {{NVIDIA Corporation}},
  title  = {{NVIDIA} {DRIVE} auto-pilot and cockpit computers},
  year   = {2017},
  url    = {http://www.nvidia.com/object/drive-automotive-technology.html},
  type   = {Unpublished {Work}},
}

@article{scaramuzza_visual_2011,
	title = {Visual {Odometry} [{Tutorial}]},
	volume = {18},
	issn = {1070-9932},
	doi = {10.1109/MRA.2011.943233},
	number = {4},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Scaramuzza, D. and Fraundorfer, F.},
	year = {2011},
	keywords = {augmented reality, automotive, cameras, egomotion estimation, illumination, image texture, motion estimation, multiple cameras, onboard cameras, pose estimation, road vehicles, robotics, robots, scene overlap, single camera, vehicle, visual odometry, wearable computing, wheel odometry, wheels},
	pages = {80--92},
	annote = {The following values have no corresponding Zotero field:number: 4}
}

@inproceedings{sunderhauf_performance_2015,
	title = {On the performance of {ConvNet} features for place recognition},
	doi = {10.1109/IROS.2015.7353986},
	booktitle = {Intelligent {Robots} and {Systems} ({IROS}), 2015 {IEEE}/{RSJ} {International} {Conference} on},
	author = {Sünderhauf, N. and Shirazi, S. and Dayoub, F. and Upcroft, B. and Milford, M.},
	year = {2015},
	keywords = {computer vision, condition-invariance, ConvNet feature, convolutional network, Feature extraction, locality-sensitive hashing, neural nets, object recognition, optimisation, optimization technique, place recognition, Real-time systems, robot vision, robotic field, Robustness, search problems, semantic place categorization, semantic search space partitioning, Semantics, SLAM, SLAM (robots), viewpoint-invariance, visual navigation, Visualization},
	pages = {4297--4304},
	annote = {The following values have no corresponding Zotero field:alt-title: Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on}
}

@InProceedings{teichmann_multinet:_2016,
  author    = {Marvin Teichmann and Michael Weber and Marius Zollner and Roberto Cipolla and Raquel Urtasun},
  title     = {MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving},
  booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
  year      = {2018},
  publisher = {{IEEE}},
  month     = jun,
  pages     = {1013--1020},
  doi       = {10.1109/IVS.2018.8500504},
  issn      = {1931-0587},
  keywords  = {driver information systems;image classification;image segmentation;inference mechanisms;object detection;real-time joint semantic reasoning;autonomous driving;semantic segmentation;unified architecture;KITTI dataset;joint classification;MultiNet;Task analysis;Decoding;Semantics;Computer architecture;Proposals;Feature extraction;Microprocessors},
}

@inproceedings{ulrich_appearance-based_2000,
	title = {Appearance-based place recognition for topological localization},
	volume = {2},
	isbn = {1050-4729},
	doi = {10.1109/ROBOT.2000.844734},
	booktitle = {Robotics and {Automation}, 2000. {Proceedings}. {ICRA} '00. {IEEE} {International} {Conference} on},
	author = {Ulrich, I. and Nourbakhsh, I.},
	year = {2000},
	keywords = {appearance-based place recognition, Color, cross-sequence tests, Histograms, image classification, image histogram matching, image matching, learning (artificial intelligence), Machine vision, mobile robot position tracking, mobile robots, nearest-neighbor learning, panoramic vision system, real-time color image classification, real-time systems, Robot kinematics, Robot sensing systems, robot vision, Sensor systems, System testing, topological localization, topology, Uncertainty, Voting, voting scheme},
	pages = {1023--1029 vol.2},
	annote = {The following values have no corresponding Zotero field:alt-title: Robotics and Automation, 2000. Proceedings. ICRA '00. IEEE International Conference on}
}

@WWW{velodyne_lidar_velodyne_2017-1,
  author = {{Velodyne LiDAR}},
  title  = {Velodyne {LiDAR}},
  year   = {2017},
  url    = {http://velodynelidar.com/},
  type   = {Unpublished {Work}},
}

@inproceedings{visin_reseg:_2016,
	title = {{ReSeg}: {A} {Recurrent} {Neural} {Network}-{Based} {Model} for {Semantic} {Segmentation}},
	doi = {10.1109/CVPRW.2016.60},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Visin, F. and Romero, A. and Cho, K. and Matteucci, M. and Ciccone, M. and Kastner, K. and Bengio, Y. and Courville, A.},
	year = {2016},
	keywords = {Computer architecture, Context modeling, convolutional neural networks, feature extraction, image classification, image resolution, image segmentation, recurrent neural nets, recurrent neural network-based model, Recurrent neural networks, ReNet, ReSeg architecture, semantic segmentation, Semantics, structured prediction architecture},
	pages = {426--433},
	annote = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}
}

@WWW{volvo_car_corporation_autonomous_2017-1,
  author = {{Volvo Car Corporation}},
  title  = {Autonomous {Driving} {\textbar} {Intellisafe} {\textbar} {Volvo} {Cars}},
  year   = {2017},
  url    = {http://www.volvocars.com/intl/about/our-innovation-brands/intellisafe/autonomous-driving},
  type   = {Unpublished {Work}},
}

@article{wijesoma_road-boundary_2004,
	title = {Road-boundary detection and tracking using ladar sensing},
	volume = {20},
	issn = {1042-296X},
	doi = {10.1109/TRA.2004.825269},
	number = {3},
	journal = {IEEE Transactions on Robotics and Automation},
	author = {Wijesoma, W. S. and Kodagoda, K. R. S. and Balasuriya, A. P.},
	year = {2004},
	keywords = {advanced driver assistance system, autonomous vehicle navigation system, Autonomous vehicles, computerised navigation, driver information systems, extended Kalman filter, feature extraction, Kalman filters, ladar measurement system, Laser radar, laser ranging, Millimeter wave radar, Mobile robots, Navigation, nonlinear filters, optical radar, Radar detection, Radar tracking, range readings, Remotely operated vehicles, road boundary detection, road lane tracking, road vehicles, roads, robot sensing systems, traffic engineering computing, Vehicle detection, Vehicle driving},
	pages = {456--464},
	annote = {The following values have no corresponding Zotero field:number: 3}
}

@article{wu_novel_2016,
	title = {A {Novel} {Line} {Space} {Voting} {Method} for {Vanishing}-{Point} {Detection} of {General} {Road} {Images}},
	volume = {16},
	issn = {1424-8220 (Electronic) 1424-8220 (Linking)},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/27347947},
	doi = {10.3390/s16070948},
	number = {7},
	journal = {Sensors (Basel)},
	author = {Wu, Z. and Fu, W. and Xue, R. and Wang, W.},
	year = {2016},
	keywords = {line space voting, Lsd, texture orientation, vanishing point detection},
	annote = {The following values have no corresponding Zotero field:auth-address: School of Mechanical and Precision Instrumental Engineering, Xi'an University of Technology, Xi'an 710048, China. wuzs2005@163.com. School of Mechanical and Precision Instrumental Engineering, Xi'an University of Technology, Xi'an 710048, China. weipingf@xaut.edu.cn. School of Information Engineering, Xizang Minzu University, XianYang 712082, China. joywinjam@163.com. School of Mechanical and Precision Instrumental Engineering, Xi'an University of Technology, Xi'an 710048, China. wangwen@xaut.edu.cn.number: 7accession-num: 27347947},
	annote = {Wu, ZongshengFu, WeipingXue, RuWang, WenengSwitzerland2016/06/28 06:00Sensors (Basel). 2016 Jun 23;16(7). pii: E948. doi: 10.3390/s16070948.}
}

@inproceedings{yao_road_2012,
	title = {Road curb detection using 3D lidar and integral laser points for intelligent vehicles},
	doi = {10.1109/SCIS-ISIS.2012.6505310},
	booktitle = {The 6th {International} {Conference} on {Soft} {Computing} and {Intelligent} {Systems}, and {The} 13th {International} {Symposium} on {Advanced} {Intelligence} {Systems}},
	author = {Yao, W. and Deng, Z. and Zhou, L.},
	year = {2012},
	keywords = {3D lidar, automated highways, ground plane, highways roads, ILP, integral laser points, intelligent vehicles, intensity data, laser beam applications, line segment, optical radar, RANSAC algorithm, road curb detection, road vehicles, urban roads},
	pages = {100--105},
	annote = {The following values have no corresponding Zotero field:alt-title: The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems}
}

@inproceedings{yi_afocal_2015,
	title = {Afocal optical flow sensor for mobile robot odometry},
	isbn = {2093-7121},
	doi = {10.1109/ICCAS.2015.7364858},
	booktitle = {Control, {Automation} and {Systems} ({ICCAS}), 2015 15th {International} {Conference} on},
	author = {Yi, D. H. and Lee, T. J. and Cho, D. I.},
	year = {2015},
	keywords = {afocal optical flow sensor, AOFS, bare laminate floor, distance measurement, encoders, Estimation, Floors, gyroscope, height variation, image sensors, image sequences, Laminates, localization error, Mice, mobile robot navigation, mobile robot odometry, mobile robots, navigation, odometry, Optical filters, optical flow sensor, Optical imaging, Optical sensors, robot moving distance estimation, robot vision, vertical-height sensitivity, wheel slippage},
	pages = {1393--1397},
	annote = {The following values have no corresponding Zotero field:alt-title: Control, Automation and Systems (ICCAS), 2015 15th International Conference on}
}

@Article{yousif_overview_2015,
  author  = {Yousif, Khalid and Bab-Hadiashar, Alireza and Hoseinnezhad, Reza},
  title   = {An {Overview} to {Visual} {Odometry} and {Visual} {SLAM}: {Applications} to {Mobile} {Robotics}},
  journal = {Intelligent Industrial Systems},
  year    = {2015},
  volume  = {1},
  number  = {4},
  pages   = {289--311},
  issn    = {2199-854X},
  doi     = {10.1007/s40903-015-0032-7},
  annote  = {The following values have no corresponding Zotero field:number: 4label: Yousif2015work-type: journal article},
}

@InCollection{zeiler_visualizing_2014,
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  title     = {Visualizing and {Understanding} {Convolutional} {Networks}},
  booktitle = {Computer {Vision} – {ECCV} 2014: 13th {European} {Conference}, {Zurich}, {Switzerland}, {September} 6-12, 2014, {Proceedings}, {Part} {I}},
  year      = {2014},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  publisher = {Springer International Publishing},
  isbn      = {978-3-319-10590-1},
  pages     = {818--833},
  doi       = {10.1007/978-3-319-10590-1_53},
  address   = {Cham},
  annote    = {The following values have no corresponding Zotero field:label: Zeiler2014electronic-resource-num: 10.1007/978-3-319-10590-1\_53},
}

@inproceedings{zhang_lidar-based_2010,
	title = {{LIDAR}-based road and road-edge detection},
	isbn = {1931-0587},
	doi = {10.1109/IVS.2010.5548134},
	booktitle = {2010 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	author = {Zhang, W.},
	year = {2010},
	keywords = {Algorithm design and analysis, DARPA Urban Challenge, Data mining, Defense Advanced Research Projects Agency, edge detection, elevation based signal processing, filtering technique, filtering theory, Laser radar, LIDAR based road edge detection, Mobile robots, optical radar, pattern recognition, Remotely operated vehicles, Road vehicles, Robustness, Sensor arrays, Signal processing, traffic information systems, Vehicle detection},
	pages = {845--848},
	annote = {The following values have no corresponding Zotero field:alt-title: 2010 IEEE Intelligent Vehicles Symposium}
}

@InProceedings{zhao_visual_2015,
  author    = {Boxin Zhao and Tianjiang Hu and Lincheng Shen},
  title     = {Visual odometry - A review of approaches},
  booktitle = {2015 {IEEE} International Conference on Information and Automation},
  year      = {2015},
  publisher = {{IEEE}},
  month     = aug,
  pages     = {2569--2573},
  doi       = {10.1109/ICInfA.2015.7279718},
  annote    = {The following values have no corresponding Zotero field:alt-title: Information and Automation, 2015 IEEE International Conference on},
  keywords  = {camera pose estimation, Cameras, Computer vision, distance measurement, Estimation, feature description, feature detection, feature extraction, feature matching, feature tracking, image matching, Optimization, pose estimation, robot autonomous navigation, Robot Localization, visual odometry, Visual SLAM, Visualization},
}

@inproceedings{zhao_curb_2012,
	title = {Curb detection and tracking using 3D-{LIDAR} scanner},
	isbn = {1522-4880},
	doi = {10.1109/ICIP.2012.6466890},
	booktitle = {2012 19th {IEEE} {International} {Conference} on {Image} {Processing}},
	author = {Zhao, G. and Yuan, J.},
	year = {2012},
	keywords = {3D-LIDAR, 3D-LIDAR scanner, autonomous ground vehicle, Cameras, collision avoidance, computational geometry, Computational modeling, Covariance matrix, Curb detection, curb point selection, curb tracking, downtown Dearborn, elevation difference, Ford Research campus, gradient value, mobile robots, moving pedestrians, moving vehicles, normal orientation, object detection, object tracking, optical radar, parabola model, particle filter, particle filtering (numerical methods), pointcloud, road accidents, road curb detection method, road curvature changes, road traffic, road vehicles, Roads, robot vision, Sensors, short-term memory technique, smoothing methods, spatial cue, spatial cues, static obstacles, traffic accidents reduction, Transforms, Vehicles},
	pages = {437--440},
	annote = {The following values have no corresponding Zotero field:alt-title: 2012 19th IEEE International Conference on Image Processing}
}

@Article{zhu_visual_2014,
  author   = {Zhu, Min and Liu, Yisha and Zhuang, Yan and Hu, Huosheng},
  title    = {Visual {Campus} {Road} {Detection} for an {UGV} using {Fast} {Scene} {Segmentation} and {Rapid} {Vanishing} {Point} {Estimation}},
  journal  = {IFAC Proceedings Volumes},
  year     = {2014},
  volume   = {47},
  number   = {3},
  pages    = {11898--11903},
  issn     = {1474-6670},
  doi      = {10.3182/20140824-6-ZA-1003.00635},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667016435093},
  annote   = {The following values have no corresponding Zotero field:number: 3},
  keywords = {Road detection, scene segmentation, unmanned ground vehicles, vanishing point detection},
}

@Article{brostow_semantic_2009,
  author   = {Brostow, Gabriel J. and Fauqueur, Julien and Cipolla, Roberto},
  title    = {Semantic object classes in video: {A} high-definition ground truth database},
  journal  = {Pattern Recognition Letters},
  year     = {2009},
  volume   = {30},
  number   = {2},
  pages    = {88--97},
  issn     = {0167-8655},
  doi      = {10.1016/j.patrec.2008.04.005},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865508001220},
  annote   = {The following values have no corresponding Zotero field:number: 2},
  keywords = {Label propagation, Object recognition, Semantic segmentation, Video database, Video understanding},
}

@WWW{palazzi_github_2017,
  author = {Palazzi, Andrea},
  title  = {{GitHub} - ndrplz/dilation-tensorflow},
  year   = {2017},
  url    = {https://github.com/ndrplz/dilation-tensorflow},
  type   = {Unpublished {Work}},
}

@WWW{anthony_github_2017,
  author = {Anthony, Thomas},
  title  = {{GitHub} - thomasantony/{CarND}-{P}04-{Advanced}-{Lane}-{Lines}},
  year   = {2017},
  url    = {https://github.com/thomasantony/CarND-P04-Advanced-Lane-Lines},
  type   = {Unpublished {Work}},
}

@InProceedings{ricaud_general_2014,
  author    = {Ricaud, Bruno and Stanciulescu, Bogdan and Breheret, Amaury},
  title     = {General Road Detection Algorithm},
  booktitle = {Proceedings of the 3rd International Conference on Pattern Recognition Applications and Methods},
  year      = {2014},
  series    = {ICPRAM 2014},
  publisher = {SCITEPRESS - Science and Technology Publications, Lda},
  location  = {ESEO, Angers, Loire Valley, France},
  isbn      = {978-989-758-018-5},
  pages     = {825--830},
  doi       = {10.5220/0004935208250830},
  acmid     = {2971070},
  address   = {Portugal},
  keywords  = {Autonomous Driving, Computation, Gabor, Image Processing, Offroad, Vanishing Point},
  numpages  = {6},
}

@inproceedings{ofjall_visual_2016,
	title = {Visual autonomous road following by symbiotic online learning},
	doi = {10.1109/IVS.2016.7535377},
	booktitle = {2016 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Öfjäll, K. and Felsberg, M. and Robinson, A.},
	year = {2016},
	keywords = {Automobiles, autonomous driving, control engineering computing, driving assistance systems, Hebbian learning, instantaneous reinforcement learning, Kernel, learning by demonstration, Learning systems, mobile robots, online perception-action learning, predictive coding, qHebb-learning, RC-car model, regression analysis, road vehicles, road visual appearance, Roads, robot vision, self-assessment method, self-reinforcement learning, SOLAR algorithm, supervised learning, symbiotic online learning of associations and regression, traffic engineering computing, Training, unsupervised learning, visual autonomous road following, Visualization},
	pages = {136--143},
	annote = {The following values have no corresponding Zotero field:alt-title: 2016 IEEE Intelligent Vehicles Symposium (IV)}
}

@Book{international_student_nodate,
  author = {{International, S. A. E.}},
  title  = {Student {Events} - {Events} - {Collegiate} {Design} {Series}},
  year   = {2019},
  url    = {https://www.sae.org/attend/student-events/},
}

@WWW{berkeleyvision_caffe_2016-1,
  author = {{BerkeleyVision}},
  title  = {Caffe {\textbackslash}textbar {Deep} {Learning} {Framework}},
  year   = {2016},
  url    = {https://caffe.berkeleyvision.org/},
}

@article{iandola_squeezenet:_2016-1,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and{\textbackslash}textless 0.5 {MB} model size},
	journal = {arXiv preprint arXiv:1602.07360},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	year = {2016}
}

@WWW{corporation_autonomous_2017,
  author = {{Volvo Car Corporation}},
  title  = {Autonomous {Driving} {\textbackslash}textbar {Intellisafe} {\textbackslash}textbar {Volvo} {Cars}},
  year   = {2017},
  url    = {https://www.volvocars.com/intl/discover-volvo/autonomous-drive},
}

@Book{commission_iec_2017,
  author = {{International Electrotechnical Commission}},
  title  = {{IEC} 61851-1:2017},
  year   = {2017},
  url    = {https://webstore.iec.ch/publication/33644},
  month  = feb,
}

@Book{ltd_veefilelectric_2015,
  author    = {Tritium},
  title     = {Veefil–{Electric} vehicle fast charger instruction manual},
  year      = {2015},
  language  = {en},
  publisher = {Tritium Pty Ltd},
  url       = {https://fccid.io/2AFHX-TRI935001US/User-Manual/Users-Manual-3059907.pdf},
  urldate   = {2018-02-02},
}

@Book{wa_rac_nodate,
  author   = {{RAC WA}},
  title    = {{RAC} {Electric} {Highway}},
  year     = {2018},
  url      = {http://electrichighway.rac.com.au/},
  abstract = {The RAC Electric Highway®, will be a network of publicly accessible electric vehicle fast-charging DC stations located between Perth and the South West.},
}

@Book{v_ccs_2018,
  author   = {{Charging Interface Initiative e.V}},
  title    = {{CCS} {Specification}},
  year     = {2018},
  language = {en},
  url      = {http://www.charinev.org/ccs-at-a-glance/ccs-specification/},
  month    = jan,
}

@Book{development_green_2018,
  author = {{Department of Infrastructure {and} Regional Development}},
  title  = {Green {Vehicle} {Guide}},
  year   = {2018},
  url    = {https://www.greenvehicleguide.gov.au/},
}

@Book{alliance_ocpp_nodate,
  author = {{Open Charge Alliance}},
  title  = {{OCPP} 1.6, {OCPP}, {Protocols} - {Open} {Charge} {Alliance}},
  year   = {2019},
  url    = {http://www.openchargealliance.org/protocols/ocpp/ocpp-16/},
}

@Book{development_vehicle_2016,
  author    = {{Department of Infrastructure {and} Regional Development}},
  title     = {Vehicle emissions standards for cleaner air},
  year      = {2016},
  language  = {English},
  publisher = {Australian Government},
  url       = {https://infrastructure.gov.au/roads/environment/forum/files/Vehicle_Noxious_Emissions_RIS.pdf},
  urldate   = {2018-02-02},
  month     = dec,
}

@Book{international_formula_nodate,
  author = {{SAE International}},
  title  = {Formula {SAE} - {SAE} {Collegiate} {Design} {Series} - {Students} - {SAE} {International}},
  year   = {2019},
  url    = {http://students.sae.org/cds/formulaseries/},
}

@Book{autonomoustuff_ibeo_nodate-1,
  author   = {{AutonomouStuff}},
  title    = {ibeo {Standard} {Four} {Layer} {Multi}-{Echo} {LUX} {Sensor} {\textbackslash}textbar {LiDAR} {\textbackslash}textbar {Product}},
  year     = {2018},
  url      = {https://autonomoustuff.com/product/ibeo-lux-standard/},
  abstract = {The ibeo LUX laser scanner is a unique full-range sensor used for object detection and classification to support ADAS applications.},
}

@Book{developers_protocol_nodate,
  author   = {{Google Developers}},
  title    = {Protocol {Buffers}},
  year     = {2018},
  url      = {https://developers.google.com/protocol-buffers/},
  abstract = {Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.},
}

@Book{xsens_mti_nodate-1,
  author   = {{Xsens}},
  title    = {{MTi} (legacy product) - {Products}},
  year     = {2018},
  url      = {https://www.xsens.com/products/mti/},
  abstract = {The MTi is discontinued since August 31st, 2014. Please refer...},
}

@Book{association_nmea_2002,
  author    = {{National Marine Electronics Association}},
  title     = {{NMEA} 0183–{Standard} for interfacing marine electronic devices},
  year      = {2002},
  publisher = {NMEA},
}

@Book{ltd_columbus_nodate,
  author = {{Victory Co Ltd}},
  title  = {Columbus {V}-800},
  year   = {2018},
  url    = {http://www.cbgps.com/v800/index_en.htm},
}

@Thesis{drage_development_2013,
  author   = {Drage, Thomas H.},
  title    = {Development of a {Navigation} {Control} {System} for an {Autonomous} {Formula} {SAE}-{Electric} {Race} {Car}},
  type     = {{BE} {Thesis}},
  year     = {2013},
  language = {en},
  month    = nov,
  url      = {http://robotics.ee.uwa.edu.au/theses/2013-REV-Navigation-Drage.pdf},
  address  = {Perth, Australia},
  school   = {The University of Western Australia},
}

@Article{chu_local_2012,
  author        = {Keonyup Chu and Minchae Lee and Myoungho Sunwoo},
  title         = {Local {Path} {Planning} for {Off}-{Road} {Autonomous} {Driving} {With} {Avoidance} of {Static} {Obstacles}},
  journal       = {IEEE Transactions on Intelligent Transportation Systems},
  year          = {2012},
  volume        = {13},
  number        = {4},
  month         = dec,
  pages         = {1599--1616},
  issn          = {1524-9050},
  doi           = {10.1109/TITS.2012.2198214},
  __markedentry = {[Psykie:6]},
  abstract      = {In this paper, a real-time path-planning algorithm that provides an optimal path for off-road autonomous driving with static obstacles avoidance is presented. The proposed planning algorithm computes a path based on a set of predefined waypoints. The predefined waypoints provide the base frame of a curvilinear coordinate system to generate path candidates for autonomous vehicle path planning. Each candidate is converted to a Cartesian coordinate system and evaluated using obstacle data. To select the optimal path, the priority of each path is determined by considering the path safety cost, path smoothness, and path consistency. The proposed path-planning algorithms were applied to the autonomous vehicle A1, which won the 2010 Autonomous Vehicle Competition organized by the Hyundai-Kia Automotive Group in Korea.},
  file          = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/9XL92ZNZ/6203588.html/\:text/html\:PDF:PDF},
  keywords      = {Autonomous vehicle, autonomous vehicle competition, autonomous vehicle path planning, collision avoidance, Collision avoidance, curvilinear coordinate system, Hyundai-Kia automotive group, Korea, local path planning, Mobile robots, nonholonomic constraints, obstacle avoidance, off-road autonomous driving, off-road vehicles, path consistency, Path planning, path safety cost, path smoothness, real-time path planning, real-time path-planning algorithm, Real-time systems, static obstacle avoidance, static obstacles},
}

@InProceedings{wang_arc-length_2002,
  author        = {Wang, Hongling and Kearney, Joseph and Atkinson, Kendall},
  title         = {Arc-length parameterized spline curves for real-time simulation},
  booktitle     = {Proc. 5th {International} {Conference} on {Curves} and {Surfaces}},
  year          = {2002},
  __markedentry = {[Psykie:6]},
}

@Book{brent_algorithms_1973,
  author        = {Brent, Richard P.},
  title         = {Algorithms for minimization without derivatives},
  year          = {1973},
  publisher     = {Courier Corporation},
  isbn          = {0-486-14368-6},
  __markedentry = {[Psykie:6]},
  address       = {Englewood Cliffs},
}

@InProceedings{xu_accurate_2008,
  author        = {Jinting Xu and Weijun Liu and Hongyou Bian and Lun Li},
  title         = {Accurate and {Efficient} {Algorithm} for the {Closest} {Point} on a {Parametric} {Curve}},
  booktitle     = {2008 {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
  year          = {2008},
  volume        = {2},
  month         = dec,
  pages         = {1000--1002},
  doi           = {10.1109/CSSE.2008.618},
  __markedentry = {[Psykie:6]},
  abstract      = {This paper presents an accurate and efficient method for computation of the closest point on parametric curves. This problem is firstly formulated in terms of solution of a polynomial equation expressed in Bernstein basis, and then based on subdivision relying on the convex hull property of Bezier curve and the recursive bintree decomposition on the parameter domain, a novel solution method is proposed. The computation of closest point is shown to be equivalent to the geometrically intuitive intersection of a curve with the parameter line. Finally, by comparing the distances between the test point and the candidate points, the closest point is found. An example illustrates the feasibility of this method.},
  file          = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/NLFD2MI4/4722219.html/\:text/html\:PDF:PDF},
  keywords      = {Automation, Bernstein basis, Bezier curve, closest point computation, computational geometry, convex hull property, curve, Equations, Informatics, Laboratories, Minimization methods, parametric curves, point inversion, Point projection, polynomial equation, polynomials, Polynomials, recursive bintree decomposition, recursive estimation, Spline, Surface reconstruction, Surface topography, Testing},
}

@Article{canny_computational_1986,
  author   = {John Canny},
  title    = {A {Computational} {Approach} to {Edge} {Detection}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {1986},
  volume   = {PAMI-8},
  number   = {6},
  month    = nov,
  pages    = {679--698},
  issn     = {0162-8828},
  doi      = {10.1109/TPAMI.1986.4767851},
  abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/5IHSQCHV/4767851.html/\:text/html\:PDF:PDF},
  keywords = {Detectors, Edge detection, feature extraction, Feature extraction, Gaussian approximation, Image edge detection, image processing, machine vision, Machine vision, multiscale image analysis, Performance analysis, Shape measurement, Signal synthesis, Signal to noise ratio, Uncertainty},
}

@Patent{hough_method_1962,
  author      = {Hough, Paul V. C.},
  title       = {Method and means for recognizing complex patterns},
  number      = {US3069654A},
  year        = {1962},
  month       = dec,
  url         = {https://patents.google.com/patent/US3069654A/en},
  urldate     = {2019-06-01},
  annote      = {Classifications G01T5/02: Processing of tracks; Analysis of tracks G06K9/4633: Detecting partial patterns, e.g. edges or contours, or configurations, e.g. loops, corners, strokes, intersections by mapping characteristic values of the pattern into a parameter space, e.g. Hough transformation},
  assignee    = {Paul V C Hough},
  file        = {pdf\:PDF:pdf\:Fulltext PDF/\:C//\:/Users/Psykie/Zotero/storage/FMNS2T64/Hough - 1962 - Method and means for recognizing complex patterns.pdf/\:application/pdf\:PDF:PDF},
  keywords    = {framelet, line, microsecond, pulse, segment},
  nationality = {US},
}

@InProceedings{dosovitskiy_carla:_2017,
  author    = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  title     = {{CARLA}: {An} {Open} {Urban} {Driving} {Simulator}},
  booktitle = {Proceedings of the 1st {Annual} {Conference} on {Robot} {Learning}},
  year      = {2017},
  pages     = {1--16},
}

@WWW{logitech_logitech_2019,
  author  = {Logitech},
  title   = {Logitech {G}920 \& {G}29 {Driving} {Force} {Steering} {Wheels} \& {Pedals}},
  year    = {2019},
  url     = {https://www.logitechg.com/en-au/products/driving/driving-force-racing-wheel.html},
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:Logitech G920 & G29 Driving Force Steering Wheels & Pedals/\:C//\:/Users/Psykie/Zotero/storage/3UUI894J/driving-force-racing-wheel.html/\:text/html\:PDF:PDF},
}

@WWW{apollo_auto_collections_2019,
  author    = {{Apollo Auto}},
  title     = {Collections of {Apollo} {Platform} {Software}.},
  year      = {2019},
  url       = {https://github.com/ApolloAuto/apollo-platform},
  note      = {original-date: 2017-06-30T18:53:11Z},
  month     = may,
  urldate   = {2019-06-02},
  publisher = {Apollo Auto},
}

@Article{gietelink_development_2006,
  author   = {Gietelink, Olaf and Ploeg, Jeroen and Schutter, Bart De and Verhaegen, Michel},
  title    = {Development of advanced driver assistance systems with vehicle hardware-in-the-loop simulations},
  journal  = {Vehicle System Dynamics},
  year     = {2006},
  volume   = {44},
  number   = {7},
  month    = jul,
  pages    = {569--590},
  issn     = {0042-3114},
  doi      = {10.1080/00423110600563338},
  urldate  = {2019-06-02},
  abstract = {This paper presents a new method for the design and validation of advanced driver assistance systems (ADASs). With vehicle hardware-in-the-loop (VEHIL) simulations, the development process, and more specifically the validation phase, of intelligent vehicles is carried out safer, cheaper, and is more manageable. In the VEHIL laboratory, a full-scale ADAS-equipped vehicle is set up in a hardware-in-the-loop simulation environment, where a chassis dynamometer is used to emulate the road interaction and robot vehicles to represent other traffic. In this controlled environment, the performance and dependability of an ADAS is tested to great accuracy and reliability. The working principle and the added value of VEHIL are demonstrated with test results of an adaptive cruise control and a forward collision warning system. On the basis of the ‘V’ diagram, the position of VEHIL in the development process of ADASs is illustrated.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/ZXMV4HXF/00423110600563338.html/\:text/html\:PDF:PDF},
  keywords = {Adaptive Cruise control, Advanced driver assistance systems, Controller design and validation, Forward collision warning, Hardware-in-the-loop simulation},
}

@Article{bella_collision_2011,
  author     = {Bella, Francesco and Russo, Roberta},
  title      = {A {Collision} {Warning} {System} for rear-end collision: a driving simulator study},
  journal    = {Procedia - Social and Behavioral Sciences},
  year       = {2011},
  series     = {The {State} of the {Art} in the {European} {Quantitative} {Oriented} {Transportation} and {Logistics} {Research} – 14th {Euro} {Working} {Group} on {Transportation} \& 26th {Mini} {Euro} {Conference} \& 1st {European} {Scientific} {Conference} on {Air} {Transport}},
  volume     = {20},
  month      = jan,
  pages      = {676--686},
  issn       = {1877-0428},
  doi        = {10.1016/j.sbspro.2011.08.075},
  url        = {http://www.sciencedirect.com/science/article/pii/S1877042811014558},
  urldate    = {2019-06-02},
  abstract   = {Collision warning and collision avoidance systems are emerging automotive safety technologies that assist drivers in avoiding rear-end collisions. Their function is to allow the driver enough time to avoid the crash and yet avoid annoying the driver with alerts perceived as occurring too early or unnecessary. The aim of this paper was analyzing the driver's behavior in order to define effective driver assistance systems which can be readily accepted by the driver. A study was performed with an interactive fixed-base driving simulator. A sample of 32 drivers drove on a two-lane rural road. Four different driving traffic conditions were implemented. The data recorded during the tests were analyzed to assess the safety distances required by the driver during a car-following situation. Based on the risk perception of the driver a new collision warning algorithm was developed.},
  file       = {html\:PDF:html\:ScienceDirect Snapshot/\:C//\:/Users/Psykie/Zotero/storage/PL2XW94U/S1877042811014558.html/\:text/html\:PDF:PDF},
  keywords   = {Collision Warning, Driving simulator, Rear-end collisions},
  shorttitle = {A {Collision} {Warning} {System} for rear-end collision},
}

@InProceedings{hassan_reconfigurable_2013,
  author    = {Hassan, B. and Berssenbrügge, J. and Qaisi, I. Al and Stöcklein, J.},
  title     = {Reconfigurable driving simulator for testing and training of advanced driver assistance systems},
  booktitle = {2013 {IEEE} {International} {Symposium} on {Assembly} and {Manufacturing} ({ISAM})},
  year      = {2013},
  month     = jul,
  pages     = {337--339},
  doi       = {10.1109/ISAM.2013.6643472},
  abstract  = {The development and test of advanced driver assistance systems (ADAS)present a challenge due to their complexity and dependency on other vehicle systems, initial conditions and their environment. Testing ADAS under real conditions leads to significant efforts and costs. Therefore, virtual prototyping and simulation are widely used instruments for developing such complex systems. One of these useful virtual prototyping tools are driving simulators.Driving simulators are usually special purpose facilities, which are developed by suppliers individually for a specific application purpose. Most of the conventional driving simulators provide some flexibility for constructing new test scenarios, but do not allow users to change system arrangementor add or remove subsystems without indepth know-how of the driving simulator structure.This paper describes the concept and main idea of a reconfigurable driving simulator for testing ADAS. The key software, hardware, and resource components of the driving simulator are identified, the interfaces and the relationships between the key components are described, and the overall system structure for the reconfigurable driving simulator is introduced.},
  file      = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/PUXLL7ZU/6643472.html/\:text/html\:PDF:PDF},
  keywords  = {ADAS testing, ADAS training, advanced driver assistance systems, advanced driver assistance systems (ADAS), Analytical models, Automotive engineering, digital simulation, driver information systems, Driving simulation, Hardware, HiL, reconfigurable driving simulator, SiL, Software, Testing, Training, Vehicles, virtual prototyping, virtual simulation},
}

@InProceedings{chapron_new_2007,
  author    = {Chapron, Thomas and Colinot, Jean-Pierre},
  title     = {The {New} {PSA} {Peugeot}-{Citroen} {Advanced} {Driving} {Simulator} {Overall} {Design} and {Motion} {Cue} {Algorithm}},
  booktitle = {Proceedings of the Driving Simulation Conference, North America 2007 (DSC-NA 2007)},
  year      = {2007},
  month     = sep,
  url       = {https://trid.trb.org/view/899872},
  urldate   = {2019-06-02},
  address   = {Iowa City IA, United States},
  file      = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/8EY7GM98/899872.html/\:text/html\:PDF:PDF},
}

@Article{murano_development_2009,
  author   = {Murano, Takahiko and Yonekawa, Takashi and Aga, Masami and Nagiri, Sueharu},
  title    = {Development of {High}-{Performance} {Driving} {Simulator}},
  journal  = {SAE International Journal of Passenger Cars - Mechanical Systems},
  year     = {2009},
  language = {English},
  volume   = {2},
  number   = {1},
  month    = apr,
  pages    = {661--669},
  doi      = {10.4271/2009-01-0450},
  url      = {https://www.sae.org/publications/technical-papers/content/2009-01-0450/},
  urldate  = {2019-06-02},
  abstract = {A number of active safety systems are already developed to support drivers' decision and action to help avoid accidents, but further enhancement of those active safety systems cannot be accomplished without increasing our understanding on driver behaviors and their interaction with vehicle systems.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/9IF2Q9C9/2009-01-0450.html/\:text/html\:PDF:PDF},
}

@InProceedings{kading_advanced_1995,
  author    = {Wilfried Käding and Friedrich Hoffmeyer},
  title     = {The Advanced Daimler-Benz Driving Simulator},
  booktitle = {SAE Technical Paper},
  year      = {1995},
  publisher = {SAE International},
  month     = feb,
  doi       = {10.4271/950175},
  abstract  = {The Daimler-Benz Driving Simulator is a tool for research and development. It was put into operation 1984/85 and was enhanced since that time in many details, having gained a lot of experience with its use in many different applications.During a main modernization from autumn '93 until summer '94, the driving dynamics computer system as well as the visual system have been exchanged and the hydraulic motion system has been extended in lateral direction for improving motion simulation quality.},
}

@Article{brodsky_autonomous_2016,
  author     = {Brodsky, Jessica S.},
  title      = {Autonomous {Vehicle} {Regulation}: {How} an {Uncertain} {Legal} {Landscape} {May} {Hit} the {Brakes} on {Self}-{Driving} {Cars}},
  journal    = {Berkeley Technology Law Journal},
  year       = {2016},
  volume     = {31},
  pages      = {851},
  url        = {https://heinonline.org/HOL/Page?handle=hein.journals/berktech31&id=875&div=&collection=},
  file       = {html\:PDF:html\:Autonomous Vehicle Regulation//\: How an Uncertain Legal Landscape May Hit the Brakes on Self-Driving Cars Cyberlaw and Venture Law 31 Berkeley Technology Law Journal 2016/\:C//\:/Users/Psykie/Zotero/storage/B7CU5JF3/LandingPage.html/\:text/html\:PDF:PDF},
  shorttitle = {Autonomous {Vehicle} {Regulation}},
}

@WWW{cognata_cognata_nodate,
  author   = {cognata},
  title    = {cognata},
  year     = {2019},
  url      = {https://www.cognata.com/},
  language = {en-US},
  urldate  = {2019-06-02},
  abstract = {Comprehensive Autonomous Simulation Platform for the testing and qualification of autonomous vehicles and ADAS systems},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\JJNDUIYB\\\\\\\\www.cognata.com.html\\\:text/html\:PDF:PDF},
  journal  = {Cognata Autonomous and ADAS Simulation},
}

@WWW{rfpro_rfpro_nodate,
  author   = {rFpro},
  title    = {{rFpro}},
  year     = {2019},
  url      = {http://www.rfpro.com/},
  language = {en-GB},
  urldate  = {2019-06-02},
  file     = {html\:PDF:html\:Snapshot\\\:C\\\\\\\:\\\\\\\\Users\\\\\\\\Psykie\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\7F5AYWL2\\\\\\\\www.rfpro.com.html\\\:text/html\:PDF:PDF},
  journal  = {rFpro},
}

@WWW{nvidia_corporation_nvidia_nodate,
  author   = {{NVIDIA Corporation}},
  title    = {{NVIDIA} {DRIVE} {Constellation}.},
  year     = {2019},
  url      = {https://www.nvidia.com/en-au/self-driving-cars/drive-constellation/},
  language = {en-au},
  urldate  = {2019-06-02},
  abstract = {NVIDIA Drive Constellation uses photorealistic simulations to create a safer, more scalable way to bring self-driving cars to the road. Learn more.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/GGYAR83C/drive-constellation.html/\:text/html\:PDF:PDF},
  journal  = {@NVIDIA},
}

@Article{li_intelligence_2016,
  author     = {Li, L. and Huang, W. and Liu, Y. and Zheng, N. and Wang, F.},
  title      = {Intelligence {Testing} for {Autonomous} {Vehicles}: {A} {New} {Approach}},
  journal    = {IEEE Transactions on Intelligent Vehicles},
  year       = {2016},
  volume     = {1},
  number     = {2},
  month      = jun,
  pages      = {158--166},
  issn       = {2379-8904},
  doi        = {10.1109/TIV.2016.2608003},
  abstract   = {In this paper, we study how to test the intelligence of an autonomous vehicle. Comprehensive testing is crucial to both vehicle manufactories and customers. Existing testing approaches can be categorized into two kinds: scenario-based testing and functionality-based testing. We first discuss the shortcomings of these two kinds of approaches, and then propose a new testing framework to combine the benefits of them. Based on the new semantic diagram definition for the intelligence of autonomous vehicles, we explain how to design a task for autonomous vehicle testing and how to evaluate test results. Experiments show that this new approach provides a quantitative way to test the intelligence of an autonomous vehicle.},
  file       = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/8EXHJX4Y/7571159.html/\:text/html\:PDF:PDF},
  keywords   = {Autonomous automobiles, autonomous vehicle, Autonomous vehicles, functionality-based testing, intelligence testing, intelligent transportation systems, Intelligent vehicles, mobile robots, Prototypes, road vehicle, road vehicles, Roads, scenario-based testing, semantic diagram definition, Semantics, testing, Testing, Vehicles},
  shorttitle = {Intelligence {Testing} for {Autonomous} {Vehicles}},
}

@mastersthesis{bradley_automotive_2009,
	address = {Perth, Australia},
	title = {Automotive {Simulation} {System}},
	url = {http://robotics.ee.uwa.edu.au/theses/2009-AutoSim-Bradley.pdf},
	language = {en},
	school = {The University of Western Australia},
	author = {Bradley, Steven},
	year = {2009}
}

@WWW{epic_games_unreal_nodate,
  author   = {{Epic Games}},
  title    = {Unreal {Engine}},
  year     = {2019},
  url      = {https://www.unrealengine.com/en-US/what-is-unreal-engine-4},
  language = {en-US},
  urldate  = {2019-06-02},
  abstract = {Unreal Engine 4 is a suite of integrated tools for game developers to design and build games, simulations, and visualizations.},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/ISYNBBNW/en-US.html/\:text/html\:PDF:PDF},
}

@WWW{canonical_ltd._ubuntu_2019,
  author  = {{Canonical Ltd.}},
  title   = {Ubuntu {Manpage}: clockdiff - measure clock difference between hosts},
  year    = {2019},
  url     = {http://manpages.ubuntu.com/manpages/bionic/man8/clockdiff.8.html},
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:Ubuntu Manpage//\: clockdiff - measure clock difference between hosts/\:C//\:/Users/Psykie/Zotero/storage/4I6M9II8/clockdiff.8.html/\:text/html\:PDF:PDF},
}

@inproceedings{lauxtermann_comparison_2007,
	address = {Ogunquit Maine, USA},
	title = {Comparison of global shutter pixels for {CMOS} image sensors},
	url = {http://www.imagesensors.org/Past%20Workshops/2007%20Workshop/2007%20Papers/021%20Lauxtermann%20et%20al.pdf},
	booktitle = {Proc, 2007 {International} {Image} {Sensor} {Workshop}},
	author = {Lauxtermann, Stefan and Lee, Adam and Stevens, John and Joshi, Atul},
	month = jun,
	year = {2007},
	pages = {82--85}
}

@WWW{sebastien_performance_2019,
  author    = {Sebastien, Godard},
  title     = {Performance monitoring tools for {Linux}.},
  year      = {2019},
  url       = {https://github.com/sysstat/sysstat},
  note      = {original-date: 2013-04-25T11:23:10Z},
  month     = jun,
  urldate   = {2019-06-02},
  copyright = {GPL-2.0},
}

@Article{noauthor_ieee_2008,
  author   = {IEEE},
  title    = {{IEEE} {Standard} for a {Precision} {Clock} {Synchronization} {Protocol} for {Networked} {Measurement} and {Control} {Systems}},
  journal  = {IEEE Std 1588-2008 (Revision of IEEE Std 1588-2002)},
  year     = {2008},
  month    = jul,
  pages    = {1--300},
  doi      = {10.1109/IEEESTD.2008.4579760},
  abstract = {A protocol is provided in this standard that enables precise synchronization of clocks in measurement and control systems implemented with technologies such as network communication, local computing, and distributed objects. The protocol is applicable to systems communicating via packet networks. Heterogeneous systems are enabled that include clocks of various inherent precision, resolution, and stability to synchronize. System-wide synchronization accuracy and precision in the sub-microsecond range are supported with minimal network and local clock computing resources. Simple systems are installed and operated without requiring the management attention of users because the default behavior of the protocol allows for it.},
  file     = {html\:PDF:html\:IEEE Xplore Abstract Record/\:C//\:/Users/Psykie/Zotero/storage/8ND8HTII/4579760.html/\:text/html\:PDF:PDF},
  keywords = {1588-2008, boundary clock, clock, distributed control, distributed objects, distributed system, heterogeneous systems, IEEE standard, IEEE standards, local clock computing resources, master clock, measurement and control system, network communication, networked control systems, networked measurement, packet networks, precision clock synchronization protocol, protocols, real-time clock, synchronisation, synchronized clock, system-wide synchronization accuracy, transparent clock},
}

@WWW{bureau_of_meteorology_climate_2013,
  author  = {{Bureau of Meteorology}},
  title   = {Climate {Data} {Online}},
  year    = {2013},
  url     = {http://www.bom.gov.au/climate/data/},
  month   = dec,
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:Climate Data Online/\:C//\:/Users/Psykie/Zotero/storage/I5AGTNTZ/data.html/\:text/html\:PDF:PDF},
}

@Article{noauthor_self-drive_2018,
  author   = {{BBC News}},
  title    = {Self-drive buses enter 'mass production'},
  journal  = {BBC NEews},
  year     = {2018},
  date     = {2018-07-04},
  language = {en-GB},
  month    = jul,
  url      = {https://www.bbc.com/news/technology-44713298},
  urldate  = {2019-06-02},
  abstract = {Chinese technology giant Baidu says the vehicles will soon be used across China and in Tokyo.},
  chapter  = {Technology},
  file     = {html\:PDF:html\:Snapshot/\:C//\:/Users/Psykie/Zotero/storage/84VFSX3Z/technology-44713298.html/\:text/html\:PDF:PDF},
}

@WWW{autoware_open-source_2019,
  author    = {Autoware},
  title     = {Open-source software for self-driving vehicles.},
  year      = {2019},
  url       = {https://github.com/autowarefoundation/autoware},
  note      = {original-date: 2015-08-24T23:17:57Z},
  month     = jun,
  urldate   = {2019-06-02},
  copyright = {Apache-2.0},
  publisher = {The Autoware Foundation},
}

@WWW{ros_pointcloud_to_laserscan_nodate,
  author  = {ROS},
  title   = {pointcloud\_to\_laserscan - {ROS} {Wiki}},
  year    = {2019},
  url     = {http://wiki.ros.org/pointcloud_to_laserscan},
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:pointcloud_to_laserscan - ROS Wiki/\:C//\:/Users/Psykie/Zotero/storage/XRAHRCMA/pointcloud_to_laserscan.html/\:text/html\:PDF:PDF},
}

@WWW{ros_lms1xx_nodate,
  author  = {ROS},
  title   = {{LMS}1xx - {ROS} {Wiki}},
  year    = {2019},
  url     = {http://wiki.ros.org/LMS1xx},
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:LMS1xx - ROS Wiki/\:C//\:/Users/Psykie/Zotero/storage/Q3LY8LLW/LMS1xx.html/\:text/html\:PDF:PDF},
}

@WWW{carla_cameras_nodate,
  author  = {Carla},
  title   = {Cameras and sensors - {CARLA} {Simulator}},
  year    = {2019},
  url     = {https://carla.readthedocs.io/en/latest/cameras_and_sensors/},
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:Cameras and sensors - CARLA Simulator/\:C//\:/Users/Psykie/Zotero/storage/QLUIDZIV/cameras_and_sensors.html/\:text/html\:PDF:PDF},
}

@WWW{ros_sensor_msgs/laserscan_nodate,
  author  = {ROS},
  title   = {sensor\_msgs/{LaserScan} {Documentation}},
  year    = {2019},
  url     = {http://docs.ros.org/melodic/api/sensor_msgs/html/msg/LaserScan.html},
  urldate = {2019-06-02},
  file    = {html\:PDF:html\:sensor_msgs/LaserScan Documentation/\:C//\:/Users/Psykie/Zotero/storage/BPVLK8AY/LaserScan.html/\:text/html\:PDF:PDF},
}

@Report{standards_association_of_australia_electrical_2018,
  author      = {{Standards Association of Australia} and {Joint Technical Committee EL/001} and {Standards Australia Limited} and {Standards New Zealand}},
  title       = {Electrical installations (known as the {Australian}/{New} {Zealand} wiring rules)},
  type        = {Report},
  institution = {Standards Australia},
  year        = {2018},
  language    = {English},
  note        = {OCLC: 1052794055},
  abstract    = {"This Standard sets out requirements for the design, construction and verification of electrical installations, including the selection and installation of electrical equipment forming part of such electrical installations. These requirements are intended to protect persons, livestock, and property from electric shock, fire and physical injury hazards that may arise from an electrical installation that is used with reasonable care and with due regard to the intended purpose of the electrical installation." -- Page 33.},
  isbn        = {978-1-76035-993-5},
}

@Report{frost_&_sullivan_global_2018,
  author   = {{Frost \& Sullivan}},
  title    = {Global {Autonomous} {Driving} {Market} {Outlook}, 2018},
  type     = {Market {Research}},
  year     = {2018},
  language = {en},
  number   = {K24A-01-00-00-00},
  month    = mar,
  url      = {https://store.frost.com/global-autonomous-driving-market-outlook-2018.html},
  urldate  = {2019-06-09},
  address  = {San Antonio},
  file     = {html:Global Autonomous Driving Market Outlook, 2018\:C/\://Users//Psykie//Zotero//storage//GXV362WM//global-autonomous-driving-market-outlook-2018.html\:text/html:PDF},
  school   = {Frost \& Sullivan},
}

@TechReport{hubbard_synthesis_2017,
  author      = {Hubbard, Sarah M},
  title       = {Synthesis of {Automated} {Vehicle} {Legislation}},
  institution = {Purdue University},
  year        = {2017},
  date        = {2017-10},
  language    = {English},
  number      = {FHWA/IN/JTRP-2017/21},
  month       = oct,
  doi         = {10.5703/1288284316575},
  url         = {https://rosap.ntl.bts.gov/view/dot/35994},
  abstract    = {This report provides a synthesis of issues addressed by state legislation regarding automated vehicles (AV); AV technologies are rapidly evolving and many states have developed legislation to govern AV testing and deployment and to assure safety on public roads. Topics include license and registration, operator requirements, insurance and liability, infrastructure, vehicle testing and operations, commercial vehicle operations and privacy. States are interested in supporting AV because the expected benefits include increased safety, increased capacity, and decreased congestion. Other expected benefits include increased productivity due to hands-free travel and increased mobility for people unable to drive themselves. The projected economic impact of AV is significant, with an estimated market of \$7 trillion by 2050. Although speculative, this value indicates the dramatic impact that AV may have on the future of transportation. The benefits of AV may be significant, however, there are also potential challenges, including the potential for increased costs, liability issues, licensing issues, security concerns, privacy considerations, and cybersecurity issues, as well as job losses in the transportation sector. Legislative responses to this technology have varied significantly, depending on the state. Currently nineteen states and the District of Columbia have passed legislation related to AV, and four states have executive orders. Even if federal AV legislation is passed, there will still be an important role for states regarding AV licensure, registration, insurance, traffic laws, enforcement, infrastructure and emergency response. The objective of this research is to identify and synthesize current state legislation related to AV. The results provide important information as agencies and decision makers develop strategic plans for AV activities at every level.},
  editor      = {{Purdue University. Joint Transportation Research Program}},
  keywords    = {automated vehicles, Intelligent vehicles, Legislation, state legislation},
  publisher   = {Purdue University},
  school      = {Purdue University},
}

@TechReport{htf_market_intelligence_overview_2019,
  author      = {{HTF Market Intelligence}},
  title       = {Overview of {Global} {Self} driving {Car} {Market} review now and beyond},
  institution = {HTF Market Intelligence},
  year        = {2019},
  date        = {2019-03},
  type        = {Market {Research}},
  language    = {english},
  number      = {HTF1304659},
  month       = mar,
  pages       = {100},
  url         = {https://www.htfmarketreport.com/reports/1304659-global-self-driving-car-market-2},
  urldate     = {2019-06-09},
  abstract    = {What are the market opportunities and threats faced by the vendors in the Global Self driving Car Market review?  Get in-depth details about factors influencing the market shares of the important regions like United States, Asia-Pacific, United Kingdom, France \& Germany?},
  address     = {Pune},
  file        = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//DVAZKSDJ//1304659-global-self-driving-car-market-2.html\:text/html:PDF},
  school      = {HTF Market Intelligence},
}

@TechReport{mordor_intelligence_autonomous/driverless_nodate,
  author  = {{Mordor Intelligence}},
  title   = {Autonomous/{Driverless} {Car} {Market} - {Growth}, {Trends}, and {Forecast} (2019 – 2024)},
  year    = {2018},
  type    = {Market {Research}},
  url     = {https://www.mordorintelligence.com/industry-reports/autonomous-driverless-cars-market-potential-estimation},
  urldate = {2019-06-09},
  address = {Hyderabad},
  file    = {html:Autonomous/Driverless Car Market - Growth, Trends, and Forecast (2019 – 2024)\:C/\://Users//Psykie//Zotero//storage//Y3BF9F6M//autonomous-driverless-cars-market-potential-estimation.html\:text/html:PDF},
  school  = {Mordor Intelligence},
}

@TechReport{grand_view_research_self-driving_2018,
  author      = {{Grand View Research}},
  title       = {Self-driving {Cars} {And} {Trucks} {Market} {Size} {\textbar} {Industry} {Report}, 2020-2030},
  institution = {Grand View Research},
  year        = {2018},
  date        = {2018-06},
  type        = {Market {Research}},
  language    = {en},
  number      = {978-1-68038-884-8},
  month       = jun,
  pages       = {100},
  url         = {https://www.grandviewresearch.com/industry-analysis/driverless-cars-market},
  urldate     = {2019-06-09},
  abstract    = {The global self‑driving cars and trucks market size is expected to be approximately 6.7 thousand units in 2020 and is anticipated to expand at a CAGR of 63.1\% from 2021 to 2030. Self-drive cars, also known as autonomous vehicles, are a key innovation in the automotive industry},
  address     = {San Francisco},
  file        = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//5RQSC6PJ//driverless-cars-market.html\:text/html:PDF},
  school      = {Grand View Research},
}

@TechReport{kumar_autonomous_2018,
  author      = {Kumar, Rahul and Richa},
  title       = {Autonomous {Vehicle} {Market} by {Level} of {Automation} ({Level} 3, {Level} 4, and {Level} 5) and {Component} ({Hardware}, {Software}, and {Service}) and {Application} ({Civil}, {Robo} {Taxi}, {Self}-driving {Bus}, {Ride} {Share}, {Self}-driving {Truck}, and {Ride} {Hail}) - {Global} {Opportunity} {Analysis} and {Industry} {Forecast}, 2019-2026},
  institution = {Allied Market Research},
  year        = {2018},
  date        = {2018-05},
  type        = {Market {Research}},
  language    = {en},
  number      = {AU\_184649},
  month       = may,
  pages       = {493},
  url         = {https://www.alliedmarketresearch.com/autonomous-vehicle-market},
  urldate     = {2019-06-09},
  address     = {Pune},
  file        = {html:Autonomous Vehicle Market Size, Share and Growth Analysis | Forecast\:C/\://Users//Psykie//Zotero//storage//L3HEVC7P//autonomous-vehicle-market.html\:text/html:PDF},
  school      = {Allied Market Research},
}

@Article{dickmanns_autonomous_1987,
  author   = {Dickmanns, E. D. and Zapp, A.},
  title    = {Autonomous {High} {Speed} {Road} {Vehicle} {Guidance} by {Computer} {Vision}1},
  journal  = {IFAC Proceedings Volumes},
  year     = {1987},
  series   = {10th {Triennial} {IFAC} {Congress} on {Automatic} {Control} - 1987 {Volume} {IV}, {Munich}, {Germany}, 27-31 {July}},
  volume   = {20},
  number   = {5, Part 4},
  month    = jul,
  pages    = {221--226},
  issn     = {1474-6670},
  doi      = {10.1016/S1474-6670(17)55320-3},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667017553203},
  urldate  = {2019-06-09},
  abstract = {A visual feedback control system has been developed which is able to guide road vehicles on well structured roads at high speeds. The road boundary markings are tracked by a multiprocessor image processing system using contour correlation and curvature models together with the laws of perspective projection. Feature position data are the input into Kalman filters to estimate both the vehicle state vector relative to the driving lane and road curvature parameters. Velocity is measured conventionally. Longitudinal control by throttle and braking is geared to lateral acceleration due to road curvature; lateral control has an anticipatory feed forward and a compensatory feedback component. The control system has been tested with a CCD TV-camera and image sequence processing hardware in a real time simulation loop and with our experimental vehicle, a 5 ton-van equipped with sensors, onboard computers and actuators for autonomous driving},
  file     = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//T6EVTKHC//S1474667017553203.html\:text/html:PDF},
  keywords = {Automatically guided vehicles, Computer vision, Knowledge based control, Multiloop feedback control, Multiprocessors},
}

@WWW{national_conference_of_state_legislatures_autonomous_2019,
  author  = {{National Conference of State Legislatures}},
  title   = {Autonomous {Vehicles} {State} {Bill} {Tracking} {Database}},
  year    = {2019},
  url     = {http://www.ncsl.org/research/transportation/autonomous-vehicles-legislative-database.aspx},
  month   = apr,
  urldate = {2019-06-09},
  file    = {html:Autonomous Vehicles State Bill Tracking Database\:C/\://Users//Psykie//Zotero//storage//C54ZQTBU//autonomous-vehicles-legislative-database.html\:text/html:PDF},
}

@WWW{national_transport_commission_automated_2019,
  author  = {{National Transport Commission}},
  title   = {Automated vehicles in {Australia}},
  year    = {2019},
  url     = {https://www.ntc.gov.au/roads/technology/automated-vehicles-in-australia/},
  month   = apr,
  urldate = {2019-06-09},
  file    = {html:NTC - National Transport Commission\:C/\://Users//Psykie//Zotero//storage//AWANHZ88//automated-vehicles-in-australia.html\:text/html:PDF},
}

@WWW{department_of_transport_automated_2018,
  author   = {{Department of Transport}},
  title    = {Automated vehicles},
  year     = {2018},
  url      = {https://www.transport.wa.gov.au/projects/automated-vehicles.asp},
  language = {en},
  month    = dec,
  urldate  = {2019-06-09},
  abstract = {The Western Australian Department of Transport (DoT) is working to make automated, connected and related vehicle technologies available in our state, to improve safety, improve liveability and increase productivity.},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//LMVILSRX//automated-vehicles.html\:text/html:PDF},
}

@TechReport{singh_critical_2015,
  author      = {Singh, Santokh},
  title       = {Critical {Reasons} for {Crashes} {Investigated} in the {National} {Motor} {Vehicle} {Crash} {Causation} {Survey}},
  institution = {National Center for Statistics and Analysis},
  year        = {2015},
  date        = {2015-02},
  type        = {Publication},
  language    = {en},
  number      = {DOT HS 812 115},
  month       = feb,
  pages       = {2},
  url         = {https://trid.trb.org/view/1346216},
  urldate     = {2019-06-09},
  file        = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//3QD6RKJE//1346216.html\:text/html:PDF},
  school      = {National Center for Statistics and Analysis},
}

@WWW{department_of_infrastructure_transport_cities_and_regional_development_safety_2019,
  author   = {{Department of Infrastructure, Transport, Cities {and} Regional Development}},
  title    = {Safety {Statistics}},
  year     = {2019},
  url      = {https://www.bitre.gov.au/statistics/safety/},
  language = {en-US},
  month    = may,
  urldate  = {2019-06-09},
  abstract = {The latest Australian road fatality data (April 2019) shows that:},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//QM48VZU8//safety.html\:text/html:PDF},
}

@WWW{mit_technology_review_insights_autonomous_2019,
  author     = {{MIT Technology Review Insights}},
  title      = {Autonomous driving: {Safety} first},
  year       = {2019},
  url        = {https://www.technologyreview.com/s/613087/autonomous-driving-safety-first/},
  language   = {en-US},
  month      = mar,
  urldate    = {2019-06-09},
  abstract   = {Self-driving vehicle technology has made significant advancements; now there needs to be an industry standard for self-driving safely.},
  file       = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//FGIB42CV//autonomous-driving-safety-first.html\:text/html:PDF},
  journal    = {MIT Technology Review},
  shorttitle = {Autonomous driving},
}

@WWW{bertoncello_ten_2015,
  author   = {Bertoncello, Michele and Wee, Dominik},
  title    = {Ten ways autonomous driving could redefine the automotive world},
  year     = {2015},
  url      = {https://www.mckinsey.com/industries/automotive-and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive-world},
  language = {en},
  month    = jun,
  urldate  = {2019-06-09},
  abstract = {The development of self-driving, or autonomous, vehicles is accelerating. Here\&rsquo;s how they could affect consumers and companies.},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//WRZYMJEL//ten-ways-autonomous-driving-could-redefine-the-automotive-world.html\:text/html:PDF},
  journal  = {McKinsey},
}

@Article{rogelj_paris_2016,
  author    = {Rogelj, Joeri and den Elzen, Michel and Höhne, Niklas and Fransen, Taryn and Fekete, Hanna and Winkler, Harald and Schaeffer, Roberto and Sha, Fu and Riahi, Keywan and Meinshausen, Malte},
  title     = {Paris {Agreement} climate proposals need a boost to keep warming well below 2 °{C}},
  journal   = {Nature},
  year      = {2016},
  language  = {en},
  volume    = {534},
  number    = {7609},
  month     = jun,
  pages     = {631--639},
  issn      = {1476-4687},
  doi       = {10.1038/nature18307},
  url       = {https://www.nature.com/articles/nature18307},
  urldate   = {2019-06-09},
  abstract  = {The Paris climate agreement aims at holding global warming to well below 2 degrees Celsius and to “pursue efforts” to limit it to 1.5 degrees Celsius. To accomplish this, countries have submitted Intended Nationally Determined Contributions (INDCs) outlining their post-2020 climate action. Here we assess the effect of current INDCs on reducing aggregate greenhouse gas emissions, its implications for achieving the temperature objective of the Paris climate agreement, and potential options for overachievement. The INDCs collectively lower greenhouse gas emissions compared to where current policies stand, but still imply a median warming of 2.6–3.1 degrees Celsius by 2100. More can be achieved, because the agreement stipulates that targets for reducing greenhouse gas emissions are strengthened over time, both in ambition and scope. Substantial enhancement or over-delivery on current INDCs by additional national, sub-national and non-state actions is required to maintain a reasonable chance of meeting the target of keeping warming well below 2 degrees Celsius.},
  copyright = {2016 Nature Publishing Group},
  file      = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//RX8QKWR6//nature18307.html\:text/html:PDF},
}

@TechReport{united_states_environmental_protection_agency_inventory_2019,
  author      = {{United States Environmental Protection Agency}},
  title       = {Inventory of {U}.{S}. {Greenhouse} {Gas} {Emissions} and {Sinks}: 1990-2017},
  institution = {United States Environmental Protection Agency},
  year        = {2019},
  date        = {2019-04-11},
  type        = {Report},
  language    = {en},
  number      = {EPA 430-R-19-001},
  month       = apr,
  pages       = {675},
  url         = {https://www.epa.gov/sites/production/files/2019-04/documents/us-ghg-inventory-2019-main-text.pdf},
  urldate     = {2019-06-09},
  address     = {Washington, D.C.},
  file        = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//K5XLMEPC//inventory-us-greenhouse-gas-emissions-and-sinks-1990-2017.html\:text/html:PDF},
  school      = {United States Environmental Protection Agency},
  shorttitle  = {Inventory of {U}.{S}. {Greenhouse} {Gas} {Emissions} and {Sinks}},
}

@Article{santos_road_2017,
  author     = {Santos, Georgina},
  title      = {Road transport and {CO}2 emissions: {What} are the challenges?},
  journal    = {Transport Policy},
  year       = {2017},
  volume     = {59},
  month      = oct,
  pages      = {71--74},
  issn       = {0967-070X},
  doi        = {10.1016/j.tranpol.2017.06.007},
  url        = {http://www.sciencedirect.com/science/article/pii/S0967070X17304262},
  urldate    = {2019-06-09},
  abstract   = {In order for the world to stay within the safety threshold of a 2°C increase in average temperature agreed by virtually all governments, the transport sector needs to be decarbonized. The two main obstacles that have prevented this from happening have been the absence of a global legally binding deal and the high relative cost of clean vehicle/energy technologies. The Paris Agreement, which commits countries to reductions of GHG emissions, has virtually solved the first problem and paved the way for countries to implement environmental taxes and subsidies in order to change the relative costs of clean alternatives, which would solve the second problem. These policy actions combined with investment in clean infrastructure and regulation can decarbonize the transport sector.},
  file       = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//Y4UBNIHF//S0967070X17304262.html\:text/html:PDF},
  keywords   = {2°C, Carbon emissions, Carbon taxes, Climate change, CO emissions, Environmental agreements, Environmental subsidies, Environmental taxes, GHG emissions, Global deals, Global warming, Paris Agreement, Subsidies to clean R\&D, Transport, Transportation},
  shorttitle = {Road transport and {CO}2 emissions},
}

@TechReport{department_of_the_environment_and_energy_australias_2018,
  author      = {{Department of the Environment and Energy}},
  title       = {Australia’s emissions projections 2018},
  institution = {Australian Government},
  year        = {2018},
  date        = {2018-12},
  type        = {Report},
  language    = {en},
  number      = {CC227.1118},
  month       = dec,
  pages       = {46},
  url         = {https://www.environment.gov.au/system/files/resources/128ae060-ac07-4874-857e-dced2ca22347/files/australias-emissions-projections-2018.pdf},
  urldate     = {2019-06-09},
  abstract    = {About these documents The report provides detail on emissions trends, including sector specific analysis of factors driving emissions. The report estimates the emissions reduction effort required to meet Australia’s emissions reduction targets. The projections include sensitivity analyses to illustrate how emissions may differ under changes in economic growth.},
  address     = {Canberra},
  file        = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//LDCSBYGX//emissions-projections-2018.html\:text/html:PDF},
  school      = {Department of the Environment and Energy},
}

@Book{kirsch_electric_2000,
  author    = {Kirsch, David A.},
  title     = {The electric vehicle and the burden of history},
  year      = {2000},
  publisher = {Rutgers University Press},
  isbn      = {978-0-8135-2808-3 978-0-8135-2809-0},
  address   = {New Brunswick, N.J},
  keywords  = {Automobile industry and trade, Automobiles, Design and construction History, Electric automobiles, History, United States},
}

@Article{johnson_environmental_1999,
  author     = {Johnson, Brian C.},
  title      = {Environmental products that drive organizational change: {General} motor's electric vehicle ({EV}1)},
  journal    = {Corporate Environmental Strategy},
  year       = {1999},
  volume     = {6},
  number     = {2},
  month      = jan,
  pages      = {140--150},
  issn       = {1066-7938},
  doi        = {10.1016/S1066-7938(00)80024-X},
  url        = {http://www.sciencedirect.com/science/article/pii/S106679380080024X},
  urldate    = {2019-06-09},
  abstract   = {Environmental change can come from numerous sources, both internal and external to a company. Very often, sweeping environmental progress is a result of both types of forces acting in concert. In the case of General Motors, a change in environmental mindset helped lead to the development of a revolutionary new vehicle, the EV1, while the same vehicle also continues to be the force behind further change at the company, and even in the industry as a whole. This case study tells the story of the EVI, and how it may be considered one of the most important new cars in decades.},
  file       = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//XGGZ4PIE//S106679380080024X.html\:text/html:PDF},
  shorttitle = {Environmental products that drive organizational change},
}

@WWW{joseph_how_2016,
  author   = {Joseph, Jacob},
  title    = {How much does the electricity cost for an electric car?},
  year     = {2016},
  url      = {https://www.finder.com.au/electricity-cost-for-an-electric-car},
  language = {en-AU},
  month    = jan,
  urldate  = {2019-06-09},
  abstract = {With the rising costs of petrol, the emergence of electric cars have changed the dynamics of the market – but is it actually cheaper to switch?},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//EKJNZDPX//electricity-cost-for-an-electric-car.html\:text/html:PDF},
  journal  = {finder.com.au},
}

@Article{broadbent_analysis_2019,
  author     = {Broadbent, Gail Helen and Metternicht, Graciela and Drozdzewski, Danielle},
  title      = {An {Analysis} of {Consumer} {Incentives} in {Support} of {Electric} {Vehicle} {Uptake}: {An} {Australian} {Case} {Study}},
  journal    = {World Electric Vehicle Journal},
  year       = {2019},
  language   = {en},
  volume     = {10},
  number     = {1},
  month      = mar,
  pages      = {11},
  doi        = {10.3390/wevj10010011},
  url        = {https://www.mdpi.com/2032-6653/10/1/11},
  urldate    = {2019-06-09},
  abstract   = {Transitioning from internal combustion engine vehicles (ICEVs) to innovative technologies, including electric vehicles (EVs), can be a crucial pathway to reducing Greenhouse Gas (GHG) emissions and other negative externalities arising from fossil-fueled cars used for personal transport. Government action to correct insufficient market incentives has been essential in countries working to enhance EV acceptance; however, to date in Australia, there has been little government support to enact EV uptake. This paper identifies barriers and incentives to EV adoption in Australia through a survey of pro-environmental motorists, including an experimental component to test information provision influences on attitude change. Results evidence that wide ranging factors influence vehicle choice including EVs. Purchase barriers are focused on lack of a comprehensive recharge network and high EV purchase price. Factors encouraging fully EV uptake showed affordable price (56\%) increased vehicle range (26\%) and an adequate recharge network (28\%) were mentioned most often; only 13\% specifically indicated environmental regard as influential. Information provided about EVs increased the likelihood of positive attitudes towards EV purchase and decreased uncertainty about the technology. Recommendations arising from this research could be considered by laggard countries that, like Australia, have yet to take significant action to encourage transition to EVs.},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  file       = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//DHSQGWNJ//htm.html\:text/html:PDF},
  keywords   = {Australia, consumer choice survey, electric vehicle, EV adoption, implementation, public policy},
  shorttitle = {An {Analysis} of {Consumer} {Incentives} in {Support} of {Electric} {Vehicle} {Uptake}},
}

@Article{broadbent_electric_2018,
  author     = {Broadbent, Gail Helen and Drozdzewski, Danielle and Metternicht, Graciela},
  title      = {Electric vehicle adoption: {An} analysis of best practice and pitfalls for policy making from experiences of {Europe} and the {US}},
  journal    = {Geography Compass},
  year       = {2018},
  language   = {en},
  volume     = {12},
  number     = {2},
  pages      = {1--15},
  issn       = {1749-8198},
  doi        = {10.1111/gec3.12358},
  urldate    = {2019-06-09},
  abstract   = {Accelerating the rate of electric vehicle (EV) adoption is an objective of many countries to mitigate and ameliorate negative externalities arising from the use of fossil fuels for personal motorised transportation including: greenhouse gas (GHG) emissions, air pollution and noise, as well as increasing energy security and reducing budget deficits. Within the dynamic field of EVs, this paper highlights strategic directions for policy makers to increase EV uptake. The paper critically reviews measures adopted by some industrialised countries to motivate consumer purchase of EVs rather than conventional internal combustion vehicles (ICVs). A key focus is the role of financial and soft incentives to encourage EV adoption. The analysis reveals that not all incentives are equally effective; an adequate recharger network appears to be a common concerning factor for EV adoption due to customer anxiety and vehicle limitations. Best practice strategies that could foster a faster transition to EV adoption include appropriate legislation, installation and maintenance of an adequate public recharger network, government procurement programs, and investment in information programs to accelerate the transition towards fossil free driving. The paper evidences how implementation of these strategies can affect overall adoption rates.},
  copyright  = {© 2017 The Author(s) Geography Compass © 2017 John Wiley \& Sons Ltd},
  file       = {html:Full Text PDF\:C/\://Users//Psykie//Zotero//storage//U7ZSYXSN//Broadbent et al. - 2018 - Electric vehicle adoption An analysis of best pra.pdf\:application/pdf\;Snapshot\:C/\://Users//Psykie//Zotero//storage//23STQE3M//gec3.html\:text/html:PDF},
  shorttitle = {Electric vehicle adoption},
}

@Article{bauer_impact_2018,
  author   = {Bauer, Gordon},
  title    = {The impact of battery electric vehicles on vehicle purchase and driving behavior in {Norway}},
  journal  = {Transportation Research Part D: Transport and Environment},
  year     = {2018},
  volume   = {58},
  month    = jan,
  pages    = {239--258},
  issn     = {1361-9209},
  doi      = {10.1016/j.trd.2017.12.011},
  url      = {http://www.sciencedirect.com/science/article/pii/S1361920916305235},
  urldate  = {2019-06-09},
  abstract = {Battery electric vehicles (BEVs) represent a promising solution to carbon emissions within the transportation sector, but relatively little is known about how limited-range vehicles will change consumer behavior. This study evaluates the impact of large-scale introduction of BEVs on the new car market in Norway, and seeks to determine whether BEVs lead to an increase in household vehicle ownership. Analysis of online survey responses from 4405 new car owners suggests that BEVs lead to an increase in household vehicle ownership of approximately 15–20\%. We arrive at this finding using three independent methods: logistic regression of the likelihood of replacing a vehicle on vehicle type, analysis of correlation between the rate of vehicle replacement and BEV market share across municipalities, and linear regression of changes in total new car sales on changes in BEV sales across counties and years. Survey responses on driving habits also suggest that BEV purchases lead to a moderate increase in vehicle miles traveled. These results suggest that real-world emissions savings may be somewhat less than those predicted by life-cycle analysis. Policy recommendations include focusing incentives for BEVs on those bought as replacements for a conventional vehicle.},
  file     = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//WQZQUVC6//S1361920916305235.html\:text/html:PDF},
  keywords = {Electric vehicles, Household vehicle ownership, Passenger car markets, Purchasing behavior, Rebound effects},
}

@Report{senate_select_committee_on_electric_vehicles_report_2019,
  author   = {{Senate Select Committee on Electric Vehicles}},
  title    = {Report},
  type     = {Report},
  year     = {2019},
  date     = {2019-01-30},
  language = {English},
  month    = jan,
  pages    = {197},
  url      = {https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Electric_Vehicles/ElectricVehicles/~/media/Committees/electricvehicles_ctte/report.pdf},
  address  = {Canberra},
  school   = {Parliament of Australia},
}

@Article{pearre_review_2019,
  author   = {Pearre, Nathaniel S. and Ribberink, Hajo},
  title    = {Review of research on {V}2X technologies, strategies, and operations},
  journal  = {Renewable and Sustainable Energy Reviews},
  year     = {2019},
  volume   = {105},
  month    = may,
  pages    = {61--70},
  issn     = {1364-0321},
  doi      = {10.1016/j.rser.2019.01.047},
  url      = {http://www.sciencedirect.com/science/article/pii/S1364032119300516},
  urldate  = {2019-06-09},
  abstract = {Vehicle-to-Anything, or V2X, is a term which references technologies that use the energy in the batteries of plug-in electric vehicles (PEVs) for any purpose outside the vehicle. The purpose of this document is to provide a comprehensive and current review of concepts, recently published studies, and demonstration projects/deployments of V2X technology from around the world. While the focus will be kept on recently published studies and current technological developments, some historical perspective and important project reports from earlier periods of V2X development will be included for context. The report is broken down into sections focusing on different classes of V2X service: services to overall regional grid stability and reliability (V2G); reliability and backup energy supply services to individual homes or buildings (V2H); and studies focusing on services to commercial buildings generally inapplicable to residential houses (V2B). Attention will be paid to the operational strategy of the reviewed articles, which reflect a broad spectrum of optimization approaches and objectives.},
  file     = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//FY3MTMLV//S1364032119300516.html\:text/html:PDF},
  keywords = {Electric vehicle, Grid services, Micro grid, Transportation electrification, V2G, V2X},
}

@Article{papathanassiou_cellular_2017,
  author  = {Papathanassiou, Apostolos and Khoryaev, Alexey},
  title   = {Cellular {V}2X as the essential enabler of superior global connected transportation services},
  journal = {IEEE 5G Tech Focus},
  year    = {2017},
  volume  = {1},
  number  = {2},
  month   = jun,
  url     = {https://futurenetworks.ieee.org/tech-focus/june-2017/cellular-v2x},
}

@Article{khattak_toward_2019,
  author   = {Khattak, H. A. and Farman, H. and Jan, B. and Din, I. U.},
  title    = {Toward {Integrating} {Vehicular} {Clouds} with {IoT} for {Smart} {City} {Services}},
  journal  = {IEEE Network},
  year     = {2019},
  volume   = {33},
  number   = {2},
  month    = mar,
  pages    = {65--71},
  issn     = {0890-8044},
  doi      = {10.1109/MNET.2019.1800236},
  abstract = {Vehicular ad hoc networks, cloud computing, and the Internet of Things are among the emerging technology enablers offering a wide array of new application possibilities in smart urban spaces. These applications consist of smart building automation systems, healthcare monitoring systems, and intelligent and connected transportation, among others. The integration of IoT-based vehicular technologies will enrich services that are eventually going to ignite the proliferation of exciting and even more advanced technological marvels. However, depending on different requirements and design models for networking and architecture, such integration needs the development of newer communication architectures and frameworks. This work proposes a novel framework for architectural and communication design to effectively integrate vehicular networking clouds with IoT, referred to as VCoT, to materialize new applications that provision various IoT services through vehicular clouds. In this article, we particularly put emphasis on smart city applications deployed, operated, and controlled through LoRaWAN-based vehicular networks. LoraWAN, being a new technology, provides efficient and long-range communication possibilities. The article also discusses possible research issues in such an integration including data aggregation, security, privacy, data quality, and network coverage. These issues must be addressed in order to realize the VCoT paradigm deployment, and to provide insights for investors and key stakeholders in VCoT service provisioning. The article presents deep insights for different real-world application scenarios (i.e., smart homes, intelligent traffic light, and smart city) using VCoT for general control and automation along with their associated challenges. It also presents initial insights, through preliminary results, regarding data and resource management in IoT-based resource constrained environments through vehicular clouds.},
  file     = {html:IEEE Xplore Abstract Record\:C/\://Users//Psykie//Zotero//storage//XJ5JIZT2//8675174.html\:text/html:PDF},
  keywords = {architectural communication design, cloud computing, Cloud computing, communication architectures, Computer architecture, data aggregation, health care, healthcare monitoring systems, intelligent traffic light, intelligent transportation, Internet of Things, IoT-based resource constrained environments, IoT-based vehicular technologies, Logic gates, long-range communication, LoRaWAN-based vehicular networks, mobile radio, real-world application scenarios, smart building automation systems, Smart cities, smart city, smart city applications, smart city services, smart homes, smart urban spaces, VCoT paradigm deployment, VCoT service provisioning, vehicular ad hoc networks, Vehicular ad hoc networks, vehicular clouds, vehicular networking clouds, wide area networks},
}

@Report{weeratunga_connected_2015,
  author   = {Weeratunga, Kamal and Somers, Andrew},
  title    = {Connected {Vehicles}: {Are} we ready?},
  type     = {Report},
  year     = {2015},
  language = {en},
  month    = jun,
  pages    = {64},
  url      = {https://www.mainroads.wa.gov.au/Documents/Connect%20Vehicles%20Web.RCN-D15%5E23413758.PDF},
  urldate  = {2019-06-09},
  address  = {East Perth},
  school   = {Main Roads Western Australia},
}

@Report{transport_for_nsw_connected_2018,
  author   = {{Transport for NSW}},
  title    = {Connected and {Automated} {Vehicles} {Plan}},
  type     = {Report},
  year     = {2018},
  language = {en-AU},
  month    = oct,
  pages    = {56},
  url      = {https://future.transport.nsw.gov.au/node/204},
  urldate  = {2019-06-09},
  abstract = {Overview of the Connected and Automated Vehicles Plan},
  address  = {Chippendale},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//DHXL3EYV//connected-and-automated-vehicles-plan.html\:text/html:PDF},
  school   = {Transport for NSW},
}

@WWW{queensland_government_cavi:_2017,
  author     = {{Queensland Government}},
  title      = {{CAVI}: {Cooperative} and {Automated} {Vehicle} {Initiative}},
  year       = {2017},
  url        = {https://www.qld.gov.au/transport/projects/cavi},
  language   = {en},
  month      = aug,
  urldate    = {2019-06-09},
  abstract   = {Find out about the CAVI project.},
  file       = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//VENVR7M9//cavi.html\:text/html:PDF},
  shorttitle = {{CAVI}},
  type       = {Collection},
}

@Article{deng_cooperative_2019,
  author   = {Deng, R. and Di, B. and Song, L.},
  title    = {Cooperative {Collision} {Avoidance} for {Overtaking} {Maneuvers} in {Cellular} {V}2X-{Based} {Autonomous} {Driving}},
  journal  = {IEEE Transactions on Vehicular Technology},
  year     = {2019},
  volume   = {68},
  number   = {5},
  month    = may,
  pages    = {4434--4446},
  issn     = {0018-9545},
  doi      = {10.1109/TVT.2019.2906509},
  abstract = {In this paper, we consider a cooperative autonomous driving system where a vehicle overtakes the one in front based on collective perception. To avoid collisions with vehicles on the other lane, we propose a V2X-based cooperative collision avoidance scheme. The overtaking vehicle estimates the distance between itself and the neighbors via V2V communications and decides whether to overtake or not. Two cases where the distance information is obtained independently and cooperatively are taken into account. We derive the probability of collision avoidance and analyze the influence of different factors such as speed and density of vehicles on the system performance. Simulation results verify our analysis and show that our V2X-based cooperative collision avoidance scheme performs better than traditional GNSS-based collision avoidance scheme. The performance gain brought by the cooperative case compared to the independent case in our scheme can also be observed.},
  file     = {html:IEEE Xplore Abstract Record\:C/\://Users//Psykie//Zotero//storage//NWEJ7MWB//8671732.html\:text/html:PDF},
  keywords = {Automated driving, Autonomous vehicles, Collision avoidance, cooperative collision avoidance, Interference, Simulation, Time-frequency analysis, V2X, Vehicle-to-everything, Vehicular ad hoc networks},
}

@Article{hensher_tackling_2018,
  author   = {Hensher, David A.},
  title    = {Tackling road congestion – {What} might it look like in the future under a collaborative and connected mobility model?},
  journal  = {Transport Policy},
  year     = {2018},
  volume   = {66},
  month    = aug,
  pages    = {A1--A8},
  issn     = {0967-070X},
  doi      = {10.1016/j.tranpol.2018.02.007},
  url      = {http://www.sciencedirect.com/science/article/pii/S0967070X17308867},
  urldate  = {2019-06-09},
  abstract = {Traffic congestion continues to be the bane of many metropolitan areas and has exercised the minds of experts for at least the last 60 years. With the advent of smart (intelligent) mobility, aligned with digital disruption and future connected and collaborative transport including extensions to autonomous vehicles, the question of whether we have a new window of opportunity to tame congestion is now high on the list of possibilities. It is however very unclear what the future will look like in respect of congestion on the roads, especially if we rely on ‘smart’ technology and continue to reject reform of road user charging and new opportunities to fund the sharing model. This paper looks at a number of themes as a way of highlighting possibilities and challenges and promotes a position that congestion may not be reduced, especially without a significant switch to the sharing economy and relinquishing of private car ownership; the urgent need for government to define the institutional setting within which smart mobility can deliver reductions in congestion; and the crucial role that road pricing reform must play to ensure that those who benefit (suppliers and travellers) contribute to pay for the infrastructure (in particular) that they gain benefit from.},
  file     = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//FN58TF7L//S0967070X17308867.html\:text/html:PDF},
  keywords = {Business models, Congestion, Data ownership, Governance, Intelligent mobility, Pricing and funding reform, Public transport futures, Smart cities},
}

@WWW{shuttleworth_sae_2019,
  author   = {Shuttleworth, Jennifer},
  title    = {{SAE} {J}3016 automated-driving graphic},
  year     = {2019},
  url      = {https://www.sae.org/news/2019/01/sae-updates-j3016-automated-driving-graphic},
  language = {en},
  month    = jan,
  urldate  = {2019-06-09},
  abstract = {SAE J3016 automated-driving graphic update},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//7GLAR4SY//sae-updates-j3016-automated-driving-graphic.html\:text/html:PDF},
  journal  = {SAE Standards News},
}

@Report{on-road_automated_driving_orad_committee_taxonomy_2018,
  author    = {{On-Road Automated Driving (ORAD) committee}},
  title     = {Taxonomy and {Definitions} for {Terms} {Related} to {Driving} {Automation} {Systems} for {On}-{Road} {Motor} {Vehicles}},
  type      = {Report},
  year      = {2018},
  date      = {2018-06-15},
  language  = {en},
  month     = jun,
  pages     = {35},
  doi       = {10.4271/J3016_201806},
  url       = {https://www.sae.org/content/j3016_201806},
  urldate   = {2019-06-09},
  publisher = {{SAE} International},
  school    = {SAE International},
}

@WWW{burns_anyone_2019,
  author   = {Burns, Matt},
  title    = {‘{Anyone} relying on lidar is doomed,’ {Elon} {Musk} says},
  year     = {2019},
  url      = {http://social.techcrunch.com/2019/04/22/anyone-relying-on-lidar-is-doomed-elon-musk-says/},
  language = {en-US},
  month    = apr,
  urldate  = {2019-06-09},
  abstract = {Today at Tesla’s first Autonomy Day event, Elon Musk took questions from the press but didn’t have time for questions about lidar. Historically, he’s been vocal about the technology, and this time he put it as clear as he could. “Lidar is a fool’s errand,” Elon Musk said. “A…},
  file     = {html:Snapshot\:C/\://Users//Psykie//Zotero//storage//DGEXDQHK//anyone-relying-on-lidar-is-doomed-elon-musk-says.html\:text/html:PDF},
  journal  = {TechCrunch},
}

@InProceedings{bimbraw_autonomous_2015,
  author     = {Bimbraw, K.},
  title      = {Autonomous cars: {Past}, present and future a review of the developments in the last century, the present scenario and the expected future of autonomous vehicle technology},
  booktitle  = {2015 12th {International} {Conference} on {Informatics} in {Control}, {Automation} and {Robotics} ({ICINCO})},
  year       = {2015},
  volume     = {01},
  month      = jul,
  pages      = {191--198},
  abstract   = {The field of autonomous automation is of interest to researchers, and much has been accomplished in this area, of which this paper presents a detailed chronology. This paper can help one understand the trends in autonomous vehicle technology for the past, present, and future. We see a drastic change in autonomous vehicle technology since 1920s, when the first radio controlled vehicles were designed. In the subsequent decades, we see fairly autonomous electric cars powered by embedded circuits in the roads. By 1960s, autonomous cars having similar electronic guide systems came into picture. 1980s saw vision guided autonomous vehicles, which was a major milestone in technology and till date we use similar or modified forms of vision and radio guided technologies. Various semi-autonomous features introduced in modern cars such as lane keeping, automatic braking and adaptive cruise control are based on such systems. Extensive network guided systems in conjunction with vision guided features is the future of autonomous vehicles. It is predicted that most companies will launch fully autonomous vehicles by the advent of next decade. The future of autonomous vehicles is an ambitious era of safe and comfortable transportation.},
  file       = {html:IEEE Xplore Abstract Record\:C/\://Users//Psykie//Zotero//storage//2QT9YZ7P//7350466.html\:text/html:PDF},
  keywords   = {adaptive cruise control, automatic braking, Automation, automobiles, Automobiles, autonomous automation, Autonomous automobiles, Autonomous Cars, autonomous electric cars, autonomous vehicle technology, Autonomous Vehicles, Cars, chronology, electric vehicles, electronic guide systems, embedded circuits, embedded systems, Intelligent Transportation Technologies and Systems, lane keeping, Mechatronics Systems, mobile robots, radio controlled vehicles, radio guided technologies, radiocommunication, Roads, robot vision, time measurement},
  shorttitle = {Autonomous cars},
}

@Article{kaur_trust_2018,
  author     = {Kaur, Kanwaldeep and Rampersad, Giselle},
  title      = {Trust in driverless cars: {Investigating} key factors influencing the adoption of driverless cars},
  journal    = {Journal of Engineering and Technology Management},
  year       = {2018},
  volume     = {48},
  month      = apr,
  pages      = {87--96},
  issn       = {0923-4748},
  doi        = {10.1016/j.jengtecman.2018.04.006},
  url        = {http://www.sciencedirect.com/science/article/pii/S0923474817304253},
  urldate    = {2019-06-09},
  abstract   = {Driverless cars are seen as one of the key disruptors in the next technology revolution. However, the main barrier to adoption is the lack of public trust. The purpose of this study is to investigate the key factors influencing the adoption of driverless cars. Drawing on quantitative evidence, the study found that the ability of the driverless car to meet performance expectations and its reliability were important adoption determinants. Significant concerns included privacy (autonomy, location tracking and surveillance) and security (from hackers). The paper provides implications for firms developing the next generation of car features and early implementation sites.},
  file       = {html:ScienceDirect Snapshot\:C/\://Users//Psykie//Zotero//storage//RZNMCAJV//S0923474817304253.html\:text/html:PDF},
  keywords   = {Driverless cars, New product development, R\&D, Technology adoption, Trust in technology},
  shorttitle = {Trust in driverless cars},
}

@Article{han_road_2017,
  author   = {Han, Xiaofeng and Wang, Huan and Lu, Jianfeng and Zhao, Chunxia},
  title    = {Road detection based on the fusion of {Lidar} and image data},
  journal  = {International Journal of Advanced Robotic Systems},
  year     = {2017},
  language = {en},
  volume   = {14},
  number   = {6},
  month    = nov,
  pages    = {10},
  issn     = {1729-8814},
  doi      = {10.1177/1729881417738102},
  urldate  = {2019-06-09},
  abstract = {In this article, we propose a road detection method based on the fusion of Lidar and image data under the framework of conditional random field. Firstly, Lidar point clouds are projected into the monocular images by cross calibration to get the sparse height images, and then we get high-resolution height images via a joint bilateral filter. Then, for all the training image pixels which have corresponding Lidar points, we extract their features from color image and Lidar point clouds, respectively, and use these features together with the location features to train an Adaboost classifier. After that, all the testing pixels are classified into road or non-road under a conditional random field framework. In this conditional random field framework, we use the scores computed from the Adaboost classifier as the unary potential and take the height value of each pixel and its color information into consideration together for the pairwise potential. Finally, experimental tests have been carried out on the KITTI Road data set, and the results show that our method performs well on this data set.},
}

@WWW{sae_international_u.s._2016-1,
  author  = {{SAE International}},
  title   = {U.{S}. {Department} of {Transportation}’s {New} {Policy} on {Automated} {Vehicles} {Adopts} {SAE} {International}’s {Levels} of {Automation} for {Defining} {Driving} {Automation} in {On}-{Road} {Motor} {Vehicles}},
  year    = {2016},
  url     = {https://www.sae.org/news/3544/},
  address = {WARRENDALE, Pa.},
  type    = {Unpublished {Work}},
}

@WWW{comma._ai_openpilot_2017,
  author = {comma.ai},
  title  = {openpilot},
  year   = {2017},
  url    = {https://openpilot.comma.ai},
  type   = {Unpublished {Work}},
}

@WWW{comma._ai_comma.ai_2017,
  author = {comma.ai},
  title  = {comma.ai},
  year   = {2017},
  url    = {https://comma.ai/},
  type   = {Unpublished {Work}},
}

@WWW{lenovo_lenovo_2016,
  author   = {Lenovo},
  title    = {Lenovo {Phab} 2 {Pro} {\textbar} {The} {World}'s {First} {Google} {Tango}-{Enabled} {Smartphone}},
  year     = {2016},
  url      = {https://www.lenovo.com/us/en/smart-devices/-lenovo-smartphones/phab-series/Lenovo-Phab-2-Pro/p/WMD00000220},
  abstract = {The Phab 2 Pro is the world's first smartphone to include Tango—a new technology from Google that enables augmented reality gaming and utilities.},
  keywords = {Tango, Project Tango, Lenovo Phab 2 Pro, Google Tango, Lenovo and Google, Tango Enabled Smartphone},
}

@Book{buehler_2005_2007,
  title      = {The 2005 {DARPA} {Grand} {Challenge}: {The} {Great} {Robot} {Race}},
  year       = {2007},
  date       = {2007-09-06},
  editor     = {Buehler, Martin and Iagnemma, Karl and Singh, Sanjiv},
  language   = {English},
  edition    = {2007 edition},
  publisher  = {Springer},
  isbn       = {978-3-540-73428-4},
  pagetotal  = {568},
  url        = {https://www.ebook.de/de/product/6700184/the_2005_darpa_grand_challenge.html},
  abstract   = {The DARPA Grand Challenge was a landmark in the field of robotics: a race by autonomous vehicles through 132 miles of rough Nevada terrain. It showcased exciting and unprecedented capabilities in robotic perception, navigation, and control. The event took place in October 2005 and drew teams of competitors from academia and industry, as well as many garage hobbyists. This book presents fifteen technical papers that describe each team's driverless vehicle, race strategy, and insights. As a whole, they present the state of the art in autonomous vehicle technology and offer a glimpse of future technology for tomorrow’s driverless cars.},
  address    = {Berlin ; New York},
  ean        = {9783540734284},
  month      = oct,
  shorttitle = {The 2005 {DARPA} {Grand} {Challenge}},
}

@Article{hannan_review_2017,
  author     = {Hannan, M. A. and Hoque, M. M. and Mohamed, A. and Ayob, A.},
  title      = {Review of energy storage systems for electric vehicle applications: {Issues} and challenges},
  journal    = {Renewable and Sustainable Energy Reviews},
  year       = {2017},
  volume     = {69},
  month      = mar,
  pages      = {771--789},
  issn       = {1364-0321},
  doi        = {10.1016/j.rser.2016.11.171},
  url        = {http://www.sciencedirect.com/science/article/pii/S1364032116309182},
  urldate    = {2019-06-11},
  abstract   = {The electric vehicle (EV) technology addresses the issue of the reduction of carbon and greenhouse gas emissions. The concept of EVs focuses on the utilization of alternative energy resources. However, EV systems currently face challenges in energy storage systems (ESSs) with regard to their safety, size, cost, and overall management issues. In addition, hybridization of ESSs with advanced power electronic technologies has a significant influence on optimal power utilization to lead advanced EV technologies. This paper comprehensively reviews technologies of ESSs, its classifications, characteristics, constructions, electricity conversion, and evaluation processes with advantages and disadvantages for EV applications. Moreover, this paper discusses various classifications of ESS according to their energy formations, composition materials, and techniques on average power delivery over its capacity and overall efficiencies exhibited within their life expectancies. The rigorous review indicates that existing technologies for ESS can be used for EVs, but the optimum use of ESSs for efficient EV energy storage applications has not yet been achieved. This review highlights many factors, challenges, and problems for sustainable development of ESS technologies in next-generation EV applications. Thus, this review will widen the effort toward the development of economic and efficient ESSs with a longer lifetime for future EV uses.},
  file       = {ScienceDirect Full Text PDF:C\:\\Users\\Kai\\Zotero\\storage\\SLA7L9RV\\Hannan et al. - 2017 - Review of energy storage systems for electric vehi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Kai\\Zotero\\storage\\ZGL9XLRJ\\S1364032116309182.html:text/html},
  keywords   = {Electric vehicle, Energy storage systems, Hybridization, Power electronics},
  shorttitle = {Review of energy storage systems for electric vehicle applications},
}

@Article{smit_where_2018,
  author   = {Smit, Robin and Whitehead, Jake and Washington, Simon},
  title    = {Where are we heading with electric vehicles?},
  journal  = {Air Quality and Climate Change},
  year     = {2018},
  language = {EN},
  volume   = {52},
  number   = {3},
  month    = sep,
  pages    = {18},
  url      = {https://search.informit.com.au/documentSummary;dn=978000152970203;res=IELNZC},
  urldate  = {2019-06-11},
  abstract = {This paper concludes that compared to conventional fossil-fuelled vehicle technologies, electric vehicles (EVs) are the best and most robust option with regard to moving to a zero emission road transport system. They enable significant to very deep (98\%) 'reductions' in greenhouse gas emissions, where large reductions depend on the extent that the Australian electricity generation will eventually use renewable energy. Fuel cell vehicles appear not to have the same benefits as battery electric vehicles. They perform only slightly better than conventional fossil-fuelled vehicles in terms of energy use per km. In contrast, EVs use approximately a factor of 3-5 times less energy. In fact, fuel cell vehicles are expected to produce a large 'increase' in greenhouse gas emission of about a factor of two, but have the potential to substantially reduce greenhouse gas emission of about 80\%, but have the potential to substantially reduce greenhouse gas emissions in the long term, but only on the strict condition that Australia significantly increases its use of renewable energy. {\textless}br /{\textgreater}{\textless}br /{\textgreater} EVs and fuel cell vehicles are both expected to significantly improve local air quality, particularly in urban areas where population and associated transport needs are concentrated. However, the extent to which renewable energy is used, is again an important factor in relation to level of improvement that will be achieved. The economic case for EVs is strong. The (hidden) economic costs of air pollution and associated public health impacts caused by fossil-fuelled vehicles will be substantially reduced. 'Total cost' parity (purchase plus operating) with conventional vehicles is expected to occur in the early to mid 2020s. {\textless}br /{\textgreater}{\textless}br /{\textgreater} In contrast to other regions in the world, Australia has a relatively sluggish track record in EV promotion and uptake, mainly due to a lack of supportive policies. New Zealand and some jurisdictions in Australia have taken a significantly more active stance regarding EVs. It is prudent that Australia does the same given the current transformation towards connected and autonomous vehicles (AVs) where electric vehicles are considered to be the 'natural partner' of AVs. EVs can help a transition to a more renewable energy system as relatively cheap energy storage devices. {\textless}br /{\textgreater}{\textless}br /{\textgreater} So where are we heading with electric vehicles? Although there are significant differences between countries and regions in the world, the available data suggest we are definitely heading away from fossil-fuelled road transport towards a fully transformed road transport system where electric vehicles will dominate, or at least play a key role. Electric vehicles are the obvious choice when considering environmental and economic benefits. Other fundamental shifts such as autonomous vehicles and renewable energy are mutually reinforcing developments. Co-development with a clean and climate-friendly electricity generation system will enable deep cuts in greenhouse and air pollution emissions.},
  file     = {Snapshot:C\:\\Users\\Kai\\Zotero\\storage\\BSFH8QQD\\documentSummary\;dn=978000152970203\;res=IELNZC.html:text/html},
}

@TechReport{market_research_future_connected_2019,
  author      = {{Market Research Future}},
  title       = {Connected {Mobility} {Solutions} {Market} {Research} {Report} – {Forecast} to 2023},
  institution = {Market Research Future},
  year        = {2019},
  date        = {2019-05},
  type        = {Market {Research}},
  language    = {en},
  number      = {MRFR/ICT/0369-HCRR},
  month       = may,
  pages       = {145},
  url         = {https://www.marketresearchfuture.com/reports/connected-mobility-solutions-market-871},
  urldate     = {2019-09-21},
  abstract    = {Connected Mobility Solutions Market expected to reach 19\% CAGR forecast period 2018-2023, Connected Mobility Solutions Market Categorizes the Global Market by Technology, Applications, Connectivity {\textbar} Connected Mobility Solutions Industry},
  address     = {Maharashtra},
  file        = {Snapshot:C\:\\Users\\Psykie\\Zotero\\storage\\4J8UDFUH\\connected-mobility-solutions-market-871.html:text/html},
}

@Comment{jabref-meta: databaseType:biblatex;}
